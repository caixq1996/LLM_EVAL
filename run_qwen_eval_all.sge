#!/bin/bash
#$ -S /bin/bash
#$ -cwd
#$ -jc gtn-container_g8.24h
#$ -ac d=nvcr-cuda-12.4.1-ubuntu22.04,d_shm=256g
#$ -j y
set -x
# set -e 

PROJECT_NAME="OPRA"
EXP_NAMES="${EXP_NAMES:-OPRA-LoRA}"
BASE_ROOT="${BASE_ROOT:-/hss/giil/caixq/model}"
PROMPT_TYPE="${PROMPT_TYPE:-think-boxed}"
MAX_TOKENS="${MAX_TOKENS:-3072}"

# 1. 自动探测 GPU 数量
if [[ -z "${NUM_GPUS:-}" ]]; then
  if command -v nvidia-smi >/dev/null 2>&1; then
    NUM_GPUS=$(nvidia-smi --list-gpus | grep -c '^GPU')
    [[ "${NUM_GPUS}" -ge 1 ]] || NUM_GPUS=1
  else
    NUM_GPUS=1
  fi
fi

PYTHON_BIN="${PYTHON_BIN:-$HOME/miniconda3/envs/eval/bin/python3}"
OUT_ROOT="${OUT_ROOT:-$HOME/project/${PROJECT_NAME}/new_eval_${EXP_NAMES}_${PROMPT_TYPE}}"
MODEL_ROOT="${MODEL_ROOT:-$HOME/project/${PROJECT_NAME}/checkpoints/${EXP_NAMES}}"
MAX_SAMPLE_NUMS="${MAX_SAMPLE_NUMS:-32}"
SKIP_BASE_EVAL="${SKIP_BASE_EVAL:-false}"

export TZ='JST-9'
mkdir -p eval_log
TS="$(date +%Y%m%d_%H%M%S)"

echo "[INFO] Job started at ${TS}. Detected ${NUM_GPUS} GPUs."

TEMP_G1="${TEMP_G1:-0.6}"
TEMP_G2="${TEMP_G2:-0.0}"
NSAMP_G1="${NSAMP_G1:-${MAX_SAMPLE_NUMS}}"
NSAMP_G2="${NSAMP_G2:-${MAX_SAMPLE_NUMS}}"

export EVAL_ONE_MODEL_TIMEOUT="${EVAL_ONE_MODEL_TIMEOUT:-21600}"
if [[ -z "${PASS_AT_KS:-}" ]]; then
  default_pass_ks=(1 8 16 32 64 128 256)
  pass_ks=()
  for k in "${default_pass_ks[@]}"; do
    if (( k > 0 && k <= MAX_SAMPLE_NUMS )) && [[ " ${pass_ks[*]} " != *" $k "* ]]; then
      pass_ks+=("$k")
    fi
  done
  PASS_AT_KS=$(IFS=,; echo "${pass_ks[*]}")
fi
export PASS_AT_KS
export TORCH_CPP_LOG_LEVEL=ERROR
export VLLM_WORKER_MULTIPROC_METHOD=spawn
export PYTHONUNBUFFERED=1
export VLLM_USE_FLASHINFER_SAMPLER=1

# 基础参数数组
base_args=(
  --model_root "$MODEL_ROOT"
  --out_root "$OUT_ROOT"
  --prompt_type "$PROMPT_TYPE"
  --max_tokens_per_call "$MAX_TOKENS"
  --base_root "$BASE_ROOT"
  --use_vllm
  --pipeline_parallel_size 1
  --vllm_batch_size 0
  --temperature_g1 "$TEMP_G1"
  --temperature_g2 "$TEMP_G2"
  --n_sampling_g1 "$NSAMP_G1"
  --n_sampling_g2 "$NSAMP_G2"
  # 注意：必须去掉 --cleanup_exported，防止进程间删除模型冲突
  # 清理工作可以留给 merge 步骤或最后统一清理
)

if [ "$SKIP_BASE_EVAL" = "true" ]; then
  base_args+=( --skip_base_eval )
fi

# =======================================================
# 2. 多进程启动逻辑 (Data Parallelism)
# =======================================================

pids=()

for ((i=0; i<NUM_GPUS; i++)); do
    LOG_FILE="eval_log/qwen-eval.${TS}.rank_${i}.log"
    
    echo "[INFO] Starting Worker $i/$NUM_GPUS on GPU $i... Log: $LOG_FILE"
    
    CUDA_VISIBLE_DEVICES=$i "$PYTHON_BIN" -u tools/run_qwen_eval_all_shared.py \
      "${base_args[@]}" \
      --nproc 1 \
      --shard_id "$i" \
      --num_shards "$NUM_GPUS" \
      > "$LOG_FILE" 2>&1 &
      
    pids+=($!)
    sleep 10 
done

# =======================================================
# 3. 等待所有任务完成
# =======================================================

echo "[INFO] All workers started. Waiting for completion..."
wait

# =======================================================
# 4. 合并结果 (Merge Shards)
# =======================================================

echo "[INFO] Tasks completed. Starting merge process..."

# 合并 Base 结果
if [ "$SKIP_BASE_EVAL" != "true" ]; then
    BASE_NAME=$(basename "$BASE_ROOT")
    echo "[INFO] Merging results for base model..."
    "$PYTHON_BIN" tools/merge_results.py --out_root "$OUT_ROOT" --run_name "base__${BASE_NAME}" --prompt_type "$PROMPT_TYPE"
fi

# 合并 Checkpoint 结果
for RUN_DIR in "$OUT_ROOT"/"$EXP_NAMES"__global_step_*; do
    if [ -d "$RUN_DIR" ]; then
        RUN_NAME=$(basename "$RUN_DIR")
        echo "[INFO] Merging results for $RUN_NAME..."
        "$PYTHON_BIN" tools/merge_results.py --out_root "$OUT_ROOT" --run_name "$RUN_NAME" --prompt_type "$PROMPT_TYPE"
    fi
done

echo "[INFO] Done at $(date). All workers finished and results merged."
