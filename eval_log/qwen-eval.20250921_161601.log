+ TEMP_G1=0.6
+ TEMP_G2=0.0
+ NSAMP_G1=8
+ NSAMP_G2=8
+ export EVAL_ONE_MODEL_TIMEOUT=21600
+ EVAL_ONE_MODEL_TIMEOUT=21600
+ export PASS_AT_KS=1,8
+ PASS_AT_KS=1,8
+ export TORCH_CPP_LOG_LEVEL=ERROR
+ TORCH_CPP_LOG_LEVEL=ERROR
++ seq -s, 0 3
+ export CUDA_VISIBLE_DEVICES=0,1,2,3
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ export VLLM_WORKER_MULTIPROC_METHOD=spawn
+ VLLM_WORKER_MULTIPROC_METHOD=spawn
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
+ export VLLM_USE_FLASHINFER_SAMPLER=1
+ VLLM_USE_FLASHINFER_SAMPLER=1
+ args=(--model_root "$MODEL_ROOT" --out_root "$OUT_ROOT" --prompt_type "$PROMPT_TYPE" --max_tokens_per_call "$MAX_TOKENS" --nproc "$NUM_GPUS" --base_root "$BASE_ROOT" --use_vllm --pipeline_parallel_size 1 --vllm_batch_size 0 --temperature_g1 "$TEMP_G1" --temperature_g2 "$TEMP_G2" --n_sampling_g1 "$NSAMP_G1" --n_sampling_g2 "$NSAMP_G2" --cleanup_exported)
+ echo '[INFO] Running: /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3 -u tools/run_qwen_eval_all_shared.py --model_root /uge_mnt/home/caixq/project/noisy-RLVR/checkpoints/noise_rlvr_1_5b_128batchsize_deepscaler_v2 --out_root /uge_mnt/home/caixq/project/noisy-RLVR/eval_qwen_test_v7 --prompt_type think-boxed --max_tokens_per_call 3072 --nproc 4 --base_root /hss/giil/caixq/model --use_vllm --pipeline_parallel_size 1 --vllm_batch_size 0 --temperature_g1 0.6 --temperature_g2 0.0 --n_sampling_g1 8 --n_sampling_g2 8 --cleanup_exported'
[INFO] Running: /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3 -u tools/run_qwen_eval_all_shared.py --model_root /uge_mnt/home/caixq/project/noisy-RLVR/checkpoints/noise_rlvr_1_5b_128batchsize_deepscaler_v2 --out_root /uge_mnt/home/caixq/project/noisy-RLVR/eval_qwen_test_v7 --prompt_type think-boxed --max_tokens_per_call 3072 --nproc 4 --base_root /hss/giil/caixq/model --use_vllm --pipeline_parallel_size 1 --vllm_batch_size 0 --temperature_g1 0.6 --temperature_g2 0.0 --n_sampling_g1 8 --n_sampling_g2 8 --cleanup_exported
+ /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3 -u tools/run_qwen_eval_all_shared.py --model_root /uge_mnt/home/caixq/project/noisy-RLVR/checkpoints/noise_rlvr_1_5b_128batchsize_deepscaler_v2 --out_root /uge_mnt/home/caixq/project/noisy-RLVR/eval_qwen_test_v7 --prompt_type think-boxed --max_tokens_per_call 3072 --nproc 4 --base_root /hss/giil/caixq/model --use_vllm --pipeline_parallel_size 1 --vllm_batch_size 0 --temperature_g1 0.6 --temperature_g2 0.0 --n_sampling_g1 8 --n_sampling_g2 8 --cleanup_exported
INFO 09-21 16:18:39 [__init__.py:239] Automatically detected platform cuda.
[2025-09-21 16:19:45] ÂèëÁé∞ 17 ‰∏™ run„ÄÇÂÖàÂØºÂá∫/Ê£ÄÊü•ÁªìÊûúÔºåÂÜçÂØπ‚ÄúÁº∫Â§±Êï∞ÊçÆÈõÜ‚ÄùÁöÑÊ®°ÂûãÈÄê‰∏Ä‰ª•Â≠êËøõÁ®ãËøêË°å„ÄÇ
[2025-09-21 16:19:45] ‚è≠ Ë∑≥Ëøá base-onlyÔºöDeepSeek-R1-Distill-Qwen-1.5BÔºàg1/g2 Â∑≤Êúâ metricsÔºâ
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
[OK] Exported: /uge_mnt/home/caixq/project/noisy-RLVR/eval_qwen_test_v7/_exports/B_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_DeepSeek-R1-Distill-Qwen-1.5B/global_step_100
[OK] Exported: /uge_mnt/home/caixq/project/noisy-RLVR/eval_qwen_test_v7/_exports/B_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_DeepSeek-R1-Distill-Qwen-1.5B/global_step_200
[OK] Exported: /uge_mnt/home/caixq/project/noisy-RLVR/eval_qwen_test_v7/_exports/B_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_DeepSeek-R1-Distill-Qwen-1.5B/global_step_313
[2025-09-21 16:24:29] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_DeepSeek-R1-Distill-Qwen-1.5B__global_step_100Ôºàg1/g2 ÂÖ®ÈÉ®Â∑≤Êúâ metricsÔºâ
[2025-09-21 16:24:29] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_DeepSeek-R1-Distill-Qwen-1.5B__global_step_200Ôºàg1/g2 ÂÖ®ÈÉ®Â∑≤Êúâ metricsÔºâ
[2025-09-21 16:24:29] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_DeepSeek-R1-Distill-Qwen-1.5B__global_step_313Ôºàg1/g2 ÂÖ®ÈÉ®Â∑≤Êúâ metricsÔºâ
INFO 09-21 16:24:34 [__init__.py:239] Automatically detected platform cuda.
[2025-09-21 16:24:38] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/hss/giil/caixq/model/Qwen2.5-math-1.5B
INFO 09-21 16:25:02 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.
INFO 09-21 16:25:02 [config.py:1770] Defaulting to use ray for distributed inference
INFO 09-21 16:25:02 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 09-21 16:25:12 [__init__.py:239] Automatically detected platform cuda.
INFO 09-21 16:25:17 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/hss/giil/caixq/model/Qwen2.5-math-1.5B', speculative_config=None, tokenizer='/hss/giil/caixq/model/Qwen2.5-math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/hss/giil/caixq/model/Qwen2.5-math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
2025-09-21 16:25:30,790	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-09-21 16:25:31,085	INFO worker.py:1918 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
INFO 09-21 16:25:33 [ray_utils.py:335] No current placement group found. Creating a new placement group.
WARNING 09-21 16:25:33 [ray_utils.py:342] The number of required GPUs exceeds the total number of available GPUs in the placement group.
INFO 09-21 16:25:43 [ray_utils.py:233] Waiting for creating a placement group of specs for 10 seconds. specs=[{'GPU': 1.0, 'node:172.17.0.5': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 09-21 16:26:03 [ray_utils.py:233] Waiting for creating a placement group of specs for 30 seconds. specs=[{'GPU': 1.0, 'node:172.17.0.5': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 09-21 16:26:43 [ray_utils.py:233] Waiting for creating a placement group of specs for 70 seconds. specs=[{'GPU': 1.0, 'node:172.17.0.5': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 09-21 16:28:03 [ray_utils.py:233] Waiting for creating a placement group of specs for 150 seconds. specs=[{'GPU': 1.0, 'node:172.17.0.5': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 09-21 16:30:43 [ray_utils.py:233] Waiting for creating a placement group of specs for 310 seconds. specs=[{'GPU': 1.0, 'node:172.17.0.5': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 09-21 16:36:03 [ray_utils.py:233] Waiting for creating a placement group of specs for 630 seconds. specs=[{'GPU': 1.0, 'node:172.17.0.5': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 09-21 16:46:43 [ray_utils.py:233] Waiting for creating a placement group of specs for 1270 seconds. specs=[{'GPU': 1.0, 'node:172.17.0.5': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
