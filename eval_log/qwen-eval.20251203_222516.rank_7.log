INFO 12-03 22:27:16 [__init__.py:239] Automatically detected platform cuda.
[2025-12-03 22:28:24] ÂèëÁé∞ 7 ‰∏™ run„ÄÇ
[2025-12-03 22:28:24] ‚è≠ Ë∑≥Ëøá base-onlyÔºöLlama-3.2-3B-Instruct
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_100
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_200
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_300
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_313
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_313
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100
INFO 12-03 22:28:30 [__init__.py:239] Automatically detected platform cuda.
[2025-12-03 22:28:39] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:29:01 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:29:01 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:29:01 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:29:11 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:29:20 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:29:24 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc1061924a0>
INFO 12-03 22:29:57 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:29:57 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:29:57 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:29:58 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:25<00:25, 25.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:26<00:00, 11.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:26<00:00, 13.35s/it]

INFO 12-03 22:30:27 [loader.py:458] Loading weights took 26.77 seconds
INFO 12-03 22:30:27 [gpu_model_runner.py:1347] Model loading took 6.0160 GiB and 29.225930 seconds
INFO 12-03 22:30:30 [kv_cache_utils.py:634] GPU KV cache size: 258,240 tokens
INFO 12-03 22:30:30 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.97x
INFO 12-03 22:30:30 [core.py:159] init engine (profile, create kv cache, warmup model) took 2.89 seconds
INFO 12-03 22:30:30 [core_client.py:439] Core engine process 0 ready.
[2025-12-03 22:30:30] ‚ÑπÔ∏è  ÂΩìÂâçÂ∑•‰ΩúËäÇÁÇπÂàÜÁâá: 7/8
[2025-12-03 22:30:30] ‚úì Ê®°ÂûãÂ∞±Áª™ÔºåÂºÄÂßãËØÑÊµã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200ÔºàÂÖ±‰∫´Âêå‰∏Ä LLMÔºå‰ªÖË°•Áº∫Êï∞ÊçÆÈõÜÔºâ
[2025-12-03 22:30:30] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1  ÂæÖËØÑÊµã=['aime25x8', 'amc23x8', 'aime24x8']  T=0.6  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 7/8 handling range [210:240]
==================================================
data: aime25x8  ,remain samples: 30
{'idx': 210, 'problem': 'Find the sum of all integer bases $b>9$ for which $17_{b}$ is a divisor of $97_{b}$.', 'answer': '70'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 331.86it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:05<22:26,  5.63s/it, est. speed input: 27.69 toks/s, output: 34.43 toks/s][A
Processed prompts:   1%|          | 2/240 [00:06<11:44,  2.96s/it, est. speed input: 46.41 toks/s, output: 64.86 toks/s][A
Processed prompts:   1%|‚ñè         | 3/240 [00:08<08:41,  2.20s/it, est. speed input: 56.89 toks/s, output: 91.45 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:09<05:15,  1.34s/it, est. speed input: 91.88 toks/s, output: 147.41 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:09<04:06,  1.06s/it, est. speed input: 101.33 toks/s, output: 179.84 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:10<03:12,  1.21it/s, est. speed input: 107.12 toks/s, output: 212.65 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:10<02:48,  1.38it/s, est. speed input: 113.54 toks/s, output: 240.71 toks/s][A
Processed prompts:   4%|‚ñç         | 9/240 [00:10<02:12,  1.75it/s, est. speed input: 119.50 toks/s, output: 273.97 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:11<02:04,  1.85it/s, est. speed input: 128.69 toks/s, output: 300.48 toks/s][A
Processed prompts:   5%|‚ñç         | 11/240 [00:11<01:35,  2.39it/s, est. speed input: 135.79 toks/s, output: 334.92 toks/s][A
Processed prompts:   5%|‚ñå         | 13/240 [00:11<01:11,  3.17it/s, est. speed input: 152.91 toks/s, output: 398.29 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:11<00:59,  3.80it/s, est. speed input: 157.84 toks/s, output: 432.70 toks/s][A
Processed prompts:   6%|‚ñã         | 15/240 [00:12<01:17,  2.89it/s, est. speed input: 157.40 toks/s, output: 450.37 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:12<01:16,  2.92it/s, est. speed input: 160.88 toks/s, output: 476.34 toks/s][A
Processed prompts:   8%|‚ñä         | 19/240 [00:12<00:40,  5.49it/s, est. speed input: 197.57 toks/s, output: 584.67 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:13<00:32,  6.73it/s, est. speed input: 216.73 toks/s, output: 682.59 toks/s][A
Processed prompts:  10%|‚ñâ         | 23/240 [00:13<00:35,  6.11it/s, est. speed input: 220.60 toks/s, output: 708.44 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:13<00:29,  7.38it/s, est. speed input: 231.05 toks/s, output: 775.65 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:13<00:30,  7.02it/s, est. speed input: 239.02 toks/s, output: 833.08 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 29/240 [00:13<00:24,  8.75it/s, est. speed input: 251.26 toks/s, output: 902.53 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:14<00:24,  8.37it/s, est. speed input: 262.17 toks/s, output: 961.35 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 33/240 [00:14<00:20, 10.11it/s, est. speed input: 272.19 toks/s, output: 1030.06 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 36/240 [00:14<00:19, 10.61it/s, est. speed input: 294.19 toks/s, output: 1124.24 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 38/240 [00:14<00:24,  8.31it/s, est. speed input: 303.45 toks/s, output: 1171.31 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 40/240 [00:15<00:23,  8.43it/s, est. speed input: 313.63 toks/s, output: 1228.71 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 43/240 [00:15<00:21,  9.34it/s, est. speed input: 332.35 toks/s, output: 1319.76 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 45/240 [00:15<00:19, 10.05it/s, est. speed input: 355.53 toks/s, output: 1381.46 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 48/240 [00:15<00:17, 11.26it/s, est. speed input: 377.99 toks/s, output: 1475.16 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 50/240 [00:15<00:16, 11.61it/s, est. speed input: 391.05 toks/s, output: 1535.92 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 52/240 [00:16<00:22,  8.40it/s, est. speed input: 395.84 toks/s, output: 1571.85 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñé       | 54/240 [00:16<00:18,  9.96it/s, est. speed input: 404.52 toks/s, output: 1637.41 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 57/240 [00:16<00:14, 12.66it/s, est. speed input: 436.80 toks/s, output: 1737.36 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 59/240 [00:16<00:14, 12.22it/s, est. speed input: 446.35 toks/s, output: 1793.97 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 61/240 [00:16<00:14, 12.40it/s, est. speed input: 462.14 toks/s, output: 1852.90 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 63/240 [00:17<00:25,  6.94it/s, est. speed input: 458.72 toks/s, output: 1862.88 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 65/240 [00:17<00:21,  8.24it/s, est. speed input: 476.16 toks/s, output: 1924.80 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 67/240 [00:17<00:17,  9.88it/s, est. speed input: 484.22 toks/s, output: 1989.05 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 69/240 [00:17<00:14, 11.54it/s, est. speed input: 503.88 toks/s, output: 2053.04 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 73/240 [00:18<00:14, 11.80it/s, est. speed input: 539.38 toks/s, output: 2165.21 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 76/240 [00:18<00:16, 10.15it/s, est. speed input: 554.74 toks/s, output: 2233.08 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 78/240 [00:18<00:15, 10.74it/s, est. speed input: 572.37 toks/s, output: 2290.55 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:18<00:13, 11.72it/s, est. speed input: 594.51 toks/s, output: 2350.91 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 82/240 [00:19<00:16,  9.75it/s, est. speed input: 595.28 toks/s, output: 2389.90 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 84/240 [00:19<00:16,  9.20it/s, est. speed input: 600.49 toks/s, output: 2434.81 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 86/240 [00:20<00:28,  5.43it/s, est. speed input: 591.78 toks/s, output: 2419.83 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 87/240 [00:20<00:41,  3.68it/s, est. speed input: 584.00 toks/s, output: 2376.02 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 88/240 [00:21<00:42,  3.59it/s, est. speed input: 578.65 toks/s, output: 2379.89 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 89/240 [00:22<01:02,  2.42it/s, est. speed input: 560.81 toks/s, output: 2321.18 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 90/240 [00:22<01:02,  2.39it/s, est. speed input: 561.28 toks/s, output: 2314.11 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 91/240 [00:22<00:51,  2.89it/s, est. speed input: 568.95 toks/s, output: 2338.96 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 92/240 [00:22<00:41,  3.55it/s, est. speed input: 577.60 toks/s, output: 2366.46 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 94/240 [00:22<00:27,  5.31it/s, est. speed input: 586.34 toks/s, output: 2429.21 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 95/240 [00:23<00:26,  5.54it/s, est. speed input: 587.87 toks/s, output: 2451.09 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 96/240 [00:23<00:36,  3.97it/s, est. speed input: 582.26 toks/s, output: 2440.86 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 97/240 [00:23<00:32,  4.42it/s, est. speed input: 582.12 toks/s, output: 2463.03 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:23<00:29,  4.83it/s, est. speed input: 583.60 toks/s, output: 2485.09 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 99/240 [00:24<00:33,  4.23it/s, est. speed input: 587.56 toks/s, output: 2491.15 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 100/240 [00:24<00:29,  4.68it/s, est. speed input: 587.37 toks/s, output: 2513.32 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 101/240 [00:24<00:32,  4.26it/s, est. speed input: 590.03 toks/s, output: 2522.17 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/240 [00:24<00:37,  3.66it/s, est. speed input: 592.45 toks/s, output: 2523.44 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 103/240 [00:25<00:33,  4.06it/s, est. speed input: 592.57 toks/s, output: 2543.31 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 104/240 [00:25<00:28,  4.72it/s, est. speed input: 594.41 toks/s, output: 2568.36 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/240 [00:25<00:30,  4.42it/s, est. speed input: 593.03 toks/s, output: 2580.43 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 106/240 [00:25<00:27,  4.88it/s, est. speed input: 594.09 toks/s, output: 2603.04 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:25<00:26,  5.07it/s, est. speed input: 593.31 toks/s, output: 2623.26 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 110/240 [00:26<00:16,  8.03it/s, est. speed input: 608.68 toks/s, output: 2716.93 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 111/240 [00:28<01:10,  1.83it/s, est. speed input: 568.48 toks/s, output: 2550.71 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 112/240 [00:28<01:01,  2.08it/s, est. speed input: 566.88 toks/s, output: 2565.53 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 113/240 [00:28<00:50,  2.52it/s, est. speed input: 568.84 toks/s, output: 2590.96 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 114/240 [00:28<00:44,  2.85it/s, est. speed input: 567.42 toks/s, output: 2609.16 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 115/240 [00:29<00:39,  3.19it/s, est. speed input: 565.79 toks/s, output: 2628.24 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 116/240 [00:29<00:41,  2.95it/s, est. speed input: 561.54 toks/s, output: 2630.61 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 118/240 [00:29<00:31,  3.81it/s, est. speed input: 563.52 toks/s, output: 2677.59 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/240 [00:30<00:25,  4.74it/s, est. speed input: 571.44 toks/s, output: 2730.77 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:30<00:28,  4.07it/s, est. speed input: 567.08 toks/s, output: 2752.10 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 123/240 [00:30<00:27,  4.27it/s, est. speed input: 566.91 toks/s, output: 2773.58 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:31<00:28,  4.07it/s, est. speed input: 569.96 toks/s, output: 2786.67 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:32<00:57,  1.99it/s, est. speed input: 551.58 toks/s, output: 2713.78 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 126/240 [00:32<00:58,  1.96it/s, est. speed input: 546.01 toks/s, output: 2708.51 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 127/240 [00:34<01:19,  1.42it/s, est. speed input: 534.54 toks/s, output: 2650.07 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:35<01:27,  1.29it/s, est. speed input: 523.78 toks/s, output: 2615.68 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:35<01:26,  1.29it/s, est. speed input: 515.99 toks/s, output: 2597.81 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 130/240 [00:36<01:10,  1.56it/s, est. speed input: 514.74 toks/s, output: 2614.96 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 131/240 [00:36<00:54,  2.01it/s, est. speed input: 516.35 toks/s, output: 2642.99 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 133/240 [00:37<00:45,  2.34it/s, est. speed input: 514.62 toks/s, output: 2671.44 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 134/240 [00:37<00:38,  2.75it/s, est. speed input: 519.16 toks/s, output: 2698.11 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:37<00:48,  2.18it/s, est. speed input: 512.73 toks/s, output: 2685.06 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 136/240 [00:39<01:15,  1.37it/s, est. speed input: 496.99 toks/s, output: 2624.60 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 137/240 [00:41<01:44,  1.02s/it, est. speed input: 481.99 toks/s, output: 2551.50 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:42<01:37,  1.04it/s, est. speed input: 475.54 toks/s, output: 2541.39 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:44<02:29,  1.48s/it, est. speed input: 449.34 toks/s, output: 2424.17 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 140/240 [00:47<03:03,  1.84s/it, est. speed input: 426.98 toks/s, output: 2324.53 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:53<04:52,  2.95s/it, est. speed input: 384.14 toks/s, output: 2117.01 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:54<03:02,  1.88s/it, est. speed input: 382.40 toks/s, output: 2146.24 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:55<02:40,  1.67s/it, est. speed input: 377.49 toks/s, output: 2144.22 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:58<03:13,  2.03s/it, est. speed input: 361.39 toks/s, output: 2070.34 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 146/240 [00:59<02:41,  1.71s/it, est. speed input: 358.85 toks/s, output: 2078.74 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [01:04<04:09,  2.68s/it, est. speed input: 332.62 toks/s, output: 1949.13 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 148/240 [01:06<03:39,  2.38s/it, est. speed input: 326.33 toks/s, output: 1938.61 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [01:13<05:59,  3.95s/it, est. speed input: 293.64 toks/s, output: 1770.94 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [01:25<09:10,  6.11s/it, est. speed input: 255.34 toks/s, output: 1571.43 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [01:25<06:27,  4.35s/it, est. speed input: 255.70 toks/s, output: 1604.95 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 232/240 [01:26<00:01,  7.44it/s, est. speed input: 409.34 toks/s, output: 4474.46 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 235/240 [01:27<00:00,  6.62it/s, est. speed input: 406.74 toks/s, output: 4507.02 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 238/240 [01:28<00:00,  5.89it/s, est. speed input: 404.52 toks/s, output: 4543.79 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:29<00:00,  5.47it/s, est. speed input: 403.48 toks/s, output: 4572.31 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:29<00:00,  2.67it/s, est. speed input: 403.48 toks/s, output: 4572.31 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 17889.65it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 0, 'acc': 0.0, 'total_acc': 0.4166666666666667, 'pass_at_k_percent': {'1': 0.4, '8': 3.3}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime25x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part7.jsonl
[2025-12-03 22:32:01] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime25x8  acc=0.0 pass_at_k={'1': 0.4, '8': 3.3}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:31<03:02, 91.18s/ds][Info] Sharding enabled: Process 7/8 handling range [280:320]
==================================================
data: amc23x8  ,remain samples: 40
{'idx': 280, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}

  0%|          | 0/40 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:00<00:00, 427.43it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/320 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/320 [00:00<00:39,  8.08it/s, est. speed input: 558.01 toks/s, output: 24.25 toks/s][A
Processed prompts:   1%|          | 2/320 [00:02<07:55,  1.50s/it, est. speed input: 93.77 toks/s, output: 48.82 toks/s] [A
Processed prompts:   1%|          | 3/320 [00:04<07:50,  1.48s/it, est. speed input: 87.15 toks/s, output: 78.26 toks/s][A
Processed prompts:   2%|‚ñè         | 5/320 [00:04<03:59,  1.32it/s, est. speed input: 117.91 toks/s, output: 160.95 toks/s][A
Processed prompts:   2%|‚ñè         | 6/320 [00:04<03:05,  1.69it/s, est. speed input: 137.51 toks/s, output: 201.40 toks/s][A
Processed prompts:   2%|‚ñé         | 8/320 [00:05<02:16,  2.28it/s, est. speed input: 168.47 toks/s, output: 268.85 toks/s][A
Processed prompts:   3%|‚ñé         | 9/320 [00:05<01:54,  2.71it/s, est. speed input: 184.62 toks/s, output: 306.88 toks/s][A
Processed prompts:   3%|‚ñé         | 10/320 [00:05<01:46,  2.91it/s, est. speed input: 185.33 toks/s, output: 337.05 toks/s][A
Processed prompts:   3%|‚ñé         | 11/320 [00:05<01:32,  3.35it/s, est. speed input: 195.82 toks/s, output: 371.61 toks/s][A
Processed prompts:   4%|‚ñç         | 12/320 [00:05<01:15,  4.08it/s, est. speed input: 201.49 toks/s, output: 409.65 toks/s][A
Processed prompts:   4%|‚ñç         | 14/320 [00:06<01:03,  4.80it/s, est. speed input: 229.17 toks/s, output: 474.86 toks/s][A
Processed prompts:   5%|‚ñå         | 16/320 [00:06<00:45,  6.63it/s, est. speed input: 252.30 toks/s, output: 555.15 toks/s][A
Processed prompts:   5%|‚ñå         | 17/320 [00:06<00:47,  6.35it/s, est. speed input: 260.22 toks/s, output: 583.63 toks/s][A
Processed prompts:   6%|‚ñå         | 18/320 [00:06<00:44,  6.84it/s, est. speed input: 272.74 toks/s, output: 618.27 toks/s][A
Processed prompts:   7%|‚ñã         | 21/320 [00:06<00:31,  9.49it/s, est. speed input: 301.62 toks/s, output: 730.29 toks/s][A
Processed prompts:   7%|‚ñã         | 23/320 [00:07<00:43,  6.80it/s, est. speed input: 307.51 toks/s, output: 768.91 toks/s][A
Processed prompts:   8%|‚ñä         | 24/320 [00:07<00:49,  5.92it/s, est. speed input: 303.96 toks/s, output: 785.33 toks/s][A
Processed prompts:   8%|‚ñä         | 26/320 [00:07<00:47,  6.13it/s, est. speed input: 305.93 toks/s, output: 839.70 toks/s][A
Processed prompts:   8%|‚ñä         | 27/320 [00:08<01:08,  4.27it/s, est. speed input: 295.61 toks/s, output: 830.17 toks/s][A
Processed prompts:   9%|‚ñâ         | 29/320 [00:08<01:07,  4.28it/s, est. speed input: 293.19 toks/s, output: 869.56 toks/s][A
Processed prompts:  10%|‚ñâ         | 31/320 [00:08<00:52,  5.53it/s, est. speed input: 311.67 toks/s, output: 938.47 toks/s][A
Processed prompts:  10%|‚ñà         | 32/320 [00:09<01:01,  4.67it/s, est. speed input: 310.33 toks/s, output: 944.95 toks/s][A
Processed prompts:  11%|‚ñà         | 34/320 [00:09<00:44,  6.36it/s, est. speed input: 341.93 toks/s, output: 1017.52 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 37/320 [00:09<00:30,  9.34it/s, est. speed input: 364.46 toks/s, output: 1128.26 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 39/320 [00:09<00:27, 10.04it/s, est. speed input: 375.66 toks/s, output: 1191.56 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 41/320 [00:09<00:29,  9.46it/s, est. speed input: 388.67 toks/s, output: 1244.74 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 43/320 [00:10<00:30,  9.06it/s, est. speed input: 403.06 toks/s, output: 1256.07 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 45/320 [00:10<00:31,  8.77it/s, est. speed input: 410.84 toks/s, output: 1227.15 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 47/320 [00:10<00:26, 10.16it/s, est. speed input: 425.46 toks/s, output: 1253.46 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 50/320 [00:10<00:21, 12.76it/s, est. speed input: 443.58 toks/s, output: 1278.13 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 52/320 [00:11<00:26, 10.20it/s, est. speed input: 450.78 toks/s, output: 1323.43 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 55/320 [00:11<00:21, 12.10it/s, est. speed input: 477.91 toks/s, output: 1423.26 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 57/320 [00:11<00:21, 12.03it/s, est. speed input: 495.13 toks/s, output: 1481.86 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 59/320 [00:11<00:26,  9.83it/s, est. speed input: 497.66 toks/s, output: 1495.97 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 62/320 [00:11<00:21, 12.20it/s, est. speed input: 523.85 toks/s, output: 1596.41 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 64/320 [00:11<00:19, 13.21it/s, est. speed input: 545.56 toks/s, output: 1659.95 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 66/320 [00:12<00:18, 13.87it/s, est. speed input: 554.12 toks/s, output: 1721.32 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 68/320 [00:12<00:29,  8.64it/s, est. speed input: 550.72 toks/s, output: 1713.04 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 70/320 [00:12<00:26,  9.45it/s, est. speed input: 562.26 toks/s, output: 1769.42 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 73/320 [00:12<00:22, 11.10it/s, est. speed input: 584.49 toks/s, output: 1860.02 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 75/320 [00:12<00:19, 12.43it/s, est. speed input: 594.39 toks/s, output: 1922.89 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 77/320 [00:13<00:21, 11.36it/s, est. speed input: 598.31 toks/s, output: 1947.62 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 79/320 [00:13<00:23, 10.27it/s, est. speed input: 609.28 toks/s, output: 1990.46 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 81/320 [00:13<00:24,  9.56it/s, est. speed input: 618.72 toks/s, output: 2032.57 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 83/320 [00:13<00:24,  9.85it/s, est. speed input: 621.05 toks/s, output: 2061.23 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 85/320 [00:14<00:24,  9.66it/s, est. speed input: 621.44 toks/s, output: 2074.56 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 87/320 [00:14<00:36,  6.36it/s, est. speed input: 617.54 toks/s, output: 2070.80 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 91/320 [00:14<00:23,  9.91it/s, est. speed input: 637.99 toks/s, output: 2167.88 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 94/320 [00:14<00:19, 11.64it/s, est. speed input: 649.19 toks/s, output: 2238.44 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 96/320 [00:15<00:20, 10.96it/s, est. speed input: 655.41 toks/s, output: 2267.47 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 98/320 [00:15<00:18, 11.74it/s, est. speed input: 665.27 toks/s, output: 2325.27 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà‚ñè      | 100/320 [00:15<00:16, 12.99it/s, est. speed input: 674.09 toks/s, output: 2386.44 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 102/320 [00:15<00:28,  7.71it/s, est. speed input: 675.06 toks/s, output: 2382.82 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 104/320 [00:16<00:26,  8.07it/s, est. speed input: 678.45 toks/s, output: 2400.04 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 108/320 [00:16<00:17, 12.41it/s, est. speed input: 717.21 toks/s, output: 2539.01 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 110/320 [00:16<00:15, 13.54it/s, est. speed input: 730.57 toks/s, output: 2600.05 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 112/320 [00:16<00:19, 10.83it/s, est. speed input: 729.24 toks/s, output: 2632.05 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 114/320 [00:17<00:27,  7.60it/s, est. speed input: 728.65 toks/s, output: 2635.99 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 117/320 [00:17<00:25,  7.96it/s, est. speed input: 729.81 toks/s, output: 2698.91 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 122/320 [00:17<00:16, 12.20it/s, est. speed input: 762.27 toks/s, output: 2822.79 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 125/320 [00:17<00:13, 14.07it/s, est. speed input: 776.73 toks/s, output: 2917.79 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 127/320 [00:17<00:14, 13.76it/s, est. speed input: 783.31 toks/s, output: 2956.17 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 129/320 [00:18<00:13, 14.10it/s, est. speed input: 796.33 toks/s, output: 3000.41 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 131/320 [00:18<00:14, 12.64it/s, est. speed input: 800.15 toks/s, output: 3028.41 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 133/320 [00:18<00:20,  9.11it/s, est. speed input: 795.46 toks/s, output: 3016.91 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 135/320 [00:18<00:20,  9.24it/s, est. speed input: 798.70 toks/s, output: 3045.80 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 137/320 [00:19<00:19,  9.34it/s, est. speed input: 801.94 toks/s, output: 3089.66 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 139/320 [00:19<00:19,  9.41it/s, est. speed input: 804.79 toks/s, output: 3112.73 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 141/320 [00:19<00:17, 10.15it/s, est. speed input: 809.87 toks/s, output: 3123.48 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 144/320 [00:19<00:13, 13.51it/s, est. speed input: 825.99 toks/s, output: 3202.60 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 146/320 [00:19<00:11, 14.70it/s, est. speed input: 833.05 toks/s, output: 3244.88 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 148/320 [00:20<00:20,  8.57it/s, est. speed input: 824.73 toks/s, output: 3207.89 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 151/320 [00:20<00:14, 11.62it/s, est. speed input: 847.96 toks/s, output: 3272.09 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 154/320 [00:20<00:11, 14.06it/s, est. speed input: 862.53 toks/s, output: 3367.31 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 156/320 [00:20<00:14, 11.34it/s, est. speed input: 861.76 toks/s, output: 3379.38 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 158/320 [00:20<00:13, 12.22it/s, est. speed input: 866.13 toks/s, output: 3422.92 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 160/320 [00:20<00:14, 11.04it/s, est. speed input: 866.96 toks/s, output: 3462.52 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 162/320 [00:21<00:13, 12.07it/s, est. speed input: 877.04 toks/s, output: 3518.73 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 164/320 [00:21<00:18,  8.28it/s, est. speed input: 870.59 toks/s, output: 3525.48 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 166/320 [00:21<00:17,  8.69it/s, est. speed input: 872.72 toks/s, output: 3549.85 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 168/320 [00:21<00:14, 10.41it/s, est. speed input: 883.52 toks/s, output: 3601.18 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 170/320 [00:22<00:16,  9.22it/s, est. speed input: 888.57 toks/s, output: 3633.37 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 173/320 [00:22<00:12, 12.10it/s, est. speed input: 902.54 toks/s, output: 3728.52 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 177/320 [00:22<00:10, 14.05it/s, est. speed input: 916.88 toks/s, output: 3827.34 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 179/320 [00:23<00:17,  7.94it/s, est. speed input: 912.88 toks/s, output: 3802.03 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 181/320 [00:23<00:18,  7.62it/s, est. speed input: 908.41 toks/s, output: 3793.54 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 184/320 [00:23<00:14,  9.23it/s, est. speed input: 912.04 toks/s, output: 3877.77 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 186/320 [00:23<00:16,  8.07it/s, est. speed input: 913.98 toks/s, output: 3880.68 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 189/320 [00:24<00:15,  8.31it/s, est. speed input: 915.59 toks/s, output: 3906.66 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 192/320 [00:24<00:12, 10.37it/s, est. speed input: 922.85 toks/s, output: 3965.11 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 194/320 [00:24<00:15,  8.10it/s, est. speed input: 916.43 toks/s, output: 3976.06 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 197/320 [00:25<00:15,  7.76it/s, est. speed input: 914.40 toks/s, output: 4008.16 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 198/320 [00:25<00:16,  7.39it/s, est. speed input: 917.97 toks/s, output: 4010.39 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 199/320 [00:25<00:22,  5.28it/s, est. speed input: 904.48 toks/s, output: 3960.24 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 202/320 [00:26<00:20,  5.83it/s, est. speed input: 903.47 toks/s, output: 3996.01 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 203/320 [00:27<00:29,  3.91it/s, est. speed input: 889.90 toks/s, output: 3929.75 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 205/320 [00:27<00:24,  4.62it/s, est. speed input: 890.84 toks/s, output: 3940.32 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 207/320 [00:27<00:19,  5.67it/s, est. speed input: 894.28 toks/s, output: 3993.82 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 208/320 [00:27<00:18,  6.09it/s, est. speed input: 893.50 toks/s, output: 4017.45 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 209/320 [00:27<00:17,  6.35it/s, est. speed input: 893.18 toks/s, output: 4021.67 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 211/320 [00:27<00:15,  7.18it/s, est. speed input: 894.78 toks/s, output: 4069.30 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 212/320 [00:28<00:19,  5.61it/s, est. speed input: 888.56 toks/s, output: 4048.20 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 213/320 [00:28<00:19,  5.51it/s, est. speed input: 885.81 toks/s, output: 4060.51 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 215/320 [00:28<00:21,  4.82it/s, est. speed input: 878.95 toks/s, output: 4056.22 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 216/320 [00:29<00:22,  4.60it/s, est. speed input: 875.13 toks/s, output: 4061.14 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 218/320 [00:29<00:19,  5.24it/s, est. speed input: 874.23 toks/s, output: 4099.52 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 219/320 [00:30<00:32,  3.11it/s, est. speed input: 855.38 toks/s, output: 4032.08 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 220/320 [00:31<01:04,  1.56it/s, est. speed input: 813.31 toks/s, output: 3847.44 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 221/320 [00:32<00:53,  1.85it/s, est. speed input: 810.49 toks/s, output: 3858.92 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 223/320 [00:32<00:34,  2.81it/s, est. speed input: 814.06 toks/s, output: 3916.16 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 225/320 [00:32<00:29,  3.23it/s, est. speed input: 809.43 toks/s, output: 3941.09 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 226/320 [00:33<00:28,  3.27it/s, est. speed input: 809.94 toks/s, output: 3941.53 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 227/320 [00:34<00:44,  2.11it/s, est. speed input: 790.34 toks/s, output: 3863.36 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 229/320 [00:37<01:21,  1.12it/s, est. speed input: 731.07 toks/s, output: 3624.28 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 230/320 [00:37<01:13,  1.22it/s, est. speed input: 723.35 toks/s, output: 3611.53 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 231/320 [00:39<01:27,  1.02it/s, est. speed input: 698.89 toks/s, output: 3505.77 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 232/320 [00:39<01:09,  1.27it/s, est. speed input: 697.75 toks/s, output: 3527.80 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 233/320 [00:43<02:30,  1.73s/it, est. speed input: 630.92 toks/s, output: 3222.08 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 234/320 [00:44<02:06,  1.47s/it, est. speed input: 621.24 toks/s, output: 3205.18 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 235/320 [00:45<01:45,  1.25s/it, est. speed input: 613.54 toks/s, output: 3198.15 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 236/320 [00:47<02:08,  1.54s/it, est. speed input: 589.74 toks/s, output: 3087.69 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 237/320 [00:48<01:51,  1.34s/it, est. speed input: 582.38 toks/s, output: 3072.61 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 239/320 [00:52<02:17,  1.70s/it, est. speed input: 538.91 toks/s, output: 2903.09 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 240/320 [00:52<01:48,  1.36s/it, est. speed input: 538.22 toks/s, output: 2927.03 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 241/320 [00:54<01:51,  1.41s/it, est. speed input: 525.89 toks/s, output: 2883.33 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 242/320 [01:01<03:45,  2.89s/it, est. speed input: 468.74 toks/s, output: 2599.74 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 243/320 [01:18<08:48,  6.86s/it, est. speed input: 367.39 toks/s, output: 2071.51 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 316/320 [01:21<00:01,  3.81it/s, est. speed input: 462.73 toks/s, output: 4771.08 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 317/320 [01:22<00:00,  3.55it/s, est. speed input: 456.33 toks/s, output: 4731.87 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 318/320 [01:23<00:00,  3.37it/s, est. speed input: 453.42 toks/s, output: 4719.63 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 319/320 [01:24<00:00,  3.09it/s, est. speed input: 449.16 toks/s, output: 4698.26 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:24<00:00,  3.05it/s, est. speed input: 447.80 toks/s, output: 4711.07 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:24<00:00,  3.78it/s, est. speed input: 447.80 toks/s, output: 4711.07 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/320 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [00:00<00:00, 20097.59it/s]
{'num_samples': 40, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 0, 'acc': 25.0, 'total_acc': 24.375, 'pass_at_k_percent': {'1': 24.4, '8': 62.5}, 'pass_at_k_valid_counts': {'1': 40, '8': 40}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/amc23x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part7.jsonl
[2025-12-03 22:33:28] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/amc23x8  acc=25.0 pass_at_k={'1': 24.4, '8': 62.5}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [02:57<01:28, 88.37s/ds][Info] Sharding enabled: Process 7/8 handling range [210:240]
==================================================
data: aime24x8  ,remain samples: 30
{'idx': 210, 'id': 60, 'problem': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.', 'solution': '$\\frac{9}{s} + t = 4$ in hours and $\\frac{9}{s+2} + t = 2.4$ in hours.\nSubtracting the second equation from the first, we get, \n$\\frac{9}{s} - \\frac{9}{s+2} = 1.6$\nMultiplying by $(s)(s+2)$, we get \n$9s+18-9s=18=1.6s^{2} + 3.2s$\nMultiplying by 5/2 on both sides, we get\n$0 = 4s^{2} + 8s - 45$\nFactoring gives us \n$(2s-5)(2s+9) = 0$, of which the solution we want is $s=2.5$.\nSubstituting this back to the first equation, we can find that $t = 0.4$ hours.\nLastly, $s + \\frac{1}{2} = 3$ kilometers per hour, so\n$\\frac{9}{3} + 0.4 = 3.4$ hours, or $\\framebox{204}$ minutes\n-Failure.net\nThe amount of hours spent while walking on the first travel is $\\frac{240-t}{6}$. Thus, we have the equation $(240-t)(s) = 540$, and by the same logic, the second equation yields $(144-t)(s+2) = 540$. We have $240s-st = 540$, and $288+144s-2t-st = 540$. We subtract the two equations to get $96s+2t-288 = 0$, so we have $48s+t = 144$, so $t = 144-48s$, and now we have $(96+48s)(s) = 540$. The numerator of $s$ must evenly divide 540, however, $s$ must be less than 3. We can guess that $s = 2.5$. Now, $2.5+0.5 = 3$. Taking $\\frac{9}{3} = 3$, we find that it will take three hours for the 9 kilometers to be traveled. The t minutes spent at the coffeeshop can be written as $144-48(2.5)$, so t = 24. $180 + 24 = 204$. -sepehr2010', 'answer': '204', 'url': 'https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems/Problem_1', 'question': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 463.29it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:05<23:11,  5.82s/it, est. speed input: 27.49 toks/s, output: 44.67 toks/s][A
Processed prompts:   1%|          | 2/240 [00:06<11:24,  2.87s/it, est. speed input: 37.99 toks/s, output: 83.37 toks/s][A
Processed prompts:   1%|‚ñè         | 3/240 [00:06<06:31,  1.65s/it, est. speed input: 49.94 toks/s, output: 125.22 toks/s][A
Processed prompts:   2%|‚ñè         | 4/240 [00:06<04:11,  1.07s/it, est. speed input: 61.44 toks/s, output: 166.32 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:07<03:17,  1.19it/s, est. speed input: 69.77 toks/s, output: 200.42 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:07<02:37,  1.49it/s, est. speed input: 91.85 toks/s, output: 235.21 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:07<01:54,  2.03it/s, est. speed input: 101.66 toks/s, output: 275.27 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:08<01:34,  2.45it/s, est. speed input: 111.76 toks/s, output: 311.17 toks/s][A
Processed prompts:   4%|‚ñç         | 9/240 [00:08<01:35,  2.42it/s, est. speed input: 117.29 toks/s, output: 339.02 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:08<01:14,  3.08it/s, est. speed input: 129.05 toks/s, output: 377.60 toks/s][A
Processed prompts:   5%|‚ñç         | 11/240 [00:08<00:58,  3.90it/s, est. speed input: 138.37 toks/s, output: 416.46 toks/s][A
Processed prompts:   5%|‚ñå         | 13/240 [00:09<00:43,  5.26it/s, est. speed input: 170.69 toks/s, output: 491.54 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:09<00:43,  5.17it/s, est. speed input: 188.18 toks/s, output: 523.77 toks/s][A
Processed prompts:   6%|‚ñã         | 15/240 [00:09<00:42,  5.28it/s, est. speed input: 205.48 toks/s, output: 556.85 toks/s][A
Processed prompts:   8%|‚ñä         | 18/240 [00:09<00:24,  8.99it/s, est. speed input: 237.72 toks/s, output: 678.20 toks/s][A
Processed prompts:   8%|‚ñä         | 20/240 [00:09<00:29,  7.52it/s, est. speed input: 246.46 toks/s, output: 738.36 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:10<00:29,  7.41it/s, est. speed input: 267.79 toks/s, output: 802.79 toks/s][A
Processed prompts:  10%|‚ñâ         | 23/240 [00:10<00:32,  6.78it/s, est. speed input: 271.45 toks/s, output: 829.89 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:10<00:24,  8.80it/s, est. speed input: 296.65 toks/s, output: 907.26 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:10<00:31,  6.80it/s, est. speed input: 305.29 toks/s, output: 956.01 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 29/240 [00:11<00:33,  6.23it/s, est. speed input: 322.40 toks/s, output: 1007.85 toks/s][A
Processed prompts:  12%|‚ñà‚ñé        | 30/240 [00:11<00:31,  6.69it/s, est. speed input: 331.32 toks/s, output: 1041.30 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 32/240 [00:11<00:23,  8.71it/s, est. speed input: 354.76 toks/s, output: 1116.97 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 34/240 [00:11<00:21,  9.81it/s, est. speed input: 369.39 toks/s, output: 1187.21 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 36/240 [00:11<00:19, 10.26it/s, est. speed input: 383.81 toks/s, output: 1253.70 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 39/240 [00:12<00:19, 10.49it/s, est. speed input: 407.58 toks/s, output: 1350.36 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 41/240 [00:12<00:16, 12.09it/s, est. speed input: 428.53 toks/s, output: 1423.57 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 43/240 [00:12<00:19, 10.13it/s, est. speed input: 442.84 toks/s, output: 1476.41 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 45/240 [00:12<00:19, 10.08it/s, est. speed input: 457.24 toks/s, output: 1537.06 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 47/240 [00:12<00:17, 10.83it/s, est. speed input: 476.91 toks/s, output: 1603.09 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 50/240 [00:13<00:17, 10.87it/s, est. speed input: 494.83 toks/s, output: 1694.22 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñé       | 54/240 [00:13<00:13, 13.46it/s, est. speed input: 525.92 toks/s, output: 1835.92 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 56/240 [00:13<00:17, 10.55it/s, est. speed input: 540.03 toks/s, output: 1875.70 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 58/240 [00:13<00:15, 11.96it/s, est. speed input: 553.90 toks/s, output: 1946.16 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 60/240 [00:14<00:19,  9.02it/s, est. speed input: 565.10 toks/s, output: 1977.45 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 62/240 [00:14<00:17,  9.91it/s, est. speed input: 576.19 toks/s, output: 2040.27 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 66/240 [00:14<00:12, 13.69it/s, est. speed input: 602.16 toks/s, output: 2185.91 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 68/240 [00:14<00:12, 13.65it/s, est. speed input: 613.38 toks/s, output: 2247.10 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 70/240 [00:14<00:12, 13.64it/s, est. speed input: 623.46 toks/s, output: 2308.32 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 72/240 [00:14<00:12, 13.05it/s, est. speed input: 637.01 toks/s, output: 2365.05 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 74/240 [00:14<00:11, 13.83it/s, est. speed input: 648.67 toks/s, output: 2429.44 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 76/240 [00:15<00:11, 13.81it/s, est. speed input: 656.26 toks/s, output: 2489.66 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 78/240 [00:15<00:12, 13.16it/s, est. speed input: 667.56 toks/s, output: 2545.86 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:15<00:15, 10.41it/s, est. speed input: 690.08 toks/s, output: 2581.71 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 83/240 [00:15<00:12, 12.39it/s, est. speed input: 720.78 toks/s, output: 2678.88 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 85/240 [00:16<00:21,  7.32it/s, est. speed input: 711.92 toks/s, output: 2666.79 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 87/240 [00:16<00:24,  6.27it/s, est. speed input: 705.88 toks/s, output: 2680.61 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 88/240 [00:16<00:24,  6.21it/s, est. speed input: 710.40 toks/s, output: 2695.54 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 89/240 [00:17<00:30,  4.88it/s, est. speed input: 701.75 toks/s, output: 2676.73 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 90/240 [00:17<00:28,  5.35it/s, est. speed input: 702.28 toks/s, output: 2699.73 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 91/240 [00:17<00:25,  5.82it/s, est. speed input: 703.16 toks/s, output: 2722.87 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 92/240 [00:17<00:24,  6.05it/s, est. speed input: 704.84 toks/s, output: 2742.28 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 93/240 [00:18<00:47,  3.11it/s, est. speed input: 680.87 toks/s, output: 2671.36 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 94/240 [00:19<00:59,  2.44it/s, est. speed input: 664.81 toks/s, output: 2623.16 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 95/240 [00:19<00:51,  2.80it/s, est. speed input: 678.34 toks/s, output: 2634.47 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:19<00:37,  3.82it/s, est. speed input: 680.67 toks/s, output: 2684.88 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 101/240 [00:20<00:26,  5.24it/s, est. speed input: 686.56 toks/s, output: 2769.18 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/240 [00:20<00:16,  8.28it/s, est. speed input: 704.05 toks/s, output: 2914.86 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:20<00:21,  6.26it/s, est. speed input: 696.89 toks/s, output: 2919.73 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/240 [00:21<00:30,  4.36it/s, est. speed input: 678.07 toks/s, output: 2886.49 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 111/240 [00:21<00:25,  4.99it/s, est. speed input: 680.33 toks/s, output: 2937.04 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 113/240 [00:22<00:23,  5.30it/s, est. speed input: 679.31 toks/s, output: 2978.14 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/240 [00:22<00:15,  7.72it/s, est. speed input: 689.81 toks/s, output: 3114.45 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 119/240 [00:22<00:15,  7.85it/s, est. speed input: 691.62 toks/s, output: 3164.37 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 121/240 [00:23<00:30,  3.95it/s, est. speed input: 668.99 toks/s, output: 3082.78 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:24<00:36,  3.26it/s, est. speed input: 658.04 toks/s, output: 3049.10 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 123/240 [00:24<00:32,  3.57it/s, est. speed input: 658.57 toks/s, output: 3071.68 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:25<00:35,  3.22it/s, est. speed input: 654.47 toks/s, output: 3061.55 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:25<00:45,  2.51it/s, est. speed input: 640.61 toks/s, output: 3020.56 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:26<00:28,  3.93it/s, est. speed input: 648.83 toks/s, output: 3109.17 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:26<00:29,  3.82it/s, est. speed input: 657.11 toks/s, output: 3116.79 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 130/240 [00:26<00:31,  3.47it/s, est. speed input: 652.43 toks/s, output: 3114.31 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 131/240 [00:27<00:35,  3.06it/s, est. speed input: 645.00 toks/s, output: 3104.91 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 133/240 [00:27<00:30,  3.52it/s, est. speed input: 641.31 toks/s, output: 3139.24 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 134/240 [00:28<00:29,  3.63it/s, est. speed input: 642.35 toks/s, output: 3154.25 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 136/240 [00:28<00:24,  4.30it/s, est. speed input: 647.33 toks/s, output: 3201.78 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 137/240 [00:28<00:23,  4.45it/s, est. speed input: 646.95 toks/s, output: 3221.98 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:28<00:21,  4.70it/s, est. speed input: 647.12 toks/s, output: 3244.69 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:29<00:27,  3.63it/s, est. speed input: 642.60 toks/s, output: 3235.87 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 140/240 [00:29<00:27,  3.68it/s, est. speed input: 642.59 toks/s, output: 3249.50 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:30<00:34,  2.83it/s, est. speed input: 636.04 toks/s, output: 3230.60 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:30<00:38,  2.50it/s, est. speed input: 624.81 toks/s, output: 3218.37 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:31<00:39,  2.41it/s, est. speed input: 619.87 toks/s, output: 3213.54 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:31<00:39,  2.42it/s, est. speed input: 614.62 toks/s, output: 3215.13 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 146/240 [00:32<00:34,  2.71it/s, est. speed input: 613.15 toks/s, output: 3232.92 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 148/240 [00:32<00:23,  3.88it/s, est. speed input: 619.74 toks/s, output: 3295.55 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:33<00:43,  2.11it/s, est. speed input: 600.35 toks/s, output: 3222.78 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [00:33<00:34,  2.57it/s, est. speed input: 601.89 toks/s, output: 3252.34 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [00:33<00:31,  2.80it/s, est. speed input: 603.95 toks/s, output: 3269.02 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:34<00:44,  1.96it/s, est. speed input: 590.64 toks/s, output: 3226.08 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/240 [00:35<00:53,  1.63it/s, est. speed input: 581.16 toks/s, output: 3189.37 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155/240 [00:35<00:31,  2.74it/s, est. speed input: 585.84 toks/s, output: 3264.38 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [00:36<00:44,  1.90it/s, est. speed input: 571.84 toks/s, output: 3215.13 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 157/240 [00:39<01:20,  1.03it/s, est. speed input: 544.81 toks/s, output: 3072.31 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 158/240 [00:40<01:37,  1.19s/it, est. speed input: 523.11 toks/s, output: 2979.86 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 159/240 [00:41<01:26,  1.07s/it, est. speed input: 515.83 toks/s, output: 2969.40 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 160/240 [00:42<01:18,  1.03it/s, est. speed input: 508.94 toks/s, output: 2959.88 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 161/240 [00:44<01:38,  1.24s/it, est. speed input: 489.99 toks/s, output: 2874.67 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 163/240 [00:44<00:57,  1.34it/s, est. speed input: 493.02 toks/s, output: 2941.60 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 164/240 [00:44<00:48,  1.58it/s, est. speed input: 493.10 toks/s, output: 2965.90 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 165/240 [00:45<00:50,  1.48it/s, est. speed input: 487.12 toks/s, output: 2957.02 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 166/240 [00:47<01:10,  1.05it/s, est. speed input: 474.43 toks/s, output: 2892.76 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 167/240 [00:52<02:27,  2.02s/it, est. speed input: 434.28 toks/s, output: 2669.42 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 168/240 [01:15<09:37,  8.02s/it, est. speed input: 303.01 toks/s, output: 1889.64 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:15<00:00,  3.19it/s, est. speed input: 444.44 toks/s, output: 4826.25 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 20239.93it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 3.3, 'total_acc': 5.416666666666667, 'pass_at_k_percent': {'1': 5.4, '8': 23.3}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime24x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part7.jsonl
[2025-12-03 22:34:45] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime24x8  acc=3.3 pass_at_k={'1': 5.4, '8': 23.3}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [04:14<00:00, 83.16s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [04:14<00:00, 84.85s/ds]
[2025-12-03 22:34:45] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2  ÂæÖËØÑÊµã=['minerva_math', 'olympiadbench', 'math500']  T=0.0  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 7/8 handling range [238:272]
==================================================
data: minerva_math  ,remain samples: 34
{'problem': 'Determine the highest linear density of atoms (atoms/m) encountered in vanadium (V). Please format your answer as $n \\times 10^x$ where $n$ is to 2 decimal places.', 'solution': '\\[\n\\begin{aligned}\n&\\mathrm{V}: \\quad \\text { atomic weight }=50.94 \\mathrm{~g} / \\text { mole } \\\\\n&\\rho=5.8 \\mathrm{~g} / \\mathrm{cm}^{3}\n\\end{aligned}\n\\]\n$B C C$, so $n=2$\nThe highest density would be found in the [111] direction. To find "a":\n\\[\n\\begin{aligned}\n&\\frac{\\text { atomic weight }}{\\rho}=a^{3} \\frac{N_{A}}{n} \\rightarrow a^{3}=\\frac{50.94 \\times 2}{5.8 \\times 6.023 \\times 10^{23}} \\\\\n&a=3.08 \\times 10^{-8} \\mathrm{~cm}=3.08 \\times 10^{-10} \\mathrm{~m}\n\\end{aligned}\n\\]\nThe length in the [111] direction is $\\mathrm{a} \\sqrt{3}$, so there are:\n\\[\n\\begin{aligned}\n&2 \\text { atoms } / \\mathrm{a} \\sqrt{3}=2 \\text { atoms/ }\\left(3.08 \\times 10^{-10} \\mathrm{~m} \\times \\sqrt{3}\\right) \\\\\n&= \\boxed{3.75e9} \\text { atoms } / \\mathrm{m}\n\\end{aligned}\n\\]', 'type': 'Introduction to Solid State Chemistry (3.091 Fall 2010)', 'idx': 238}

  0%|          | 0/34 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:00<00:00, 12820.85it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/272 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/272 [00:04<19:58,  4.42s/it, est. speed input: 34.82 toks/s, output: 47.02 toks/s][A
Processed prompts:   3%|‚ñé         | 9/272 [00:04<01:46,  2.48it/s, est. speed input: 269.97 toks/s, output: 391.19 toks/s][A
Processed prompts:   6%|‚ñã         | 17/272 [00:05<00:54,  4.68it/s, est. speed input: 363.30 toks/s, output: 691.92 toks/s][A
Processed prompts:   9%|‚ñâ         | 25/272 [00:05<00:32,  7.65it/s, est. speed input: 541.15 toks/s, output: 1023.73 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 33/272 [00:05<00:22, 10.64it/s, est. speed input: 629.67 toks/s, output: 1330.57 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 41/272 [00:06<00:18, 12.76it/s, est. speed input: 764.34 toks/s, output: 1594.41 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 49/272 [00:06<00:15, 14.18it/s, est. speed input: 989.15 toks/s, output: 1837.97 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 57/272 [00:07<00:14, 14.70it/s, est. speed input: 1057.14 toks/s, output: 2057.51 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 65/272 [00:07<00:11, 18.46it/s, est. speed input: 1284.37 toks/s, output: 2363.33 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 73/272 [00:09<00:26,  7.44it/s, est. speed input: 1096.34 toks/s, output: 2059.19 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 89/272 [00:10<00:13, 13.33it/s, est. speed input: 1354.48 toks/s, output: 2758.57 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 113/272 [00:10<00:07, 22.57it/s, est. speed input: 1729.20 toks/s, output: 3735.99 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 119/272 [00:11<00:11, 13.54it/s, est. speed input: 1584.89 toks/s, output: 3537.96 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 126/272 [00:13<00:15,  9.38it/s, est. speed input: 1455.68 toks/s, output: 3394.06 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 134/272 [00:14<00:13, 10.41it/s, est. speed input: 1488.02 toks/s, output: 3626.35 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 142/272 [00:15<00:14,  9.05it/s, est. speed input: 1505.79 toks/s, output: 3681.59 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 150/272 [00:16<00:13,  9.31it/s, est. speed input: 1498.02 toks/s, output: 3853.83 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 158/272 [00:16<00:12,  9.48it/s, est. speed input: 1521.49 toks/s, output: 4027.12 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 166/272 [00:17<00:11,  9.16it/s, est. speed input: 1531.97 toks/s, output: 4170.59 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 174/272 [00:29<00:49,  1.97it/s, est. speed input: 988.90 toks/s, output: 2736.02 toks/s] [A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 182/272 [01:16<03:06,  2.08s/it, est. speed input: 407.90 toks/s, output: 1209.52 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 185/272 [01:16<02:36,  1.80s/it, est. speed input: 414.83 toks/s, output: 1327.13 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 265/272 [01:19<00:02,  3.08it/s, est. speed input: 617.05 toks/s, output: 4354.43 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 269/272 [01:19<00:00,  3.24it/s, est. speed input: 621.49 toks/s, output: 4499.52 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [01:20<00:00,  3.40it/s, est. speed input: 624.54 toks/s, output: 4605.98 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [01:20<00:00,  3.39it/s, est. speed input: 624.54 toks/s, output: 4605.98 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/272 [00:00<?, ?it/s][A
Evaluate:  39%|‚ñà‚ñà‚ñà‚ñä      | 105/272 [00:00<00:00, 315.16it/s][A
Evaluate:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 137/272 [00:01<00:01, 96.43it/s] [AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [00:01<00:00, 222.57it/s]
{'num_samples': 34, 'num_scores': 272, 'timeout_samples': 0, 'empty_samples': 0, 'acc': 2.9, 'total_acc': 2.941176470588235, 'pass_at_k_percent': {'1': 2.9, '8': 2.9}, 'pass_at_k_valid_counts': {'1': 34, '8': 34}, 'type_acc': {'Introduction to Solid State Chemistry (3.091 Fall 2010)': 0.0, 'Physical Chemistry (5.61 Fall 2017)': 0.0, 'Principles of Microeconomics (14.01 Fall 2011)': 5.6}, 'type_pass_at_k_percent': {'Introduction to Solid State Chemistry (3.091 Fall 2010)': {'1': 0.0, '8': 0.0}, 'Physical Chemistry (5.61 Fall 2017)': {'1': 0.0, '8': 0.0}, 'Principles of Microeconomics (14.01 Fall 2011)': {'1': 5.6, '8': 5.6}}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/minerva_math/test_think-boxed_-1_seed0_t0.0_s0_e-1_part7.jsonl
[2025-12-03 22:36:06] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/minerva_math  acc=2.9 pass_at_k={'1': 2.9, '8': 2.9}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:21<02:43, 81.66s/ds][Info] Sharding enabled: Process 7/8 handling range [588:675]
==================================================
data: olympiadbench  ,remain samples: 87
{'idx': 588, 'id': 2984, 'subfield': 'Algebra', 'context': None, 'question': 'Compute the value of\n\n$$\n\\sin \\left(6^{\\circ}\\right) \\cdot \\sin \\left(12^{\\circ}\\right) \\cdot \\sin \\left(24^{\\circ}\\right) \\cdot \\sin \\left(42^{\\circ}\\right)+\\sin \\left(12^{\\circ}\\right) \\cdot \\sin \\left(24^{\\circ}\\right) \\cdot \\sin \\left(42^{\\circ}\\right) \\text {. }\n$$', 'solution': ['Let $S=\\left(1+\\sin 6^{\\circ}\\right)\\left(\\sin 12^{\\circ} \\sin 24^{\\circ} \\sin 42^{\\circ}\\right)$. It follows from a sum-to-product identity that $1+\\sin 6^{\\circ}=$ $\\sin 90^{\\circ}+\\sin 6^{\\circ}=2 \\sin 48^{\\circ} \\cos 42^{\\circ}$. Because the sine of an angle is the cosine of its complement, it follows that\n\n$$\nS=\\left(2 \\sin 48^{\\circ} \\cos 42^{\\circ}\\right)\\left(\\sin 12^{\\circ} \\sin 24^{\\circ} \\sin 42^{\\circ}\\right)=2\\left(\\sin 48^{\\circ}\\right)^{2}\\left(\\sin 12^{\\circ} \\sin 24^{\\circ} \\cos 48^{\\circ}\\right)\n$$\n\nBy the double-angle formula, this means $S=\\sin 12^{\\circ} \\sin 24^{\\circ} \\sin 48^{\\circ} \\sin 96^{\\circ}$. By a product-to-sum identity,\n\n$$\n\\sin 12^{\\circ} \\sin 48^{\\circ}=\\frac{\\cos 36^{\\circ}-\\cos 60^{\\circ}}{2}=\\frac{\\sqrt{5}-1}{8}\n$$\n\n\n\nand\n\n$$\n\\sin 24^{\\circ} \\sin 96^{\\circ}=\\frac{\\cos 72^{\\circ}-\\cos 120^{\\circ}}{2}=\\frac{\\sqrt{5}+1}{8}\n$$\n\nMultiply the expressions on the right-hand sides of (1) and (2) to obtain $\\frac{\\mathbf{1}}{\\mathbf{1 6}}$'], 'final_answer': ['$\\frac{1}{16}$'], 'is_multiple_answer': False, 'unit': None, 'answer_type': 'Numerical', 'error': None}

  0%|          | 0/87 [00:00<?, ?it/s][A
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 45/87 [00:00<00:00, 440.27it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:00<00:00, 433.48it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/696 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/696 [00:00<03:12,  3.61it/s, est. speed input: 271.12 toks/s, output: 10.84 toks/s][A
Processed prompts:   1%|‚ñè         | 9/696 [00:04<06:18,  1.82it/s, est. speed input: 185.73 toks/s, output: 51.51 toks/s][A
Processed prompts:   2%|‚ñè         | 17/696 [00:05<02:54,  3.90it/s, est. speed input: 607.71 toks/s, output: 404.30 toks/s][A
Processed prompts:   4%|‚ñé         | 25/696 [00:06<02:12,  5.05it/s, est. speed input: 613.95 toks/s, output: 651.72 toks/s][A
Processed prompts:   5%|‚ñç         | 33/696 [00:06<01:42,  6.47it/s, est. speed input: 697.58 toks/s, output: 920.65 toks/s][A
Processed prompts:   6%|‚ñå         | 41/696 [00:08<01:55,  5.69it/s, est. speed input: 843.52 toks/s, output: 1034.10 toks/s][A
Processed prompts:   7%|‚ñã         | 47/696 [00:09<01:39,  6.54it/s, est. speed input: 993.37 toks/s, output: 1220.49 toks/s][A
Processed prompts:   9%|‚ñâ         | 63/696 [00:12<02:00,  5.25it/s, est. speed input: 1113.63 toks/s, output: 1386.45 toks/s][A
Processed prompts:  10%|‚ñà         | 71/696 [00:13<01:43,  6.02it/s, est. speed input: 1285.53 toks/s, output: 1623.61 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 79/696 [00:13<01:18,  7.90it/s, est. speed input: 1429.40 toks/s, output: 1903.52 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 85/696 [00:14<01:16,  7.97it/s, est. speed input: 1387.94 toks/s, output: 1908.34 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 93/696 [00:14<01:00,  9.92it/s, est. speed input: 1397.68 toks/s, output: 2185.93 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 101/696 [00:14<00:45, 12.94it/s, est. speed input: 1426.72 toks/s, output: 2484.99 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 108/696 [00:15<00:46, 12.63it/s, est. speed input: 1457.54 toks/s, output: 2670.11 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 116/696 [00:15<00:38, 15.18it/s, est. speed input: 1578.08 toks/s, output: 2943.29 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 124/696 [00:16<00:31, 17.91it/s, est. speed input: 1603.02 toks/s, output: 3215.66 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 132/696 [00:20<01:47,  5.24it/s, est. speed input: 1314.57 toks/s, output: 2826.82 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 138/696 [00:20<01:35,  5.84it/s, est. speed input: 1300.10 toks/s, output: 2857.60 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 140/696 [00:22<02:18,  4.02it/s, est. speed input: 1204.96 toks/s, output: 2656.81 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 145/696 [00:22<01:48,  5.06it/s, est. speed input: 1213.44 toks/s, output: 2685.83 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 147/696 [00:23<01:46,  5.17it/s, est. speed input: 1205.50 toks/s, output: 2680.63 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 151/696 [00:23<01:41,  5.35it/s, est. speed input: 1195.53 toks/s, output: 2679.64 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 159/696 [00:24<00:59,  9.02it/s, est. speed input: 1232.91 toks/s, output: 2949.23 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 163/696 [00:24<01:10,  7.58it/s, est. speed input: 1213.88 toks/s, output: 2911.42 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 167/696 [00:25<01:17,  6.87it/s, est. speed input: 1199.38 toks/s, output: 2882.74 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 171/696 [00:27<02:02,  4.27it/s, est. speed input: 1135.19 toks/s, output: 2740.81 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 177/696 [00:32<04:04,  2.13it/s, est. speed input: 966.56 toks/s, output: 2385.21 toks/s] [A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 180/696 [00:33<03:25,  2.51it/s, est. speed input: 967.68 toks/s, output: 2408.48 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 185/696 [00:33<02:40,  3.19it/s, est. speed input: 965.96 toks/s, output: 2439.46 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 188/696 [00:34<02:13,  3.80it/s, est. speed input: 967.81 toks/s, output: 2473.75 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 191/696 [00:34<02:12,  3.81it/s, est. speed input: 952.46 toks/s, output: 2470.88 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 193/696 [00:35<02:38,  3.18it/s, est. speed input: 928.24 toks/s, output: 2432.58 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 198/696 [00:37<02:38,  3.15it/s, est. speed input: 897.71 toks/s, output: 2412.18 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 201/696 [00:37<02:14,  3.68it/s, est. speed input: 895.79 toks/s, output: 2455.53 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 207/696 [00:39<02:24,  3.39it/s, est. speed input: 867.95 toks/s, output: 2521.28 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 213/696 [00:42<02:38,  3.05it/s, est. speed input: 834.73 toks/s, output: 2493.96 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 214/696 [00:43<03:15,  2.47it/s, est. speed input: 812.52 toks/s, output: 2436.34 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 221/696 [00:43<01:51,  4.28it/s, est. speed input: 825.19 toks/s, output: 2526.52 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 223/696 [00:45<02:41,  2.93it/s, est. speed input: 797.48 toks/s, output: 2489.93 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 230/696 [00:47<02:11,  3.53it/s, est. speed input: 790.72 toks/s, output: 2609.97 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 232/696 [00:49<03:22,  2.29it/s, est. speed input: 752.83 toks/s, output: 2493.99 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 236/696 [00:49<02:24,  3.19it/s, est. speed input: 759.45 toks/s, output: 2545.77 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 238/696 [00:54<05:03,  1.51it/s, est. speed input: 699.73 toks/s, output: 2395.35 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 239/696 [01:02<11:49,  1.55s/it, est. speed input: 606.97 toks/s, output: 2094.81 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 241/696 [01:02<09:01,  1.19s/it, est. speed input: 611.37 toks/s, output: 2143.68 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 243/696 [01:03<06:46,  1.12it/s, est. speed input: 615.84 toks/s, output: 2192.73 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 245/696 [01:03<05:06,  1.47it/s, est. speed input: 619.66 toks/s, output: 2239.27 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 246/696 [01:13<05:05,  1.47it/s, est. speed input: 621.71 toks/s, output: 2263.00 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 247/696 [01:42<43:57,  5.87s/it, est. speed input: 387.28 toks/s, output: 1434.70 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 248/696 [01:42<36:46,  4.92s/it, est. speed input: 388.17 toks/s, output: 1462.87 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 263/696 [01:42<07:55,  1.10s/it, est. speed input: 404.17 toks/s, output: 1908.10 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 280/696 [01:42<03:26,  2.02it/s, est. speed input: 421.73 toks/s, output: 2410.88 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 290/696 [01:43<02:20,  2.89it/s, est. speed input: 448.24 toks/s, output: 2702.35 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 306/696 [01:43<01:20,  4.87it/s, est. speed input: 462.45 toks/s, output: 3173.07 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 332/696 [01:43<00:39,  9.28it/s, est. speed input: 489.00 toks/s, output: 3857.79 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 340/696 [01:44<00:38,  9.14it/s, est. speed input: 492.43 toks/s, output: 3978.74 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 346/696 [01:46<00:55,  6.35it/s, est. speed input: 487.78 toks/s, output: 3989.75 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 350/696 [01:49<01:17,  4.44it/s, est. speed input: 479.38 toks/s, output: 3918.33 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 353/696 [01:49<01:11,  4.78it/s, est. speed input: 480.02 toks/s, output: 3919.33 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 359/696 [01:49<00:53,  6.28it/s, est. speed input: 483.36 toks/s, output: 3936.03 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 363/696 [01:50<00:47,  7.07it/s, est. speed input: 484.36 toks/s, output: 3935.38 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 369/696 [01:50<00:44,  7.36it/s, est. speed input: 484.45 toks/s, output: 3924.17 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 372/696 [01:51<00:56,  5.75it/s, est. speed input: 482.00 toks/s, output: 3944.40 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 374/696 [01:52<00:50,  6.37it/s, est. speed input: 482.87 toks/s, output: 3970.23 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 377/696 [01:52<00:41,  7.67it/s, est. speed input: 484.31 toks/s, output: 4002.32 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 379/696 [01:52<00:36,  8.61it/s, est. speed input: 485.21 toks/s, output: 4028.27 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 381/696 [01:52<00:36,  8.73it/s, est. speed input: 485.78 toks/s, output: 4075.07 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 383/696 [01:52<00:32,  9.66it/s, est. speed input: 486.56 toks/s, output: 4099.90 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 385/696 [01:53<00:41,  7.49it/s, est. speed input: 486.64 toks/s, output: 4090.38 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 387/696 [01:54<01:37,  3.17it/s, est. speed input: 480.95 toks/s, output: 4048.33 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 388/696 [01:56<03:03,  1.68it/s, est. speed input: 473.26 toks/s, output: 3979.79 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 389/696 [01:57<03:08,  1.63it/s, est. speed input: 471.43 toks/s, output: 3960.61 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 393/696 [01:58<01:50,  2.73it/s, est. speed input: 472.84 toks/s, output: 3960.24 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 394/696 [01:58<01:44,  2.88it/s, est. speed input: 472.81 toks/s, output: 3956.24 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 396/696 [01:58<01:27,  3.43it/s, est. speed input: 472.63 toks/s, output: 3953.01 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 397/696 [01:59<01:31,  3.28it/s, est. speed input: 472.18 toks/s, output: 3966.64 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 399/696 [01:59<01:07,  4.42it/s, est. speed input: 473.62 toks/s, output: 4013.32 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 400/696 [01:59<01:00,  4.87it/s, est. speed input: 474.15 toks/s, output: 4035.06 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 402/696 [01:59<00:48,  6.10it/s, est. speed input: 475.45 toks/s, output: 4080.45 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 404/696 [01:59<00:42,  6.90it/s, est. speed input: 476.60 toks/s, output: 4124.42 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 405/696 [02:00<01:31,  3.19it/s, est. speed input: 473.51 toks/s, output: 4098.27 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 410/696 [02:00<00:41,  6.88it/s, est. speed input: 475.72 toks/s, output: 4108.45 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 412/696 [02:01<00:40,  6.97it/s, est. speed input: 475.72 toks/s, output: 4104.89 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 414/696 [02:02<01:24,  3.34it/s, est. speed input: 471.44 toks/s, output: 4063.37 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 416/696 [02:03<01:23,  3.34it/s, est. speed input: 470.62 toks/s, output: 4051.82 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 424/696 [02:03<00:35,  7.61it/s, est. speed input: 474.72 toks/s, output: 4072.90 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 426/696 [02:03<00:32,  8.23it/s, est. speed input: 475.78 toks/s, output: 4078.77 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 428/696 [02:03<00:33,  7.95it/s, est. speed input: 476.17 toks/s, output: 4079.96 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 435/696 [02:04<00:21, 12.39it/s, est. speed input: 480.54 toks/s, output: 4107.54 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 437/696 [02:04<00:26,  9.79it/s, est. speed input: 481.33 toks/s, output: 4104.77 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 444/696 [02:05<00:24, 10.34it/s, est. speed input: 486.89 toks/s, output: 4120.64 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 446/696 [02:05<00:29,  8.38it/s, est. speed input: 486.53 toks/s, output: 4133.50 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 449/696 [02:05<00:24, 10.00it/s, est. speed input: 487.87 toks/s, output: 4202.55 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 451/696 [02:05<00:22, 10.76it/s, est. speed input: 488.60 toks/s, output: 4247.13 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 453/696 [02:06<00:23, 10.34it/s, est. speed input: 488.97 toks/s, output: 4288.41 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 455/696 [02:06<00:29,  8.20it/s, est. speed input: 489.16 toks/s, output: 4282.89 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 457/696 [02:07<00:57,  4.13it/s, est. speed input: 486.15 toks/s, output: 4250.06 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 460/696 [02:07<00:41,  5.70it/s, est. speed input: 488.20 toks/s, output: 4250.67 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 462/696 [02:08<00:35,  6.61it/s, est. speed input: 489.16 toks/s, output: 4250.54 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 464/696 [02:08<00:29,  7.80it/s, est. speed input: 490.64 toks/s, output: 4280.14 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 466/696 [02:08<00:27,  8.40it/s, est. speed input: 491.88 toks/s, output: 4307.59 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 475/696 [02:08<00:12, 17.76it/s, est. speed input: 503.24 toks/s, output: 4357.22 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 479/696 [02:08<00:13, 16.10it/s, est. speed input: 504.41 toks/s, output: 4356.71 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 485/696 [02:09<00:10, 19.27it/s, est. speed input: 506.94 toks/s, output: 4357.49 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 489/696 [02:09<00:20, 10.32it/s, est. speed input: 506.77 toks/s, output: 4336.76 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 491/696 [02:10<00:32,  6.34it/s, est. speed input: 504.43 toks/s, output: 4316.95 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 493/696 [02:11<00:38,  5.21it/s, est. speed input: 503.09 toks/s, output: 4304.08 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 498/696 [02:11<00:25,  7.83it/s, est. speed input: 508.30 toks/s, output: 4328.29 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 506/696 [02:12<00:19,  9.72it/s, est. speed input: 519.42 toks/s, output: 4358.45 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 508/696 [02:12<00:21,  8.74it/s, est. speed input: 519.15 toks/s, output: 4350.42 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 512/696 [02:13<00:21,  8.74it/s, est. speed input: 520.02 toks/s, output: 4344.26 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 514/696 [02:14<00:38,  4.78it/s, est. speed input: 516.91 toks/s, output: 4311.66 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 519/696 [02:14<00:25,  6.97it/s, est. speed input: 521.27 toks/s, output: 4343.67 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 527/696 [02:16<00:34,  4.90it/s, est. speed input: 518.22 toks/s, output: 4329.82 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 531/696 [02:17<00:30,  5.50it/s, est. speed input: 519.21 toks/s, output: 4350.78 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 533/696 [02:17<00:26,  6.11it/s, est. speed input: 520.41 toks/s, output: 4391.62 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 535/696 [02:17<00:24,  6.63it/s, est. speed input: 521.37 toks/s, output: 4430.42 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 537/696 [02:17<00:21,  7.54it/s, est. speed input: 522.57 toks/s, output: 4471.13 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 539/696 [02:20<01:06,  2.37it/s, est. speed input: 513.56 toks/s, output: 4411.17 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 540/696 [02:20<01:00,  2.57it/s, est. speed input: 513.75 toks/s, output: 4409.83 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 542/696 [02:21<00:57,  2.67it/s, est. speed input: 512.68 toks/s, output: 4397.25 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 549/696 [02:21<00:26,  5.61it/s, est. speed input: 515.40 toks/s, output: 4435.12 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 552/696 [02:21<00:20,  7.04it/s, est. speed input: 517.03 toks/s, output: 4481.20 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 554/696 [02:21<00:17,  7.95it/s, est. speed input: 517.76 toks/s, output: 4520.66 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 557/696 [02:22<00:13, 10.00it/s, est. speed input: 519.39 toks/s, output: 4566.70 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 560/696 [02:23<00:22,  5.92it/s, est. speed input: 517.98 toks/s, output: 4568.62 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 562/696 [02:23<00:26,  5.07it/s, est. speed input: 517.25 toks/s, output: 4560.65 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 565/696 [02:25<00:38,  3.44it/s, est. speed input: 514.33 toks/s, output: 4532.72 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 566/696 [02:26<00:58,  2.23it/s, est. speed input: 509.82 toks/s, output: 4496.48 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 574/696 [02:26<00:22,  5.31it/s, est. speed input: 514.81 toks/s, output: 4565.08 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 577/696 [02:29<00:44,  2.68it/s, est. speed input: 507.10 toks/s, output: 4511.77 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 580/696 [02:29<00:33,  3.49it/s, est. speed input: 508.89 toks/s, output: 4569.97 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 582/696 [02:29<00:28,  4.02it/s, est. speed input: 509.68 toks/s, output: 4605.11 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 585/696 [02:30<00:29,  3.70it/s, est. speed input: 508.53 toks/s, output: 4623.42 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 587/696 [02:32<00:38,  2.85it/s, est. speed input: 505.88 toks/s, output: 4603.61 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 588/696 [02:34<01:10,  1.52it/s, est. speed input: 498.20 toks/s, output: 4546.24 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 590/696 [02:36<01:13,  1.45it/s, est. speed input: 494.87 toks/s, output: 4529.92 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 591/696 [02:36<01:09,  1.52it/s, est. speed input: 494.13 toks/s, output: 4524.98 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 592/696 [02:38<01:27,  1.19it/s, est. speed input: 490.30 toks/s, output: 4490.93 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 599/696 [02:39<00:41,  2.31it/s, est. speed input: 493.11 toks/s, output: 4534.95 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 601/696 [02:41<00:48,  1.95it/s, est. speed input: 489.41 toks/s, output: 4527.70 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 603/696 [02:41<00:40,  2.32it/s, est. speed input: 489.25 toks/s, output: 4556.18 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 604/696 [02:42<00:38,  2.36it/s, est. speed input: 488.49 toks/s, output: 4564.24 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 605/696 [02:43<00:47,  1.93it/s, est. speed input: 485.94 toks/s, output: 4555.76 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 607/696 [02:44<00:45,  1.96it/s, est. speed input: 483.74 toks/s, output: 4565.44 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 610/696 [02:44<00:27,  3.13it/s, est. speed input: 484.48 toks/s, output: 4617.78 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 611/696 [02:49<01:38,  1.16s/it, est. speed input: 469.54 toks/s, output: 4487.55 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 612/696 [02:50<01:29,  1.07s/it, est. speed input: 468.03 toks/s, output: 4481.96 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 613/696 [02:53<02:00,  1.45s/it, est. speed input: 460.93 toks/s, output: 4426.12 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 615/696 [02:53<01:14,  1.08it/s, est. speed input: 461.81 toks/s, output: 4458.69 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 617/696 [02:54<00:59,  1.33it/s, est. speed input: 460.64 toks/s, output: 4471.31 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 619/696 [02:54<00:45,  1.71it/s, est. speed input: 460.36 toks/s, output: 4493.76 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 621/696 [02:54<00:30,  2.42it/s, est. speed input: 460.99 toks/s, output: 4526.17 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 623/696 [02:55<00:22,  3.30it/s, est. speed input: 461.55 toks/s, output: 4557.92 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 625/696 [02:55<00:24,  2.89it/s, est. speed input: 460.18 toks/s, output: 4569.92 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 627/696 [02:56<00:23,  2.88it/s, est. speed input: 459.35 toks/s, output: 4586.62 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 630/696 [02:56<00:15,  4.36it/s, est. speed input: 460.46 toks/s, output: 4634.88 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 632/696 [02:57<00:17,  3.70it/s, est. speed input: 459.49 toks/s, output: 4649.75 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 633/696 [02:59<00:33,  1.91it/s, est. speed input: 455.43 toks/s, output: 4619.63 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 634/696 [03:01<00:50,  1.22it/s, est. speed input: 450.80 toks/s, output: 4581.77 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 636/696 [03:05<01:19,  1.33s/it, est. speed input: 440.87 toks/s, output: 4501.13 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 640/696 [03:07<00:45,  1.22it/s, est. speed input: 439.84 toks/s, output: 4536.50 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 641/696 [03:08<00:51,  1.06it/s, est. speed input: 436.45 toks/s, output: 4513.88 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 644/696 [03:09<00:33,  1.55it/s, est. speed input: 436.71 toks/s, output: 4549.36 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 646/696 [03:09<00:24,  2.07it/s, est. speed input: 437.60 toks/s, output: 4579.15 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 649/696 [03:10<00:18,  2.58it/s, est. speed input: 437.56 toks/s, output: 4611.30 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 683/696 [03:10<00:00, 16.13it/s, est. speed input: 455.19 toks/s, output: 5153.66 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 688/696 [03:13<00:01,  6.22it/s, est. speed input: 450.03 toks/s, output: 5138.57 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 692/696 [03:16<00:00,  4.06it/s, est. speed input: 445.68 toks/s, output: 5123.94 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 695/696 [03:18<00:00,  3.67it/s, est. speed input: 444.59 toks/s, output: 5136.73 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 696/696 [03:18<00:00,  3.51it/s, est. speed input: 445.04 toks/s, output: 5150.32 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/696 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 696/696 [00:00<00:00, 9176.90it/s]
{'num_samples': 87, 'num_scores': 696, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 24.1, 'total_acc': 22.844827586206897, 'pass_at_k_percent': {'1': 22.8, '8': 26.4}, 'pass_at_k_valid_counts': {'1': 87, '8': 87}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/olympiadbench/test_think-boxed_-1_seed0_t0.0_s0_e-1_part7.jsonl
[2025-12-03 22:39:27] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/olympiadbench  acc=24.1 pass_at_k={'1': 22.8, '8': 26.4}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [04:42<02:31, 151.74s/ds][Info] Sharding enabled: Process 7/8 handling range [434:500]
==================================================
data: math500  ,remain samples: 66
{'idx': 434, 'problem': 'In a certain isosceles right triangle, the altitude to the hypotenuse has length $4\\sqrt{2}$.  What is the area of the triangle?', 'solution': 'In isosceles right triangle $\\triangle ABC$ below, $\\overline{AD}$ is the altitude to the hypotenuse.\n\n[asy]\nimport olympiad;\nunitsize(0.8inch);\npair A,B,C,D;\nA = (0,1);\nB= (1,0);\nC = -B;\nD = (0,0);\ndraw(A--B--C--A,linewidth(1));\ndraw(A--D,linewidth(0.8));\ndraw(rightanglemark(C,A,B,s=5));\ndraw(rightanglemark(C,D,A,s=5));\nlabel("$A$",A,N);\nlabel("$B$",B,S);\nlabel("$C$",C,S);\nlabel("$D$",D,S);\n[/asy]\n\nBecause $\\triangle ABC$ is an isosceles right triangle, $\\angle ABC = 45^\\circ$.  Since $\\angle ADB = 90^\\circ$, we know that $\\angle DAB = 45^\\circ$, so $\\triangle ABD$ is also a 45-45-90 triangle.  Similarly, $\\triangle ACD$ is a 45-45-90 triangle.  Therefore, $DB=DC = DA = 4\\sqrt{2}$, so $BC = BD+DC = 8\\sqrt{2}$, and  \\[[ABC] = \\frac{(AD)(BC)}{2} = \\frac{(4\\sqrt{2})(8\\sqrt{2})}{2} = \\boxed{32}.\\]', 'answer': '32', 'subject': 'Prealgebra', 'level': 5, 'unique_id': 'test/prealgebra/1640.json'}

  0%|          | 0/66 [00:00<?, ?it/s][A
 33%|‚ñà‚ñà‚ñà‚ñé      | 22/66 [00:00<00:00, 213.98it/s][A
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 45/66 [00:00<00:00, 219.67it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:00<00:00, 217.31it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/528 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   2%|‚ñè         | 9/528 [00:02<02:23,  3.62it/s, est. speed input: 222.24 toks/s, output: 60.79 toks/s][A
Processed prompts:   3%|‚ñé         | 17/528 [00:02<01:11,  7.16it/s, est. speed input: 347.67 toks/s, output: 430.11 toks/s][A
Processed prompts:   5%|‚ñç         | 25/528 [00:03<00:57,  8.80it/s, est. speed input: 435.65 toks/s, output: 684.89 toks/s][A
Processed prompts:   6%|‚ñã         | 33/528 [00:03<00:44, 11.17it/s, est. speed input: 537.74 toks/s, output: 973.19 toks/s][A
Processed prompts:  11%|‚ñà         | 57/528 [00:04<00:19, 23.77it/s, est. speed input: 956.65 toks/s, output: 2006.55 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 73/528 [00:04<00:13, 33.50it/s, est. speed input: 1148.03 toks/s, output: 2695.87 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 89/528 [00:04<00:09, 45.42it/s, est. speed input: 1366.42 toks/s, output: 3392.83 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 105/528 [00:04<00:10, 39.73it/s, est. speed input: 1637.75 toks/s, output: 3750.77 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 113/528 [00:05<00:17, 24.40it/s, est. speed input: 1519.06 toks/s, output: 3524.22 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 121/528 [00:06<00:17, 23.94it/s, est. speed input: 1650.40 toks/s, output: 3682.33 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 126/528 [00:07<00:30, 12.99it/s, est. speed input: 1442.75 toks/s, output: 3260.73 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 131/528 [00:07<00:27, 14.67it/s, est. speed input: 1496.40 toks/s, output: 3424.07 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 147/528 [00:07<00:15, 25.15it/s, est. speed input: 1635.55 toks/s, output: 3913.72 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 155/528 [00:07<00:13, 27.22it/s, est. speed input: 1786.67 toks/s, output: 4051.83 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 163/528 [00:08<00:16, 22.12it/s, est. speed input: 1726.49 toks/s, output: 4108.04 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 169/528 [00:08<00:17, 20.75it/s, est. speed input: 1722.00 toks/s, output: 4064.36 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 179/528 [00:10<00:27, 12.63it/s, est. speed input: 1569.36 toks/s, output: 3742.36 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 187/528 [00:10<00:28, 11.97it/s, est. speed input: 1504.90 toks/s, output: 3822.68 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 195/528 [00:11<00:31, 10.71it/s, est. speed input: 1438.21 toks/s, output: 3819.46 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 197/528 [00:12<00:30, 10.96it/s, est. speed input: 1433.04 toks/s, output: 3817.91 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 205/528 [00:12<00:25, 12.49it/s, est. speed input: 1427.27 toks/s, output: 3939.00 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 221/528 [00:12<00:13, 22.50it/s, est. speed input: 1543.73 toks/s, output: 4466.43 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 229/528 [00:13<00:16, 18.39it/s, est. speed input: 1516.63 toks/s, output: 4564.04 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 237/528 [00:13<00:12, 23.01it/s, est. speed input: 1551.83 toks/s, output: 4714.99 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 243/528 [00:14<00:17, 16.04it/s, est. speed input: 1499.89 toks/s, output: 4536.15 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 251/528 [00:14<00:17, 15.85it/s, est. speed input: 1493.11 toks/s, output: 4598.11 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 254/528 [00:15<00:22, 11.99it/s, est. speed input: 1446.67 toks/s, output: 4462.73 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 265/528 [00:16<00:21, 12.45it/s, est. speed input: 1437.52 toks/s, output: 4415.80 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 267/528 [00:16<00:21, 12.14it/s, est. speed input: 1439.58 toks/s, output: 4399.99 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 275/528 [00:17<00:24, 10.16it/s, est. speed input: 1450.04 toks/s, output: 4318.85 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 283/528 [00:17<00:20, 12.24it/s, est. speed input: 1477.72 toks/s, output: 4555.08 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 291/528 [00:18<00:24,  9.60it/s, est. speed input: 1437.71 toks/s, output: 4566.38 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 293/528 [00:19<00:30,  7.72it/s, est. speed input: 1396.28 toks/s, output: 4455.68 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 301/528 [00:20<00:26,  8.43it/s, est. speed input: 1379.88 toks/s, output: 4482.61 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 305/528 [00:21<00:28,  7.76it/s, est. speed input: 1358.87 toks/s, output: 4398.98 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 309/528 [00:21<00:27,  7.96it/s, est. speed input: 1357.08 toks/s, output: 4394.59 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 323/528 [00:21<00:13, 15.58it/s, est. speed input: 1395.31 toks/s, output: 4814.96 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 327/528 [00:21<00:12, 16.43it/s, est. speed input: 1400.78 toks/s, output: 4859.26 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 339/528 [00:23<00:17, 10.55it/s, est. speed input: 1354.02 toks/s, output: 4851.65 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 343/528 [00:23<00:17, 10.64it/s, est. speed input: 1351.15 toks/s, output: 4854.06 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 349/528 [00:24<00:14, 12.48it/s, est. speed input: 1365.92 toks/s, output: 4975.93 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 355/528 [00:24<00:11, 14.89it/s, est. speed input: 1376.47 toks/s, output: 5080.38 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 358/528 [00:25<00:23,  7.36it/s, est. speed input: 1312.34 toks/s, output: 4847.35 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 361/528 [00:27<00:32,  5.11it/s, est. speed input: 1259.50 toks/s, output: 4663.08 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 363/528 [00:34<02:04,  1.32it/s, est. speed input: 988.19 toks/s, output: 3674.56 toks/s] [A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 367/528 [00:36<01:43,  1.56it/s, est. speed input: 959.93 toks/s, output: 3613.24 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 371/528 [00:36<01:12,  2.16it/s, est. speed input: 965.38 toks/s, output: 3688.08 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 373/528 [00:37<01:12,  2.15it/s, est. speed input: 947.43 toks/s, output: 3649.61 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 377/528 [00:52<03:55,  1.56s/it, est. speed input: 687.67 toks/s, output: 2695.10 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 385/528 [00:56<02:31,  1.06s/it, est. speed input: 648.66 toks/s, output: 2684.47 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 393/528 [01:31<05:28,  2.43s/it, est. speed input: 406.77 toks/s, output: 1846.28 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 399/528 [01:32<03:39,  1.70s/it, est. speed input: 412.21 toks/s, output: 2039.94 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 435/528 [01:37<00:53,  1.73it/s, est. speed input: 444.19 toks/s, output: 3059.41 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 443/528 [01:37<00:40,  2.10it/s, est. speed input: 450.62 toks/s, output: 3302.10 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 451/528 [01:38<00:29,  2.63it/s, est. speed input: 470.96 toks/s, output: 3548.24 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 467/528 [01:38<00:14,  4.15it/s, est. speed input: 491.19 toks/s, output: 4043.22 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 475/528 [01:40<00:12,  4.08it/s, est. speed input: 490.03 toks/s, output: 4204.77 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 481/528 [01:41<00:11,  4.14it/s, est. speed input: 491.20 toks/s, output: 4330.19 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 485/528 [01:41<00:09,  4.78it/s, est. speed input: 494.09 toks/s, output: 4446.35 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 488/528 [01:41<00:07,  5.22it/s, est. speed input: 495.47 toks/s, output: 4526.16 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 491/528 [01:47<00:17,  2.14it/s, est. speed input: 471.91 toks/s, output: 4376.15 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 493/528 [01:48<00:16,  2.09it/s, est. speed input: 468.96 toks/s, output: 4388.82 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 497/528 [01:48<00:11,  2.79it/s, est. speed input: 471.64 toks/s, output: 4493.44 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 499/528 [01:50<00:12,  2.25it/s, est. speed input: 466.20 toks/s, output: 4477.76 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 501/528 [01:51<00:12,  2.14it/s, est. speed input: 463.77 toks/s, output: 4487.43 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 503/528 [01:52<00:11,  2.20it/s, est. speed input: 463.08 toks/s, output: 4509.68 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 507/528 [01:53<00:07,  2.75it/s, est. speed input: 465.11 toks/s, output: 4583.63 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 511/528 [01:54<00:05,  3.22it/s, est. speed input: 465.73 toks/s, output: 4657.13 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 515/528 [01:55<00:03,  3.35it/s, est. speed input: 464.93 toks/s, output: 4719.38 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 519/528 [01:56<00:02,  3.82it/s, est. speed input: 465.36 toks/s, output: 4794.97 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 521/528 [01:56<00:01,  3.98it/s, est. speed input: 464.91 toks/s, output: 4830.93 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 524/528 [01:56<00:00,  5.26it/s, est. speed input: 466.22 toks/s, output: 4905.10 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 527/528 [01:58<00:00,  3.65it/s, est. speed input: 462.47 toks/s, output: 4923.70 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528/528 [01:58<00:00,  4.47it/s, est. speed input: 463.11 toks/s, output: 4948.69 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/528 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528/528 [00:00<00:00, 13136.82it/s]
{'num_samples': 66, 'num_scores': 528, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 48.5, 'total_acc': 48.484848484848484, 'pass_at_k_percent': {'1': 48.5, '8': 50.0}, 'pass_at_k_valid_counts': {'1': 66, '8': 66}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/math500/test_think-boxed_-1_seed0_t0.0_s0_e-1_part7.jsonl
[2025-12-03 22:41:27] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/math500  acc=48.5 pass_at_k={'1': 48.5, '8': 50.0}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [06:42<00:00, 137.27s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [06:42<00:00, 134.17s/ds]
[2025-12-03 22:41:27] ‚úÖ ÂÆåÊàêÔºöB_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200Ôºàg1+g2 Áº∫Â§±Êï∞ÊçÆÈõÜÂ∑≤Ë°•ÂÖ®Ôºâ
[2025-12-03 22:41:29] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:41:29 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:41:29 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:41:29 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:41:35 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:41:41 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:41:41 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1e64fa9ea0>
INFO 12-03 22:42:17 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:42:17 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:42:17 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:42:17 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]

INFO 12-03 22:42:19 [loader.py:458] Loading weights took 1.98 seconds
INFO 12-03 22:42:19 [gpu_model_runner.py:1347] Model loading took 6.0160 GiB and 2.102884 seconds
INFO 12-03 22:42:20 [kv_cache_utils.py:634] GPU KV cache size: 258,240 tokens
INFO 12-03 22:42:20 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.97x
INFO 12-03 22:42:20 [core.py:159] init engine (profile, create kv cache, warmup model) took 0.80 seconds
INFO 12-03 22:42:20 [core_client.py:439] Core engine process 0 ready.
[2025-12-03 22:42:20] ‚ÑπÔ∏è  ÂΩìÂâçÂ∑•‰ΩúËäÇÁÇπÂàÜÁâá: 7/8
[2025-12-03 22:42:20] ‚úì Ê®°ÂûãÂ∞±Áª™ÔºåÂºÄÂßãËØÑÊµã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300ÔºàÂÖ±‰∫´Âêå‰∏Ä LLMÔºå‰ªÖË°•Áº∫Êï∞ÊçÆÈõÜÔºâ
[2025-12-03 22:42:20] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1  ÂæÖËØÑÊµã=['aime25x8', 'amc23x8', 'aime24x8']  T=0.6  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 7/8 handling range [210:240]
==================================================
data: aime25x8  ,remain samples: 30
{'idx': 210, 'problem': 'Find the sum of all integer bases $b>9$ for which $17_{b}$ is a divisor of $97_{b}$.', 'answer': '70'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 472.66it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:05<20:40,  5.19s/it, est. speed input: 33.13 toks/s, output: 46.61 toks/s][A
Processed prompts:   1%|          | 2/240 [00:06<12:02,  3.04s/it, est. speed input: 40.04 toks/s, output: 81.43 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:06<03:36,  1.09it/s, est. speed input: 94.83 toks/s, output: 212.94 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:07<02:51,  1.36it/s, est. speed input: 108.92 toks/s, output: 252.65 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:07<01:45,  2.19it/s, est. speed input: 134.18 toks/s, output: 337.29 toks/s][A
Processed prompts:   4%|‚ñç         | 9/240 [00:07<01:40,  2.30it/s, est. speed input: 140.51 toks/s, output: 366.29 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:07<00:43,  5.15it/s, est. speed input: 224.03 toks/s, output: 576.15 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:08<00:47,  4.73it/s, est. speed input: 236.43 toks/s, output: 629.50 toks/s][A
Processed prompts:   7%|‚ñã         | 17/240 [00:08<00:44,  4.97it/s, est. speed input: 243.82 toks/s, output: 663.18 toks/s][A
Processed prompts:   8%|‚ñä         | 20/240 [00:09<00:45,  4.85it/s, est. speed input: 264.36 toks/s, output: 744.86 toks/s][A
Processed prompts:   9%|‚ñâ         | 21/240 [00:09<00:43,  5.08it/s, est. speed input: 271.15 toks/s, output: 777.22 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:09<00:40,  5.44it/s, est. speed input: 279.24 toks/s, output: 811.04 toks/s][A
Processed prompts:  10%|‚ñâ         | 23/240 [00:09<00:40,  5.35it/s, est. speed input: 284.13 toks/s, output: 838.54 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:09<00:33,  6.45it/s, est. speed input: 296.79 toks/s, output: 908.59 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 28/240 [00:10<00:22,  9.26it/s, est. speed input: 321.88 toks/s, output: 1026.13 toks/s][A
Processed prompts:  12%|‚ñà‚ñé        | 30/240 [00:10<00:35,  5.90it/s, est. speed input: 326.53 toks/s, output: 1052.08 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:10<00:33,  6.15it/s, est. speed input: 331.67 toks/s, output: 1082.91 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 32/240 [00:10<00:33,  6.22it/s, est. speed input: 337.89 toks/s, output: 1111.42 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 33/240 [00:11<00:30,  6.77it/s, est. speed input: 359.74 toks/s, output: 1144.65 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 36/240 [00:11<00:23,  8.82it/s, est. speed input: 390.47 toks/s, output: 1250.21 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 38/240 [00:11<00:18, 10.67it/s, est. speed input: 404.50 toks/s, output: 1325.48 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 40/240 [00:11<00:26,  7.42it/s, est. speed input: 404.66 toks/s, output: 1360.40 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 42/240 [00:12<00:24,  8.05it/s, est. speed input: 431.13 toks/s, output: 1423.41 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 44/240 [00:12<00:28,  6.94it/s, est. speed input: 451.24 toks/s, output: 1465.92 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 46/240 [00:12<00:23,  8.35it/s, est. speed input: 464.51 toks/s, output: 1536.88 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 48/240 [00:12<00:19,  9.76it/s, est. speed input: 485.24 toks/s, output: 1607.61 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñé       | 54/240 [00:13<00:16, 11.25it/s, est. speed input: 517.34 toks/s, output: 1801.67 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 57/240 [00:13<00:14, 12.90it/s, est. speed input: 536.87 toks/s, output: 1909.63 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 59/240 [00:13<00:14, 12.20it/s, est. speed input: 554.03 toks/s, output: 1966.77 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 61/240 [00:14<00:31,  5.62it/s, est. speed input: 545.63 toks/s, output: 1916.54 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 63/240 [00:14<00:25,  6.85it/s, est. speed input: 566.71 toks/s, output: 1988.44 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 65/240 [00:14<00:25,  6.80it/s, est. speed input: 572.47 toks/s, output: 2032.85 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 67/240 [00:15<00:23,  7.26it/s, est. speed input: 579.74 toks/s, output: 2086.85 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 69/240 [00:15<00:19,  8.58it/s, est. speed input: 589.57 toks/s, output: 2154.80 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 71/240 [00:15<00:18,  9.21it/s, est. speed input: 597.70 toks/s, output: 2214.50 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 73/240 [00:15<00:17,  9.41it/s, est. speed input: 615.89 toks/s, output: 2270.56 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà‚ñè      | 75/240 [00:15<00:16, 10.30it/s, est. speed input: 623.49 toks/s, output: 2333.60 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 77/240 [00:16<00:19,  8.33it/s, est. speed input: 626.24 toks/s, output: 2367.28 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:16<00:14, 10.78it/s, est. speed input: 660.50 toks/s, output: 2472.21 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 82/240 [00:16<00:23,  6.61it/s, est. speed input: 650.44 toks/s, output: 2465.30 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 84/240 [00:17<00:22,  6.79it/s, est. speed input: 653.58 toks/s, output: 2510.04 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 86/240 [00:17<00:18,  8.34it/s, est. speed input: 662.94 toks/s, output: 2579.67 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 88/240 [00:17<00:18,  8.25it/s, est. speed input: 665.71 toks/s, output: 2627.08 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 90/240 [00:17<00:17,  8.72it/s, est. speed input: 671.82 toks/s, output: 2681.84 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 92/240 [00:17<00:15,  9.40it/s, est. speed input: 687.28 toks/s, output: 2740.11 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 94/240 [00:18<00:14, 10.35it/s, est. speed input: 696.32 toks/s, output: 2801.89 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 96/240 [00:18<00:13, 10.71it/s, est. speed input: 719.20 toks/s, output: 2859.34 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:18<00:18,  7.64it/s, est. speed input: 720.57 toks/s, output: 2875.83 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 100/240 [00:19<00:21,  6.61it/s, est. speed input: 717.12 toks/s, output: 2899.18 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 101/240 [00:20<00:42,  3.25it/s, est. speed input: 683.08 toks/s, output: 2790.50 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/240 [00:20<00:43,  3.16it/s, est. speed input: 676.19 toks/s, output: 2784.65 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/240 [00:20<00:26,  5.08it/s, est. speed input: 688.60 toks/s, output: 2886.56 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 106/240 [00:20<00:29,  4.57it/s, est. speed input: 685.67 toks/s, output: 2884.45 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:21<00:28,  4.67it/s, est. speed input: 689.39 toks/s, output: 2899.49 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/240 [00:21<00:29,  4.44it/s, est. speed input: 687.65 toks/s, output: 2916.66 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 111/240 [00:21<00:24,  5.27it/s, est. speed input: 698.03 toks/s, output: 2967.29 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 112/240 [00:22<00:27,  4.64it/s, est. speed input: 692.63 toks/s, output: 2966.44 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 113/240 [00:22<00:32,  3.90it/s, est. speed input: 689.73 toks/s, output: 2955.42 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 114/240 [00:22<00:28,  4.40it/s, est. speed input: 691.20 toks/s, output: 2979.79 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 115/240 [00:23<00:45,  2.74it/s, est. speed input: 673.04 toks/s, output: 2924.56 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 116/240 [00:23<00:36,  3.36it/s, est. speed input: 680.68 toks/s, output: 2952.55 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/240 [00:23<00:35,  3.45it/s, est. speed input: 677.26 toks/s, output: 2961.01 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 119/240 [00:24<00:25,  4.72it/s, est. speed input: 683.20 toks/s, output: 3017.02 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/240 [00:24<00:30,  3.88it/s, est. speed input: 678.35 toks/s, output: 3009.18 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 121/240 [00:25<00:39,  2.99it/s, est. speed input: 667.80 toks/s, output: 2984.39 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:25<00:25,  4.58it/s, est. speed input: 680.81 toks/s, output: 3070.05 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:25<00:26,  4.28it/s, est. speed input: 677.35 toks/s, output: 3077.11 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 127/240 [00:26<00:26,  4.33it/s, est. speed input: 680.36 toks/s, output: 3107.52 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:26<00:20,  5.47it/s, est. speed input: 690.28 toks/s, output: 3170.33 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 130/240 [00:26<00:19,  5.62it/s, est. speed input: 688.61 toks/s, output: 3193.69 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 131/240 [00:26<00:19,  5.46it/s, est. speed input: 692.20 toks/s, output: 3211.84 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 133/240 [00:26<00:16,  6.46it/s, est. speed input: 693.45 toks/s, output: 3270.00 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 134/240 [00:29<01:09,  1.52it/s, est. speed input: 639.57 toks/s, output: 3035.79 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:30<01:13,  1.43it/s, est. speed input: 626.92 toks/s, output: 2993.47 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 136/240 [00:31<01:16,  1.37it/s, est. speed input: 614.05 toks/s, output: 2956.30 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 137/240 [00:32<01:26,  1.19it/s, est. speed input: 599.03 toks/s, output: 2894.69 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:32<01:17,  1.31it/s, est. speed input: 592.95 toks/s, output: 2887.84 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:34<01:39,  1.01it/s, est. speed input: 569.78 toks/s, output: 2799.01 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 140/240 [00:35<01:54,  1.15s/it, est. speed input: 548.23 toks/s, output: 2720.07 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:40<03:46,  2.29s/it, est. speed input: 484.12 toks/s, output: 2424.99 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 142/240 [00:44<04:29,  2.75s/it, est. speed input: 445.23 toks/s, output: 2257.49 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:50<05:52,  3.64s/it, est. speed input: 397.02 toks/s, output: 2041.60 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:59<08:26,  5.27s/it, est. speed input: 338.73 toks/s, output: 1768.41 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [01:00<06:04,  3.83s/it, est. speed input: 340.92 toks/s, output: 1795.77 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 146/240 [01:12<09:58,  6.37s/it, est. speed input: 285.26 toks/s, output: 1528.63 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [01:21<11:19,  7.31s/it, est. speed input: 252.94 toks/s, output: 1388.70 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 165/240 [01:22<01:04,  1.17it/s, est. speed input: 281.43 toks/s, output: 2055.86 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 202/240 [01:22<00:09,  4.11it/s, est. speed input: 349.83 toks/s, output: 3426.35 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 229/240 [01:23<00:01,  6.72it/s, est. speed input: 418.87 toks/s, output: 4398.59 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 234/240 [01:25<00:01,  5.55it/s, est. speed input: 415.87 toks/s, output: 4460.88 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 238/240 [01:27<00:00,  4.64it/s, est. speed input: 411.36 toks/s, output: 4495.67 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:28<00:00,  2.72it/s, est. speed input: 410.63 toks/s, output: 4529.40 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 24014.34it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 0, 'acc': 0.0, 'total_acc': 0.4166666666666667, 'pass_at_k_percent': {'1': 0.4, '8': 3.3}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1/aime25x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part7.jsonl
[2025-12-03 22:43:50] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1/aime25x8  acc=0.0 pass_at_k={'1': 0.4, '8': 3.3}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:29<02:58, 89.34s/ds][Info] Sharding enabled: Process 7/8 handling range [280:320]
==================================================
data: amc23x8  ,remain samples: 40
{'idx': 280, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}

  0%|          | 0/40 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:00<00:00, 460.11it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/320 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/320 [00:04<22:55,  4.31s/it, est. speed input: 27.37 toks/s, output: 46.39 toks/s][A
Processed prompts:   1%|          | 3/320 [00:04<06:22,  1.21s/it, est. speed input: 76.29 toks/s, output: 134.77 toks/s][A
Processed prompts:   1%|‚ñè         | 4/320 [00:04<04:35,  1.15it/s, est. speed input: 95.61 toks/s, output: 174.31 toks/s][A
Processed prompts:   2%|‚ñè         | 6/320 [00:04<02:26,  2.14it/s, est. speed input: 128.49 toks/s, output: 261.25 toks/s][A
Processed prompts:   2%|‚ñé         | 8/320 [00:05<01:40,  3.11it/s, est. speed input: 168.04 toks/s, output: 338.60 toks/s][A
Processed prompts:   3%|‚ñé         | 10/320 [00:05<01:31,  3.41it/s, est. speed input: 183.28 toks/s, output: 396.04 toks/s][A
Processed prompts:   4%|‚ñç         | 13/320 [00:05<00:55,  5.49it/s, est. speed input: 227.20 toks/s, output: 521.45 toks/s][A
Processed prompts:   5%|‚ñç         | 15/320 [00:06<00:54,  5.64it/s, est. speed input: 253.53 toks/s, output: 579.97 toks/s][A
Processed prompts:   5%|‚ñå         | 17/320 [00:06<00:48,  6.24it/s, est. speed input: 284.20 toks/s, output: 645.66 toks/s][A
Processed prompts:   6%|‚ñå         | 18/320 [00:06<00:53,  5.66it/s, est. speed input: 282.64 toks/s, output: 664.90 toks/s][A
Processed prompts:   7%|‚ñã         | 21/320 [00:06<00:39,  7.66it/s, est. speed input: 308.18 toks/s, output: 773.76 toks/s][A
Processed prompts:   7%|‚ñã         | 23/320 [00:07<00:37,  8.02it/s, est. speed input: 323.13 toks/s, output: 836.21 toks/s][A
Processed prompts:   8%|‚ñä         | 24/320 [00:07<00:36,  8.16it/s, est. speed input: 333.65 toks/s, output: 866.61 toks/s][A
Processed prompts:   8%|‚ñä         | 27/320 [00:07<00:27, 10.50it/s, est. speed input: 367.72 toks/s, output: 973.68 toks/s][A
Processed prompts:   9%|‚ñâ         | 29/320 [00:07<00:38,  7.54it/s, est. speed input: 369.19 toks/s, output: 1002.65 toks/s][A
Processed prompts:  10%|‚ñâ         | 31/320 [00:07<00:32,  8.87it/s, est. speed input: 391.10 toks/s, output: 1071.37 toks/s][A
Processed prompts:  10%|‚ñà         | 33/320 [00:08<00:33,  8.56it/s, est. speed input: 402.75 toks/s, output: 1121.42 toks/s][A
Processed prompts:  11%|‚ñà         | 35/320 [00:08<00:29,  9.68it/s, est. speed input: 415.80 toks/s, output: 1186.84 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 37/320 [00:08<00:26, 10.70it/s, est. speed input: 434.17 toks/s, output: 1251.39 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 39/320 [00:08<00:30,  9.29it/s, est. speed input: 445.38 toks/s, output: 1294.16 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 41/320 [00:08<00:25, 10.97it/s, est. speed input: 465.08 toks/s, output: 1361.94 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 43/320 [00:09<00:30,  9.07it/s, est. speed input: 469.45 toks/s, output: 1397.92 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 45/320 [00:09<00:30,  8.91it/s, est. speed input: 469.26 toks/s, output: 1445.45 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 47/320 [00:09<00:30,  8.96it/s, est. speed input: 472.78 toks/s, output: 1493.61 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 49/320 [00:10<00:38,  6.95it/s, est. speed input: 466.56 toks/s, output: 1508.74 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 50/320 [00:10<00:37,  7.24it/s, est. speed input: 477.05 toks/s, output: 1532.70 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 51/320 [00:10<00:38,  7.07it/s, est. speed input: 486.71 toks/s, output: 1549.91 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 54/320 [00:10<00:25, 10.32it/s, est. speed input: 508.72 toks/s, output: 1622.94 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 56/320 [00:10<00:37,  6.95it/s, est. speed input: 506.06 toks/s, output: 1627.11 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 57/320 [00:11<00:39,  6.69it/s, est. speed input: 504.74 toks/s, output: 1641.28 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 59/320 [00:11<00:35,  7.34it/s, est. speed input: 512.94 toks/s, output: 1688.27 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 61/320 [00:11<00:34,  7.56it/s, est. speed input: 530.05 toks/s, output: 1730.68 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 63/320 [00:11<00:30,  8.56it/s, est. speed input: 543.29 toks/s, output: 1758.32 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 65/320 [00:11<00:27,  9.38it/s, est. speed input: 556.72 toks/s, output: 1811.75 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 67/320 [00:12<00:28,  9.02it/s, est. speed input: 563.66 toks/s, output: 1854.36 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 68/320 [00:12<00:28,  8.70it/s, est. speed input: 571.64 toks/s, output: 1873.39 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 70/320 [00:12<00:23, 10.64it/s, est. speed input: 587.06 toks/s, output: 1935.29 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñé       | 72/320 [00:12<00:21, 11.76it/s, est. speed input: 598.09 toks/s, output: 1992.90 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 74/320 [00:12<00:19, 12.64it/s, est. speed input: 612.46 toks/s, output: 2050.01 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 76/320 [00:12<00:18, 13.30it/s, est. speed input: 623.86 toks/s, output: 2106.67 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 78/320 [00:13<00:32,  7.45it/s, est. speed input: 627.22 toks/s, output: 2099.87 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 81/320 [00:13<00:22, 10.54it/s, est. speed input: 647.76 toks/s, output: 2182.60 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 84/320 [00:13<00:18, 13.04it/s, est. speed input: 669.70 toks/s, output: 2277.74 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 86/320 [00:13<00:18, 12.89it/s, est. speed input: 675.97 toks/s, output: 2296.89 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 88/320 [00:13<00:18, 12.25it/s, est. speed input: 677.15 toks/s, output: 2343.80 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 90/320 [00:13<00:16, 13.57it/s, est. speed input: 698.75 toks/s, output: 2381.59 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 92/320 [00:14<00:27,  8.28it/s, est. speed input: 694.95 toks/s, output: 2355.74 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 96/320 [00:14<00:21, 10.34it/s, est. speed input: 721.78 toks/s, output: 2444.82 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 98/320 [00:14<00:20, 10.79it/s, est. speed input: 730.60 toks/s, output: 2475.79 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 101/320 [00:15<00:19, 11.36it/s, est. speed input: 743.09 toks/s, output: 2535.89 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 103/320 [00:15<00:18, 11.63it/s, est. speed input: 743.98 toks/s, output: 2587.21 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 106/320 [00:15<00:16, 12.94it/s, est. speed input: 754.66 toks/s, output: 2672.70 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 108/320 [00:15<00:21, 10.00it/s, est. speed input: 752.66 toks/s, output: 2693.19 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 110/320 [00:16<00:27,  7.58it/s, est. speed input: 741.10 toks/s, output: 2696.78 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 111/320 [00:16<00:27,  7.58it/s, est. speed input: 743.11 toks/s, output: 2698.58 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 113/320 [00:16<00:23,  8.98it/s, est. speed input: 747.26 toks/s, output: 2754.50 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 115/320 [00:16<00:20,  9.87it/s, est. speed input: 758.37 toks/s, output: 2790.49 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 117/320 [00:16<00:17, 11.58it/s, est. speed input: 767.54 toks/s, output: 2829.27 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 119/320 [00:17<00:19, 10.44it/s, est. speed input: 769.34 toks/s, output: 2867.41 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 121/320 [00:17<00:17, 11.07it/s, est. speed input: 776.16 toks/s, output: 2903.13 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 123/320 [00:17<00:20,  9.39it/s, est. speed input: 783.14 toks/s, output: 2932.72 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 126/320 [00:17<00:17, 11.31it/s, est. speed input: 793.69 toks/s, output: 2993.77 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 128/320 [00:18<00:23,  8.25it/s, est. speed input: 788.28 toks/s, output: 2982.42 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 130/320 [00:18<00:20,  9.16it/s, est. speed input: 796.21 toks/s, output: 2997.02 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 132/320 [00:18<00:23,  8.16it/s, est. speed input: 791.81 toks/s, output: 2999.57 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 134/320 [00:18<00:21,  8.81it/s, est. speed input: 793.75 toks/s, output: 3047.54 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 136/320 [00:18<00:20,  9.04it/s, est. speed input: 796.11 toks/s, output: 3070.54 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 139/320 [00:19<00:18,  9.55it/s, est. speed input: 803.67 toks/s, output: 3105.20 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 142/320 [00:19<00:14, 12.08it/s, est. speed input: 813.11 toks/s, output: 3176.66 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 144/320 [00:19<00:17, 10.21it/s, est. speed input: 812.35 toks/s, output: 3208.06 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 146/320 [00:19<00:16, 10.81it/s, est. speed input: 823.02 toks/s, output: 3250.28 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 148/320 [00:20<00:20,  8.33it/s, est. speed input: 815.82 toks/s, output: 3265.56 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 150/320 [00:20<00:20,  8.42it/s, est. speed input: 815.31 toks/s, output: 3285.41 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 152/320 [00:20<00:19,  8.77it/s, est. speed input: 818.41 toks/s, output: 3330.01 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 153/320 [00:20<00:19,  8.59it/s, est. speed input: 820.64 toks/s, output: 3348.21 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 155/320 [00:20<00:15, 10.51it/s, est. speed input: 826.20 toks/s, output: 3409.32 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 157/320 [00:21<00:19,  8.22it/s, est. speed input: 821.11 toks/s, output: 3408.20 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 159/320 [00:21<00:24,  6.60it/s, est. speed input: 815.85 toks/s, output: 3385.38 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 160/320 [00:21<00:25,  6.39it/s, est. speed input: 814.15 toks/s, output: 3377.13 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 162/320 [00:21<00:19,  8.31it/s, est. speed input: 818.76 toks/s, output: 3438.99 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 165/320 [00:22<00:21,  7.08it/s, est. speed input: 815.71 toks/s, output: 3464.10 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 166/320 [00:22<00:24,  6.23it/s, est. speed input: 812.00 toks/s, output: 3463.96 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 167/320 [00:23<00:37,  4.03it/s, est. speed input: 794.42 toks/s, output: 3394.77 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 168/320 [00:23<00:32,  4.61it/s, est. speed input: 796.62 toks/s, output: 3418.68 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 169/320 [00:23<00:29,  5.10it/s, est. speed input: 797.28 toks/s, output: 3438.77 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 170/320 [00:23<00:25,  5.80it/s, est. speed input: 797.06 toks/s, output: 3462.66 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 171/320 [00:23<00:28,  5.32it/s, est. speed input: 793.92 toks/s, output: 3450.63 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 173/320 [00:24<00:24,  5.96it/s, est. speed input: 792.91 toks/s, output: 3472.06 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 176/320 [00:24<00:16,  8.94it/s, est. speed input: 808.39 toks/s, output: 3552.46 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 181/320 [00:24<00:10, 12.65it/s, est. speed input: 822.97 toks/s, output: 3692.33 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 183/320 [00:25<00:23,  5.92it/s, est. speed input: 801.36 toks/s, output: 3634.16 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 185/320 [00:25<00:20,  6.58it/s, est. speed input: 803.82 toks/s, output: 3683.25 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 187/320 [00:25<00:20,  6.59it/s, est. speed input: 803.12 toks/s, output: 3702.45 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 189/320 [00:26<00:16,  8.07it/s, est. speed input: 809.46 toks/s, output: 3765.92 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 191/320 [00:26<00:23,  5.44it/s, est. speed input: 801.69 toks/s, output: 3734.13 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 193/320 [00:26<00:19,  6.63it/s, est. speed input: 808.59 toks/s, output: 3775.83 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 195/320 [00:27<00:17,  6.99it/s, est. speed input: 808.19 toks/s, output: 3818.93 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 197/320 [00:27<00:21,  5.64it/s, est. speed input: 801.12 toks/s, output: 3824.68 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 198/320 [00:27<00:21,  5.74it/s, est. speed input: 800.48 toks/s, output: 3827.04 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 199/320 [00:28<00:40,  2.99it/s, est. speed input: 776.35 toks/s, output: 3722.89 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 200/320 [00:28<00:36,  3.27it/s, est. speed input: 773.47 toks/s, output: 3735.70 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 202/320 [00:29<00:24,  4.77it/s, est. speed input: 777.42 toks/s, output: 3786.37 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 203/320 [00:29<00:27,  4.18it/s, est. speed input: 773.49 toks/s, output: 3769.95 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 204/320 [00:29<00:28,  4.04it/s, est. speed input: 768.78 toks/s, output: 3774.49 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 206/320 [00:30<00:37,  3.01it/s, est. speed input: 751.74 toks/s, output: 3739.62 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 207/320 [00:30<00:32,  3.52it/s, est. speed input: 753.23 toks/s, output: 3765.10 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 208/320 [00:31<00:39,  2.82it/s, est. speed input: 747.36 toks/s, output: 3735.70 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 210/320 [00:31<00:28,  3.90it/s, est. speed input: 747.94 toks/s, output: 3787.68 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 211/320 [00:32<00:36,  3.01it/s, est. speed input: 739.32 toks/s, output: 3757.79 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 212/320 [00:32<00:39,  2.72it/s, est. speed input: 736.21 toks/s, output: 3735.80 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 213/320 [00:33<00:58,  1.82it/s, est. speed input: 720.20 toks/s, output: 3650.50 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 214/320 [00:34<01:14,  1.41it/s, est. speed input: 703.98 toks/s, output: 3565.13 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 216/320 [00:34<00:44,  2.33it/s, est. speed input: 710.87 toks/s, output: 3630.89 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 217/320 [00:37<01:25,  1.20it/s, est. speed input: 672.08 toks/s, output: 3462.86 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 218/320 [00:38<01:36,  1.06it/s, est. speed input: 651.97 toks/s, output: 3389.18 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 219/320 [00:39<01:27,  1.16it/s, est. speed input: 643.80 toks/s, output: 3373.03 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 221/320 [00:39<01:02,  1.60it/s, est. speed input: 639.35 toks/s, output: 3391.39 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 222/320 [00:40<01:06,  1.46it/s, est. speed input: 629.69 toks/s, output: 3359.04 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 223/320 [00:47<03:35,  2.22s/it, est. speed input: 540.81 toks/s, output: 2916.42 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 224/320 [00:52<04:47,  3.00s/it, est. speed input: 490.75 toks/s, output: 2669.79 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 225/320 [00:59<06:38,  4.20s/it, est. speed input: 433.03 toks/s, output: 2379.98 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 226/320 [01:08<08:37,  5.51s/it, est. speed input: 378.93 toks/s, output: 2111.97 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 227/320 [01:18<10:14,  6.60s/it, est. speed input: 334.58 toks/s, output: 1896.70 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 228/320 [01:23<09:30,  6.20s/it, est. speed input: 314.52 toks/s, output: 1814.87 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 236/320 [01:23<02:02,  1.46s/it, est. speed input: 321.99 toks/s, output: 2102.59 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 251/320 [01:23<00:33,  2.03it/s, est. speed input: 345.82 toks/s, output: 2644.94 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 289/320 [01:24<00:04,  6.78it/s, est. speed input: 404.25 toks/s, output: 4026.39 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 302/320 [01:26<00:02,  6.16it/s, est. speed input: 412.29 toks/s, output: 4362.66 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 307/320 [01:27<00:02,  5.99it/s, est. speed input: 419.16 toks/s, output: 4488.26 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 311/320 [01:28<00:01,  6.31it/s, est. speed input: 421.04 toks/s, output: 4607.45 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 314/320 [01:30<00:01,  4.57it/s, est. speed input: 415.09 toks/s, output: 4606.65 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 316/320 [01:31<00:01,  3.49it/s, est. speed input: 409.28 toks/s, output: 4584.26 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 318/320 [01:33<00:00,  3.01it/s, est. speed input: 405.49 toks/s, output: 4585.76 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:34<00:00,  2.88it/s, est. speed input: 403.34 toks/s, output: 4608.91 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:34<00:00,  3.40it/s, est. speed input: 403.34 toks/s, output: 4608.91 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/320 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [00:00<00:00, 23150.17it/s]
{'num_samples': 40, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 27.5, 'total_acc': 25.0, 'pass_at_k_percent': {'1': 25.0, '8': 70.0}, 'pass_at_k_valid_counts': {'1': 40, '8': 40}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1/amc23x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part7.jsonl
[2025-12-03 22:45:25] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1/amc23x8  acc=27.5 pass_at_k={'1': 25.0, '8': 70.0}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [03:04<01:32, 92.78s/ds][Info] Sharding enabled: Process 7/8 handling range [210:240]
==================================================
data: aime24x8  ,remain samples: 30
{'idx': 210, 'id': 60, 'problem': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.', 'solution': '$\\frac{9}{s} + t = 4$ in hours and $\\frac{9}{s+2} + t = 2.4$ in hours.\nSubtracting the second equation from the first, we get, \n$\\frac{9}{s} - \\frac{9}{s+2} = 1.6$\nMultiplying by $(s)(s+2)$, we get \n$9s+18-9s=18=1.6s^{2} + 3.2s$\nMultiplying by 5/2 on both sides, we get\n$0 = 4s^{2} + 8s - 45$\nFactoring gives us \n$(2s-5)(2s+9) = 0$, of which the solution we want is $s=2.5$.\nSubstituting this back to the first equation, we can find that $t = 0.4$ hours.\nLastly, $s + \\frac{1}{2} = 3$ kilometers per hour, so\n$\\frac{9}{3} + 0.4 = 3.4$ hours, or $\\framebox{204}$ minutes\n-Failure.net\nThe amount of hours spent while walking on the first travel is $\\frac{240-t}{6}$. Thus, we have the equation $(240-t)(s) = 540$, and by the same logic, the second equation yields $(144-t)(s+2) = 540$. We have $240s-st = 540$, and $288+144s-2t-st = 540$. We subtract the two equations to get $96s+2t-288 = 0$, so we have $48s+t = 144$, so $t = 144-48s$, and now we have $(96+48s)(s) = 540$. The numerator of $s$ must evenly divide 540, however, $s$ must be less than 3. We can guess that $s = 2.5$. Now, $2.5+0.5 = 3$. Taking $\\frac{9}{3} = 3$, we find that it will take three hours for the 9 kilometers to be traveled. The t minutes spent at the coffeeshop can be written as $144-48(2.5)$, so t = 24. $180 + 24 = 204$. -sepehr2010', 'answer': '204', 'url': 'https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems/Problem_1', 'question': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 468.91it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 2/240 [00:03<07:02,  1.78s/it, est. speed input: 70.66 toks/s, output: 47.58 toks/s][A
Processed prompts:   1%|‚ñè         | 3/240 [00:03<04:16,  1.08s/it, est. speed input: 105.11 toks/s, output: 92.82 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:04<02:27,  1.60it/s, est. speed input: 137.05 toks/s, output: 170.05 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:05<02:52,  1.35it/s, est. speed input: 147.77 toks/s, output: 182.54 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:05<02:47,  1.39it/s, est. speed input: 145.94 toks/s, output: 207.29 toks/s][A
Processed prompts:   4%|‚ñç         | 9/240 [00:06<02:06,  1.83it/s, est. speed input: 158.39 toks/s, output: 273.19 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:06<01:44,  2.20it/s, est. speed input: 168.14 toks/s, output: 311.45 toks/s][A
Processed prompts:   5%|‚ñç         | 11/240 [00:06<01:28,  2.59it/s, est. speed input: 182.63 toks/s, output: 348.11 toks/s][A
Processed prompts:   5%|‚ñå         | 12/240 [00:07<01:41,  2.24it/s, est. speed input: 180.01 toks/s, output: 364.43 toks/s][A
Processed prompts:   6%|‚ñã         | 15/240 [00:07<00:59,  3.78it/s, est. speed input: 209.73 toks/s, output: 480.69 toks/s][A
Processed prompts:   8%|‚ñä         | 18/240 [00:08<00:57,  3.86it/s, est. speed input: 232.13 toks/s, output: 564.96 toks/s][A
Processed prompts:   8%|‚ñä         | 20/240 [00:08<00:45,  4.89it/s, est. speed input: 267.25 toks/s, output: 643.71 toks/s][A
Processed prompts:   9%|‚ñâ         | 21/240 [00:08<00:42,  5.13it/s, est. speed input: 277.71 toks/s, output: 677.39 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:09<00:45,  4.76it/s, est. speed input: 279.85 toks/s, output: 701.28 toks/s][A
Processed prompts:  10%|‚ñâ         | 23/240 [00:09<00:48,  4.47it/s, est. speed input: 281.51 toks/s, output: 724.85 toks/s][A
Processed prompts:  11%|‚ñà         | 26/240 [00:09<00:31,  6.80it/s, est. speed input: 316.59 toks/s, output: 840.86 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:09<00:30,  7.01it/s, est. speed input: 322.96 toks/s, output: 874.21 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 29/240 [00:09<00:25,  8.43it/s, est. speed input: 340.61 toks/s, output: 948.25 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:10<00:29,  7.10it/s, est. speed input: 346.82 toks/s, output: 1000.41 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 34/240 [00:10<00:23,  8.84it/s, est. speed input: 375.01 toks/s, output: 1109.05 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 36/240 [00:10<00:22,  9.16it/s, est. speed input: 396.45 toks/s, output: 1175.19 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 39/240 [00:10<00:16, 11.98it/s, est. speed input: 425.56 toks/s, output: 1292.36 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 41/240 [00:11<00:30,  6.55it/s, est. speed input: 425.24 toks/s, output: 1299.87 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 46/240 [00:11<00:18, 10.50it/s, est. speed input: 468.15 toks/s, output: 1495.82 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 49/240 [00:11<00:15, 12.60it/s, est. speed input: 492.97 toks/s, output: 1609.57 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 51/240 [00:12<00:17, 10.95it/s, est. speed input: 506.33 toks/s, output: 1659.39 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 53/240 [00:12<00:21,  8.60it/s, est. speed input: 509.26 toks/s, output: 1692.93 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 55/240 [00:12<00:20,  9.07it/s, est. speed input: 521.44 toks/s, output: 1753.78 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 57/240 [00:12<00:20,  8.80it/s, est. speed input: 536.92 toks/s, output: 1805.76 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 59/240 [00:13<00:24,  7.33it/s, est. speed input: 548.48 toks/s, output: 1836.72 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 62/240 [00:13<00:25,  6.97it/s, est. speed input: 569.25 toks/s, output: 1899.80 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 65/240 [00:14<00:27,  6.39it/s, est. speed input: 575.77 toks/s, output: 1952.43 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 66/240 [00:14<00:29,  5.90it/s, est. speed input: 573.59 toks/s, output: 1961.62 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 68/240 [00:14<00:25,  6.87it/s, est. speed input: 586.27 toks/s, output: 2023.31 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 69/240 [00:14<00:25,  6.65it/s, est. speed input: 589.01 toks/s, output: 2042.40 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 71/240 [00:15<00:22,  7.49it/s, est. speed input: 594.93 toks/s, output: 2099.84 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 72/240 [00:15<00:25,  6.65it/s, est. speed input: 594.92 toks/s, output: 2111.77 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 74/240 [00:16<00:40,  4.13it/s, est. speed input: 581.75 toks/s, output: 2086.66 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà‚ñè      | 75/240 [00:16<00:35,  4.68it/s, est. speed input: 586.23 toks/s, output: 2116.07 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 77/240 [00:16<00:30,  5.37it/s, est. speed input: 595.34 toks/s, output: 2164.60 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 78/240 [00:16<00:32,  5.01it/s, est. speed input: 591.10 toks/s, output: 2174.40 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 79/240 [00:16<00:30,  5.31it/s, est. speed input: 593.60 toks/s, output: 2197.31 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:17<00:28,  5.59it/s, est. speed input: 592.94 toks/s, output: 2220.06 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 83/240 [00:17<00:17,  8.87it/s, est. speed input: 606.28 toks/s, output: 2326.56 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 87/240 [00:17<00:12, 12.25it/s, est. speed input: 632.48 toks/s, output: 2467.27 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 89/240 [00:17<00:12, 12.55it/s, est. speed input: 645.33 toks/s, output: 2530.53 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 91/240 [00:18<00:18,  8.00it/s, est. speed input: 640.12 toks/s, output: 2545.37 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 93/240 [00:18<00:18,  8.03it/s, est. speed input: 644.93 toks/s, output: 2594.73 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 95/240 [00:18<00:15,  9.10it/s, est. speed input: 649.81 toks/s, output: 2657.89 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 97/240 [00:19<00:24,  5.74it/s, est. speed input: 639.39 toks/s, output: 2648.68 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 99/240 [00:19<00:19,  7.08it/s, est. speed input: 654.85 toks/s, output: 2715.53 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 101/240 [00:19<00:21,  6.49it/s, est. speed input: 660.36 toks/s, output: 2747.98 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/240 [00:19<00:22,  6.18it/s, est. speed input: 661.81 toks/s, output: 2762.29 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 103/240 [00:20<00:27,  5.05it/s, est. speed input: 657.13 toks/s, output: 2756.77 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 104/240 [00:21<00:53,  2.56it/s, est. speed input: 628.17 toks/s, output: 2659.62 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 106/240 [00:21<00:50,  2.64it/s, est. speed input: 618.16 toks/s, output: 2654.52 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:22<01:04,  2.05it/s, est. speed input: 598.24 toks/s, output: 2593.91 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/240 [00:23<00:48,  2.72it/s, est. speed input: 600.73 toks/s, output: 2637.17 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 111/240 [00:23<00:34,  3.71it/s, est. speed input: 606.71 toks/s, output: 2700.26 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 112/240 [00:24<00:50,  2.55it/s, est. speed input: 601.52 toks/s, output: 2643.21 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 114/240 [00:24<00:41,  3.04it/s, est. speed input: 599.84 toks/s, output: 2678.80 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 115/240 [00:25<00:42,  2.92it/s, est. speed input: 599.72 toks/s, output: 2678.28 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 118/240 [00:25<00:29,  4.18it/s, est. speed input: 608.06 toks/s, output: 2763.28 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/240 [00:25<00:25,  4.65it/s, est. speed input: 608.38 toks/s, output: 2811.58 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 121/240 [00:26<00:41,  2.87it/s, est. speed input: 590.18 toks/s, output: 2753.22 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:26<00:40,  2.91it/s, est. speed input: 589.71 toks/s, output: 2761.62 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:27<00:28,  4.05it/s, est. speed input: 593.86 toks/s, output: 2828.30 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:27<00:34,  3.36it/s, est. speed input: 586.54 toks/s, output: 2820.03 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 127/240 [00:27<00:23,  4.81it/s, est. speed input: 594.53 toks/s, output: 2891.69 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:28<00:26,  4.30it/s, est. speed input: 591.84 toks/s, output: 2899.92 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:28<00:26,  4.20it/s, est. speed input: 590.19 toks/s, output: 2915.45 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 131/240 [00:28<00:18,  6.01it/s, est. speed input: 594.94 toks/s, output: 2987.09 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 132/240 [00:29<00:39,  2.71it/s, est. speed input: 575.79 toks/s, output: 2919.54 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 133/240 [00:31<01:22,  1.29it/s, est. speed input: 541.78 toks/s, output: 2771.88 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:32<01:05,  1.61it/s, est. speed input: 534.74 toks/s, output: 2786.78 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 137/240 [00:32<00:51,  2.02it/s, est. speed input: 537.81 toks/s, output: 2822.76 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:34<01:11,  1.44it/s, est. speed input: 518.84 toks/s, output: 2744.76 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 140/240 [00:34<00:53,  1.87it/s, est. speed input: 519.90 toks/s, output: 2787.27 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:35<00:56,  1.76it/s, est. speed input: 513.41 toks/s, output: 2775.35 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 142/240 [00:36<00:51,  1.89it/s, est. speed input: 510.19 toks/s, output: 2786.12 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:36<00:42,  2.28it/s, est. speed input: 510.18 toks/s, output: 2815.27 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:36<00:39,  2.43it/s, est. speed input: 509.16 toks/s, output: 2841.50 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [00:37<00:31,  2.94it/s, est. speed input: 509.83 toks/s, output: 2892.11 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:37<00:25,  3.62it/s, est. speed input: 512.72 toks/s, output: 2951.78 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [00:38<00:36,  2.45it/s, est. speed input: 502.44 toks/s, output: 2921.94 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [00:38<00:33,  2.69it/s, est. speed input: 503.59 toks/s, output: 2945.73 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:39<00:38,  2.31it/s, est. speed input: 497.54 toks/s, output: 2940.78 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/240 [00:42<01:23,  1.04it/s, est. speed input: 472.04 toks/s, output: 2806.87 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 154/240 [00:43<01:36,  1.12s/it, est. speed input: 460.48 toks/s, output: 2748.10 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155/240 [00:46<02:23,  1.69s/it, est. speed input: 431.01 toks/s, output: 2602.95 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [00:46<01:45,  1.26s/it, est. speed input: 431.44 toks/s, output: 2635.75 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 157/240 [00:47<01:29,  1.08s/it, est. speed input: 429.48 toks/s, output: 2642.35 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 158/240 [00:47<01:06,  1.24it/s, est. speed input: 430.81 toks/s, output: 2675.11 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 159/240 [00:47<00:52,  1.55it/s, est. speed input: 432.20 toks/s, output: 2702.15 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 160/240 [00:51<01:49,  1.37s/it, est. speed input: 408.51 toks/s, output: 2578.97 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 161/240 [00:51<01:24,  1.06s/it, est. speed input: 409.37 toks/s, output: 2603.87 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 162/240 [00:52<01:24,  1.09s/it, est. speed input: 402.71 toks/s, output: 2588.77 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 163/240 [00:53<01:16,  1.00it/s, est. speed input: 398.97 toks/s, output: 2591.91 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 164/240 [00:56<01:58,  1.56s/it, est. speed input: 381.63 toks/s, output: 2499.62 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 165/240 [01:05<04:42,  3.76s/it, est. speed input: 332.14 toks/s, output: 2197.98 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 166/240 [01:17<07:45,  6.29s/it, est. speed input: 281.93 toks/s, output: 1891.05 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 193/240 [01:17<00:23,  1.98it/s, est. speed input: 327.52 toks/s, output: 2960.12 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:17<00:00,  3.10it/s, est. speed input: 432.11 toks/s, output: 4824.11 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 25445.73it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 3.3, 'total_acc': 6.666666666666667, 'pass_at_k_percent': {'1': 6.7, '8': 16.7}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1/aime24x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part7.jsonl
[2025-12-03 22:46:43] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1/aime24x8  acc=3.3 pass_at_k={'1': 6.7, '8': 16.7}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [04:22<00:00, 86.17s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [04:22<00:00, 87.61s/ds]
[2025-12-03 22:46:43] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2  ÂæÖËØÑÊµã=['minerva_math', 'olympiadbench', 'math500']  T=0.0  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 7/8 handling range [238:272]
==================================================
data: minerva_math  ,remain samples: 34
{'problem': 'Determine the highest linear density of atoms (atoms/m) encountered in vanadium (V). Please format your answer as $n \\times 10^x$ where $n$ is to 2 decimal places.', 'solution': '\\[\n\\begin{aligned}\n&\\mathrm{V}: \\quad \\text { atomic weight }=50.94 \\mathrm{~g} / \\text { mole } \\\\\n&\\rho=5.8 \\mathrm{~g} / \\mathrm{cm}^{3}\n\\end{aligned}\n\\]\n$B C C$, so $n=2$\nThe highest density would be found in the [111] direction. To find "a":\n\\[\n\\begin{aligned}\n&\\frac{\\text { atomic weight }}{\\rho}=a^{3} \\frac{N_{A}}{n} \\rightarrow a^{3}=\\frac{50.94 \\times 2}{5.8 \\times 6.023 \\times 10^{23}} \\\\\n&a=3.08 \\times 10^{-8} \\mathrm{~cm}=3.08 \\times 10^{-10} \\mathrm{~m}\n\\end{aligned}\n\\]\nThe length in the [111] direction is $\\mathrm{a} \\sqrt{3}$, so there are:\n\\[\n\\begin{aligned}\n&2 \\text { atoms } / \\mathrm{a} \\sqrt{3}=2 \\text { atoms/ }\\left(3.08 \\times 10^{-10} \\mathrm{~m} \\times \\sqrt{3}\\right) \\\\\n&= \\boxed{3.75e9} \\text { atoms } / \\mathrm{m}\n\\end{aligned}\n\\]', 'type': 'Introduction to Solid State Chemistry (3.091 Fall 2010)', 'idx': 238}

  0%|          | 0/34 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:00<00:00, 12831.23it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/272 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/272 [00:04<21:31,  4.77s/it, est. speed input: 32.32 toks/s, output: 45.12 toks/s][A
Processed prompts:   3%|‚ñé         | 9/272 [00:05<01:54,  2.29it/s, est. speed input: 259.05 toks/s, output: 373.75 toks/s][A
Processed prompts:   6%|‚ñã         | 17/272 [00:05<00:55,  4.59it/s, est. speed input: 421.77 toks/s, output: 683.21 toks/s][A
Processed prompts:   9%|‚ñâ         | 25/272 [00:06<00:35,  6.91it/s, est. speed input: 591.05 toks/s, output: 969.11 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 33/272 [00:07<00:36,  6.54it/s, est. speed input: 576.32 toks/s, output: 1096.98 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 41/272 [00:08<00:32,  7.18it/s, est. speed input: 626.90 toks/s, output: 1295.87 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 49/272 [00:08<00:21, 10.30it/s, est. speed input: 707.38 toks/s, output: 1627.47 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 57/272 [00:09<00:19, 11.00it/s, est. speed input: 726.72 toks/s, output: 1847.59 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 73/272 [00:09<00:10, 18.78it/s, est. speed input: 941.18 toks/s, output: 2500.88 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 81/272 [00:10<00:13, 14.35it/s, est. speed input: 1108.19 toks/s, output: 2594.78 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 97/272 [00:11<00:11, 15.77it/s, est. speed input: 1161.39 toks/s, output: 3043.49 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 104/272 [00:11<00:10, 16.31it/s, est. speed input: 1189.69 toks/s, output: 3246.20 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 112/272 [00:12<00:10, 14.58it/s, est. speed input: 1297.81 toks/s, output: 3390.10 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 120/272 [00:14<00:17,  8.71it/s, est. speed input: 1246.50 toks/s, output: 3237.22 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 128/272 [00:16<00:22,  6.50it/s, est. speed input: 1183.48 toks/s, output: 3143.59 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 136/272 [00:18<00:26,  5.20it/s, est. speed input: 1095.49 toks/s, output: 3065.87 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 144/272 [00:20<00:28,  4.53it/s, est. speed input: 1050.99 toks/s, output: 3043.34 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 152/272 [00:22<00:26,  4.56it/s, est. speed input: 1055.43 toks/s, output: 3139.70 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 153/272 [00:24<00:36,  3.27it/s, est. speed input: 982.49 toks/s, output: 2931.72 toks/s] [A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 161/272 [00:24<00:23,  4.76it/s, est. speed input: 1063.00 toks/s, output: 3245.79 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 169/272 [00:26<00:20,  5.01it/s, est. speed input: 1080.79 toks/s, output: 3408.22 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 177/272 [01:19<03:39,  2.31s/it, est. speed input: 369.37 toks/s, output: 1267.26 toks/s] [A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 185/272 [01:19<02:17,  1.58s/it, est. speed input: 384.49 toks/s, output: 1572.78 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 257/272 [01:22<00:05,  2.92it/s, est. speed input: 571.91 toks/s, output: 4197.64 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 264/272 [01:22<00:02,  3.23it/s, est. speed input: 580.56 toks/s, output: 4450.95 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 268/272 [01:25<00:01,  2.99it/s, est. speed input: 575.13 toks/s, output: 4462.09 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 271/272 [01:25<00:00,  3.20it/s, est. speed input: 583.27 toks/s, output: 4563.93 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [01:25<00:00,  3.18it/s, est. speed input: 585.74 toks/s, output: 4595.93 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/272 [00:00<?, ?it/s][A
Evaluate:  39%|‚ñà‚ñà‚ñà‚ñä      | 105/272 [00:00<00:00, 962.90it/s][A
Evaluate:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 202/272 [00:00<00:00, 949.18it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [00:00<00:00, 1172.44it/s]
{'num_samples': 34, 'num_scores': 272, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 8.8, 'total_acc': 8.823529411764707, 'pass_at_k_percent': {'1': 8.8, '8': 8.8}, 'pass_at_k_valid_counts': {'1': 34, '8': 34}, 'type_acc': {'Introduction to Solid State Chemistry (3.091 Fall 2010)': 0.0, 'Physical Chemistry (5.61 Fall 2017)': 0.0, 'Principles of Microeconomics (14.01 Fall 2011)': 16.7}, 'type_pass_at_k_percent': {'Introduction to Solid State Chemistry (3.091 Fall 2010)': {'1': 0.0, '8': 0.0}, 'Physical Chemistry (5.61 Fall 2017)': {'1': 0.0, '8': 0.0}, 'Principles of Microeconomics (14.01 Fall 2011)': {'1': 16.7, '8': 16.7}}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2/minerva_math/test_think-boxed_-1_seed0_t0.0_s0_e-1_part7.jsonl
[2025-12-03 22:48:09] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2/minerva_math  acc=8.8 pass_at_k={'1': 8.8, '8': 8.8}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:26<02:52, 86.08s/ds][Info] Sharding enabled: Process 7/8 handling range [588:675]
==================================================
data: olympiadbench  ,remain samples: 87
{'idx': 588, 'id': 2984, 'subfield': 'Algebra', 'context': None, 'question': 'Compute the value of\n\n$$\n\\sin \\left(6^{\\circ}\\right) \\cdot \\sin \\left(12^{\\circ}\\right) \\cdot \\sin \\left(24^{\\circ}\\right) \\cdot \\sin \\left(42^{\\circ}\\right)+\\sin \\left(12^{\\circ}\\right) \\cdot \\sin \\left(24^{\\circ}\\right) \\cdot \\sin \\left(42^{\\circ}\\right) \\text {. }\n$$', 'solution': ['Let $S=\\left(1+\\sin 6^{\\circ}\\right)\\left(\\sin 12^{\\circ} \\sin 24^{\\circ} \\sin 42^{\\circ}\\right)$. It follows from a sum-to-product identity that $1+\\sin 6^{\\circ}=$ $\\sin 90^{\\circ}+\\sin 6^{\\circ}=2 \\sin 48^{\\circ} \\cos 42^{\\circ}$. Because the sine of an angle is the cosine of its complement, it follows that\n\n$$\nS=\\left(2 \\sin 48^{\\circ} \\cos 42^{\\circ}\\right)\\left(\\sin 12^{\\circ} \\sin 24^{\\circ} \\sin 42^{\\circ}\\right)=2\\left(\\sin 48^{\\circ}\\right)^{2}\\left(\\sin 12^{\\circ} \\sin 24^{\\circ} \\cos 48^{\\circ}\\right)\n$$\n\nBy the double-angle formula, this means $S=\\sin 12^{\\circ} \\sin 24^{\\circ} \\sin 48^{\\circ} \\sin 96^{\\circ}$. By a product-to-sum identity,\n\n$$\n\\sin 12^{\\circ} \\sin 48^{\\circ}=\\frac{\\cos 36^{\\circ}-\\cos 60^{\\circ}}{2}=\\frac{\\sqrt{5}-1}{8}\n$$\n\n\n\nand\n\n$$\n\\sin 24^{\\circ} \\sin 96^{\\circ}=\\frac{\\cos 72^{\\circ}-\\cos 120^{\\circ}}{2}=\\frac{\\sqrt{5}+1}{8}\n$$\n\nMultiply the expressions on the right-hand sides of (1) and (2) to obtain $\\frac{\\mathbf{1}}{\\mathbf{1 6}}$'], 'final_answer': ['$\\frac{1}{16}$'], 'is_multiple_answer': False, 'unit': None, 'answer_type': 'Numerical', 'error': None}

  0%|          | 0/87 [00:00<?, ?it/s][A
 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 43/87 [00:00<00:00, 429.35it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:00<00:00, 433.47it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:00<00:00, 432.62it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/696 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/696 [00:04<47:22,  4.09s/it, est. speed input: 20.78 toks/s, output: 46.46 toks/s][A
Processed prompts:   1%|‚ñè         | 9/696 [00:05<06:01,  1.90it/s, est. speed input: 134.29 toks/s, output: 308.40 toks/s][A
Processed prompts:   4%|‚ñé         | 25/696 [00:06<02:02,  5.47it/s, est. speed input: 769.52 toks/s, output: 923.42 toks/s][A
Processed prompts:   5%|‚ñç         | 33/696 [00:06<01:28,  7.46it/s, est. speed input: 1084.98 toks/s, output: 1230.21 toks/s][A
Processed prompts:   6%|‚ñå         | 41/696 [00:07<01:12,  9.10it/s, est. speed input: 1314.51 toks/s, output: 1491.85 toks/s][A
Processed prompts:   7%|‚ñã         | 49/696 [00:08<01:03, 10.18it/s, est. speed input: 1341.22 toks/s, output: 1720.54 toks/s][A
Processed prompts:   8%|‚ñä         | 57/696 [00:09<01:09,  9.22it/s, est. speed input: 1457.05 toks/s, output: 1843.19 toks/s][A
Processed prompts:   9%|‚ñâ         | 64/696 [00:09<01:00, 10.38it/s, est. speed input: 1613.31 toks/s, output: 2051.48 toks/s][A
Processed prompts:  10%|‚ñà         | 72/696 [00:10<00:55, 11.14it/s, est. speed input: 1744.40 toks/s, output: 2263.28 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 80/696 [00:10<00:42, 14.46it/s, est. speed input: 1794.68 toks/s, output: 2566.38 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 88/696 [00:10<00:37, 16.13it/s, est. speed input: 1950.38 toks/s, output: 2817.15 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 96/696 [00:10<00:31, 18.78it/s, est. speed input: 1978.80 toks/s, output: 3088.77 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 104/696 [00:11<00:26, 22.63it/s, est. speed input: 2148.74 toks/s, output: 3379.23 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 112/696 [00:11<00:26, 22.11it/s, est. speed input: 2162.66 toks/s, output: 3579.45 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 128/696 [00:11<00:17, 32.95it/s, est. speed input: 2231.69 toks/s, output: 3789.88 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 136/696 [00:12<00:23, 24.15it/s, est. speed input: 2183.50 toks/s, output: 3934.34 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 144/696 [00:12<00:25, 21.54it/s, est. speed input: 2164.50 toks/s, output: 4117.25 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 152/696 [00:14<00:49, 11.00it/s, est. speed input: 1973.07 toks/s, output: 3946.20 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 155/696 [00:14<00:48, 11.13it/s, est. speed input: 1959.84 toks/s, output: 3938.07 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 161/696 [00:16<01:11,  7.46it/s, est. speed input: 1812.56 toks/s, output: 3667.70 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 168/696 [00:16<00:58,  9.09it/s, est. speed input: 1823.32 toks/s, output: 3676.46 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 170/696 [00:17<01:10,  7.41it/s, est. speed input: 1770.46 toks/s, output: 3564.05 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 176/696 [00:17<00:56,  9.20it/s, est. speed input: 1784.75 toks/s, output: 3610.62 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 184/696 [00:18<01:04,  7.91it/s, est. speed input: 1705.06 toks/s, output: 3657.75 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 186/696 [00:20<01:57,  4.34it/s, est. speed input: 1553.16 toks/s, output: 3359.98 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 193/696 [00:21<01:17,  6.53it/s, est. speed input: 1574.85 toks/s, output: 3501.27 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 195/696 [00:21<01:37,  5.13it/s, est. speed input: 1513.45 toks/s, output: 3388.71 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 201/696 [00:23<01:45,  4.67it/s, est. speed input: 1433.95 toks/s, output: 3280.16 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 208/696 [00:25<02:05,  3.89it/s, est. speed input: 1326.36 toks/s, output: 3118.65 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 215/696 [00:26<01:39,  4.85it/s, est. speed input: 1311.73 toks/s, output: 3157.09 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 216/696 [00:28<02:30,  3.20it/s, est. speed input: 1231.95 toks/s, output: 2974.80 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 224/696 [00:28<01:28,  5.36it/s, est. speed input: 1243.94 toks/s, output: 3064.29 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 226/696 [00:30<02:24,  3.26it/s, est. speed input: 1164.01 toks/s, output: 2891.18 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 232/696 [00:31<02:00,  3.85it/s, est. speed input: 1148.03 toks/s, output: 2922.46 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñé      | 234/696 [00:32<02:06,  3.64it/s, est. speed input: 1127.10 toks/s, output: 2918.93 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 241/696 [00:32<01:24,  5.38it/s, est. speed input: 1129.18 toks/s, output: 3082.91 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 243/696 [00:33<01:42,  4.40it/s, est. speed input: 1102.76 toks/s, output: 3034.40 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 244/696 [00:35<02:28,  3.05it/s, est. speed input: 1064.44 toks/s, output: 2933.23 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 251/696 [00:35<01:24,  5.26it/s, est. speed input: 1068.55 toks/s, output: 2986.95 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 258/696 [00:36<01:17,  5.62it/s, est. speed input: 1048.15 toks/s, output: 3031.36 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 259/696 [00:41<04:00,  1.82it/s, est. speed input: 925.07 toks/s, output: 2702.35 toks/s] [A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 267/696 [00:42<02:33,  2.80it/s, est. speed input: 920.59 toks/s, output: 2871.81 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñä      | 269/696 [00:42<02:15,  3.15it/s, est. speed input: 920.35 toks/s, output: 2871.65 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 270/696 [00:43<02:11,  3.24it/s, est. speed input: 917.14 toks/s, output: 2860.60 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 274/696 [00:43<01:32,  4.57it/s, est. speed input: 919.33 toks/s, output: 2866.02 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 277/696 [00:49<05:10,  1.35it/s, est. speed input: 806.75 toks/s, output: 2535.46 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 285/696 [00:50<02:43,  2.51it/s, est. speed input: 822.62 toks/s, output: 2739.25 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 287/696 [00:51<02:42,  2.52it/s, est. speed input: 816.72 toks/s, output: 2749.57 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 294/696 [00:51<01:36,  4.17it/s, est. speed input: 835.63 toks/s, output: 2918.02 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 301/696 [00:51<01:06,  5.95it/s, est. speed input: 846.45 toks/s, output: 3051.86 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 303/696 [00:51<01:02,  6.32it/s, est. speed input: 847.53 toks/s, output: 3059.37 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 305/696 [00:51<00:56,  6.90it/s, est. speed input: 849.22 toks/s, output: 3069.03 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 307/696 [00:52<00:52,  7.37it/s, est. speed input: 850.22 toks/s, output: 3076.20 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 309/696 [00:53<01:29,  4.31it/s, est. speed input: 835.53 toks/s, output: 3027.94 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 310/696 [00:53<01:50,  3.51it/s, est. speed input: 826.78 toks/s, output: 2997.84 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 313/696 [00:54<01:17,  4.97it/s, est. speed input: 827.65 toks/s, output: 3005.83 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 315/696 [00:54<01:10,  5.42it/s, est. speed input: 826.50 toks/s, output: 3016.86 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 316/696 [01:07<14:05,  2.23s/it, est. speed input: 664.50 toks/s, output: 2442.44 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 319/696 [01:08<08:33,  1.36s/it, est. speed input: 667.46 toks/s, output: 2503.84 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 322/696 [01:08<05:29,  1.14it/s, est. speed input: 670.89 toks/s, output: 2566.82 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 322/696 [01:20<05:29,  1.14it/s, est. speed input: 670.89 toks/s, output: 2566.82 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 323/696 [01:43<36:36,  5.89s/it, est. speed input: 444.93 toks/s, output: 1726.79 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 324/696 [01:43<30:21,  4.90s/it, est. speed input: 445.59 toks/s, output: 1753.74 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 339/696 [01:43<06:15,  1.05s/it, est. speed input: 460.98 toks/s, output: 2193.99 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 355/696 [01:44<02:49,  2.01it/s, est. speed input: 491.27 toks/s, output: 2653.67 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 406/696 [01:44<00:42,  6.90it/s, est. speed input: 543.13 toks/s, output: 4153.69 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 420/696 [01:50<00:56,  4.87it/s, est. speed input: 528.77 toks/s, output: 4253.03 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 430/696 [01:51<00:50,  5.29it/s, est. speed input: 533.36 toks/s, output: 4259.24 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 437/696 [01:51<00:45,  5.69it/s, est. speed input: 536.42 toks/s, output: 4257.06 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 443/696 [01:52<00:41,  6.14it/s, est. speed input: 537.88 toks/s, output: 4254.26 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 447/696 [01:53<00:42,  5.88it/s, est. speed input: 537.60 toks/s, output: 4241.55 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 454/696 [01:53<00:35,  6.91it/s, est. speed input: 540.07 toks/s, output: 4242.63 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 457/696 [01:54<00:35,  6.77it/s, est. speed input: 540.40 toks/s, output: 4231.01 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 465/696 [01:56<00:40,  5.64it/s, est. speed input: 536.38 toks/s, output: 4187.28 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 467/696 [01:56<00:39,  5.81it/s, est. speed input: 536.94 toks/s, output: 4186.21 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 469/696 [01:56<00:37,  6.01it/s, est. speed input: 537.80 toks/s, output: 4185.89 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 471/696 [01:56<00:34,  6.51it/s, est. speed input: 539.07 toks/s, output: 4188.63 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 473/696 [01:57<00:40,  5.52it/s, est. speed input: 537.56 toks/s, output: 4171.57 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 474/696 [01:57<00:38,  5.77it/s, est. speed input: 537.66 toks/s, output: 4169.76 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 476/696 [01:57<00:34,  6.31it/s, est. speed input: 537.88 toks/s, output: 4166.12 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 477/696 [01:58<00:37,  5.77it/s, est. speed input: 537.33 toks/s, output: 4159.27 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 478/696 [01:58<00:35,  6.14it/s, est. speed input: 537.67 toks/s, output: 4159.38 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 493/696 [01:58<00:10, 19.93it/s, est. speed input: 547.88 toks/s, output: 4195.51 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 507/696 [01:58<00:06, 30.77it/s, est. speed input: 556.65 toks/s, output: 4242.79 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 514/696 [01:58<00:05, 34.52it/s, est. speed input: 559.78 toks/s, output: 4253.62 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 518/696 [02:00<00:17,  9.98it/s, est. speed input: 555.07 toks/s, output: 4213.96 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 521/696 [02:02<00:31,  5.50it/s, est. speed input: 550.34 toks/s, output: 4169.48 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 524/696 [02:02<00:26,  6.46it/s, est. speed input: 552.86 toks/s, output: 4186.66 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 527/696 [02:03<00:30,  5.63it/s, est. speed input: 552.92 toks/s, output: 4179.31 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 530/696 [02:03<00:28,  5.75it/s, est. speed input: 555.10 toks/s, output: 4199.71 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 532/696 [02:03<00:25,  6.55it/s, est. speed input: 556.44 toks/s, output: 4245.44 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 534/696 [02:04<00:23,  6.79it/s, est. speed input: 557.14 toks/s, output: 4286.27 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 536/696 [02:04<00:22,  7.18it/s, est. speed input: 557.97 toks/s, output: 4327.87 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 538/696 [02:04<00:25,  6.14it/s, est. speed input: 557.51 toks/s, output: 4341.98 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 546/696 [02:05<00:18,  8.12it/s, est. speed input: 560.25 toks/s, output: 4360.76 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 547/696 [02:06<00:22,  6.67it/s, est. speed input: 559.40 toks/s, output: 4351.20 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 548/696 [02:07<00:37,  3.90it/s, est. speed input: 556.22 toks/s, output: 4321.89 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 549/696 [02:07<00:34,  4.25it/s, est. speed input: 557.65 toks/s, output: 4325.21 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 550/696 [02:13<03:02,  1.25s/it, est. speed input: 532.73 toks/s, output: 4149.34 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 552/696 [02:13<02:05,  1.15it/s, est. speed input: 533.35 toks/s, output: 4188.85 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 553/696 [02:13<01:44,  1.36it/s, est. speed input: 533.48 toks/s, output: 4207.17 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 555/696 [02:13<01:09,  2.01it/s, est. speed input: 534.46 toks/s, output: 4249.31 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 557/696 [02:14<01:11,  1.93it/s, est. speed input: 531.69 toks/s, output: 4244.83 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 565/696 [02:15<00:28,  4.59it/s, est. speed input: 538.17 toks/s, output: 4293.97 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 566/696 [02:19<01:15,  1.73it/s, est. speed input: 524.50 toks/s, output: 4203.43 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 567/696 [02:20<01:23,  1.55it/s, est. speed input: 520.92 toks/s, output: 4192.95 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 568/696 [02:20<01:19,  1.61it/s, est. speed input: 519.94 toks/s, output: 4199.62 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 572/696 [02:20<00:43,  2.88it/s, est. speed input: 522.93 toks/s, output: 4281.87 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 574/696 [02:20<00:33,  3.65it/s, est. speed input: 524.32 toks/s, output: 4322.10 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 576/696 [02:24<01:20,  1.49it/s, est. speed input: 512.98 toks/s, output: 4259.02 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 579/696 [02:24<00:52,  2.24it/s, est. speed input: 514.58 toks/s, output: 4317.79 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 582/696 [02:31<01:55,  1.01s/it, est. speed input: 494.94 toks/s, output: 4197.86 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 584/696 [02:31<01:27,  1.28it/s, est. speed input: 495.61 toks/s, output: 4235.28 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 587/696 [02:31<00:58,  1.87it/s, est. speed input: 496.52 toks/s, output: 4290.55 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 589/696 [02:31<00:45,  2.34it/s, est. speed input: 496.91 toks/s, output: 4325.49 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 591/696 [02:35<01:25,  1.23it/s, est. speed input: 485.85 toks/s, output: 4258.95 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 594/696 [02:35<00:54,  1.87it/s, est. speed input: 487.19 toks/s, output: 4315.09 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 596/696 [02:35<00:41,  2.38it/s, est. speed input: 487.80 toks/s, output: 4349.93 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 598/696 [02:38<01:10,  1.39it/s, est. speed input: 479.52 toks/s, output: 4303.64 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 602/696 [02:38<00:40,  2.34it/s, est. speed input: 481.78 toks/s, output: 4376.41 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 604/696 [02:39<00:31,  2.91it/s, est. speed input: 482.74 toks/s, output: 4411.21 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 606/696 [02:44<01:19,  1.13it/s, est. speed input: 469.13 toks/s, output: 4314.23 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 608/696 [02:44<00:59,  1.49it/s, est. speed input: 469.57 toks/s, output: 4347.95 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 611/696 [02:44<00:37,  2.25it/s, est. speed input: 470.45 toks/s, output: 4400.32 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 613/696 [02:44<00:29,  2.84it/s, est. speed input: 470.83 toks/s, output: 4433.28 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 615/696 [02:46<00:42,  1.89it/s, est. speed input: 466.22 toks/s, output: 4416.43 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 617/696 [02:46<00:31,  2.52it/s, est. speed input: 467.01 toks/s, output: 4450.50 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 619/696 [02:46<00:23,  3.35it/s, est. speed input: 467.82 toks/s, output: 4484.60 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 621/696 [02:46<00:17,  4.33it/s, est. speed input: 468.55 toks/s, output: 4517.99 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 623/696 [02:50<00:52,  1.38it/s, est. speed input: 459.36 toks/s, output: 4452.53 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 626/696 [02:50<00:32,  2.17it/s, est. speed input: 461.02 toks/s, output: 4503.17 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 628/696 [02:55<01:06,  1.03it/s, est. speed input: 450.10 toks/s, output: 4416.91 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 630/696 [02:56<00:58,  1.14it/s, est. speed input: 448.51 toks/s, output: 4419.62 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 631/696 [02:57<00:55,  1.18it/s, est. speed input: 447.29 toks/s, output: 4419.83 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 633/696 [02:57<00:37,  1.68it/s, est. speed input: 448.00 toks/s, output: 4451.35 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 634/696 [03:00<01:01,  1.01it/s, est. speed input: 442.25 toks/s, output: 4401.35 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 635/696 [03:01<00:57,  1.05it/s, est. speed input: 441.31 toks/s, output: 4399.06 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 636/696 [03:03<01:22,  1.37s/it, est. speed input: 435.62 toks/s, output: 4349.27 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 641/696 [03:04<00:29,  1.88it/s, est. speed input: 440.27 toks/s, output: 4430.32 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 643/696 [03:04<00:26,  2.00it/s, est. speed input: 439.38 toks/s, output: 4443.76 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 645/696 [03:05<00:20,  2.50it/s, est. speed input: 439.83 toks/s, output: 4470.59 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 647/696 [03:05<00:15,  3.24it/s, est. speed input: 440.74 toks/s, output: 4500.08 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 650/696 [03:05<00:09,  4.84it/s, est. speed input: 442.37 toks/s, output: 4547.20 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 656/696 [03:05<00:04,  8.21it/s, est. speed input: 446.21 toks/s, output: 4640.21 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 658/696 [03:05<00:04,  7.67it/s, est. speed input: 447.01 toks/s, output: 4665.01 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 662/696 [03:06<00:03, 10.31it/s, est. speed input: 451.36 toks/s, output: 4727.23 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 664/696 [03:06<00:03,  9.79it/s, est. speed input: 452.72 toks/s, output: 4754.01 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 667/696 [03:06<00:02, 11.37it/s, est. speed input: 454.57 toks/s, output: 4799.16 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 669/696 [03:07<00:04,  5.95it/s, est. speed input: 454.35 toks/s, output: 4809.23 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 672/696 [03:07<00:03,  7.12it/s, est. speed input: 455.75 toks/s, output: 4851.94 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 696/696 [03:10<00:00,  8.55it/s, est. speed input: 463.78 toks/s, output: 5171.72 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 696/696 [03:10<00:00,  3.66it/s, est. speed input: 463.78 toks/s, output: 5171.72 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/696 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 696/696 [00:00<00:00, 10127.58it/s]
{'num_samples': 87, 'num_scores': 696, 'timeout_samples': 0, 'empty_samples': 4, 'acc': 20.7, 'total_acc': 21.408045977011493, 'pass_at_k_percent': {'1': 21.4, '8': 24.1}, 'pass_at_k_valid_counts': {'1': 87, '8': 87}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2/olympiadbench/test_think-boxed_-1_seed0_t0.0_s0_e-1_part7.jsonl
[2025-12-03 22:51:22] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2/olympiadbench  acc=20.7 pass_at_k={'1': 21.4, '8': 24.1}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [04:39<02:29, 149.01s/ds][Info] Sharding enabled: Process 7/8 handling range [434:500]
==================================================
data: math500  ,remain samples: 66
{'idx': 434, 'problem': 'In a certain isosceles right triangle, the altitude to the hypotenuse has length $4\\sqrt{2}$.  What is the area of the triangle?', 'solution': 'In isosceles right triangle $\\triangle ABC$ below, $\\overline{AD}$ is the altitude to the hypotenuse.\n\n[asy]\nimport olympiad;\nunitsize(0.8inch);\npair A,B,C,D;\nA = (0,1);\nB= (1,0);\nC = -B;\nD = (0,0);\ndraw(A--B--C--A,linewidth(1));\ndraw(A--D,linewidth(0.8));\ndraw(rightanglemark(C,A,B,s=5));\ndraw(rightanglemark(C,D,A,s=5));\nlabel("$A$",A,N);\nlabel("$B$",B,S);\nlabel("$C$",C,S);\nlabel("$D$",D,S);\n[/asy]\n\nBecause $\\triangle ABC$ is an isosceles right triangle, $\\angle ABC = 45^\\circ$.  Since $\\angle ADB = 90^\\circ$, we know that $\\angle DAB = 45^\\circ$, so $\\triangle ABD$ is also a 45-45-90 triangle.  Similarly, $\\triangle ACD$ is a 45-45-90 triangle.  Therefore, $DB=DC = DA = 4\\sqrt{2}$, so $BC = BD+DC = 8\\sqrt{2}$, and  \\[[ABC] = \\frac{(AD)(BC)}{2} = \\frac{(4\\sqrt{2})(8\\sqrt{2})}{2} = \\boxed{32}.\\]', 'answer': '32', 'subject': 'Prealgebra', 'level': 5, 'unique_id': 'test/prealgebra/1640.json'}

  0%|          | 0/66 [00:00<?, ?it/s][A
 33%|‚ñà‚ñà‚ñà‚ñé      | 22/66 [00:00<00:00, 212.20it/s][A
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 45/66 [00:00<00:00, 219.18it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:00<00:00, 216.81it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/528 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/528 [00:02<19:24,  2.21s/it, est. speed input: 29.42 toks/s, output: 52.96 toks/s][A
Processed prompts:   2%|‚ñè         | 9/528 [00:02<02:17,  3.77it/s, est. speed input: 241.02 toks/s, output: 366.58 toks/s][A
Processed prompts:   3%|‚ñé         | 17/528 [00:03<01:12,  7.05it/s, est. speed input: 641.25 toks/s, output: 695.52 toks/s][A
Processed prompts:   5%|‚ñç         | 25/528 [00:03<00:46, 10.93it/s, est. speed input: 737.55 toks/s, output: 1035.96 toks/s][A
Processed prompts:   6%|‚ñã         | 33/528 [00:03<00:33, 14.62it/s, est. speed input: 818.61 toks/s, output: 1348.39 toks/s][A
Processed prompts:   8%|‚ñä         | 41/528 [00:04<00:27, 17.68it/s, est. speed input: 886.54 toks/s, output: 1636.98 toks/s][A
Processed prompts:   9%|‚ñâ         | 49/528 [00:04<00:21, 22.67it/s, est. speed input: 1007.09 toks/s, output: 1966.60 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 65/528 [00:04<00:14, 32.91it/s, est. speed input: 1166.15 toks/s, output: 2615.13 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 73/528 [00:04<00:12, 37.04it/s, est. speed input: 1263.49 toks/s, output: 2925.22 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 81/528 [00:05<00:15, 29.57it/s, est. speed input: 1272.37 toks/s, output: 3058.54 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 89/528 [00:05<00:18, 24.26it/s, est. speed input: 1259.21 toks/s, output: 3156.24 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 97/528 [00:05<00:18, 22.87it/s, est. speed input: 1278.70 toks/s, output: 3287.62 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 113/528 [00:06<00:20, 20.23it/s, est. speed input: 1442.08 toks/s, output: 3385.28 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 121/528 [00:07<00:26, 15.39it/s, est. speed input: 1374.59 toks/s, output: 3312.89 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 129/528 [00:08<00:22, 17.49it/s, est. speed input: 1403.55 toks/s, output: 3418.77 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 137/528 [00:08<00:18, 20.64it/s, est. speed input: 1488.93 toks/s, output: 3686.40 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 145/528 [00:09<00:29, 13.18it/s, est. speed input: 1350.68 toks/s, output: 3404.36 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 153/528 [00:09<00:22, 16.92it/s, est. speed input: 1407.83 toks/s, output: 3572.47 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 161/528 [00:10<00:24, 15.17it/s, est. speed input: 1421.49 toks/s, output: 3691.19 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 169/528 [00:10<00:21, 16.60it/s, est. speed input: 1467.06 toks/s, output: 3918.05 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñé      | 177/528 [00:10<00:17, 20.32it/s, est. speed input: 1504.23 toks/s, output: 4212.78 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 185/528 [00:10<00:13, 25.26it/s, est. speed input: 1570.33 toks/s, output: 4502.74 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 193/528 [00:11<00:13, 24.13it/s, est. speed input: 1690.29 toks/s, output: 4558.49 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 201/528 [00:11<00:12, 25.60it/s, est. speed input: 1701.16 toks/s, output: 4657.59 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 209/528 [00:12<00:21, 15.02it/s, est. speed input: 1621.30 toks/s, output: 4466.43 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 225/528 [00:12<00:13, 21.92it/s, est. speed input: 1718.03 toks/s, output: 4979.32 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 233/528 [00:13<00:14, 20.68it/s, est. speed input: 1690.24 toks/s, output: 5157.13 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 241/528 [00:13<00:11, 25.34it/s, est. speed input: 1716.96 toks/s, output: 5467.46 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 249/528 [00:13<00:09, 28.69it/s, est. speed input: 1730.84 toks/s, output: 5730.58 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 257/528 [00:14<00:09, 27.49it/s, est. speed input: 1742.82 toks/s, output: 5814.34 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 265/528 [00:15<00:17, 14.88it/s, est. speed input: 1649.79 toks/s, output: 5544.93 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 273/528 [00:15<00:15, 16.69it/s, est. speed input: 1653.83 toks/s, output: 5541.82 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 281/528 [00:15<00:12, 20.58it/s, est. speed input: 1678.89 toks/s, output: 5708.51 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 297/528 [00:15<00:07, 32.76it/s, est. speed input: 1773.83 toks/s, output: 6129.94 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 305/528 [00:15<00:06, 35.82it/s, est. speed input: 1802.81 toks/s, output: 6390.14 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 313/528 [00:18<00:20, 10.26it/s, est. speed input: 1608.56 toks/s, output: 5702.59 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 321/528 [00:19<00:19, 10.68it/s, est. speed input: 1592.67 toks/s, output: 5661.54 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 329/528 [00:19<00:16, 12.05it/s, est. speed input: 1618.69 toks/s, output: 5857.72 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 337/528 [00:20<00:15, 12.72it/s, est. speed input: 1616.85 toks/s, output: 5937.15 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 345/528 [00:20<00:14, 12.27it/s, est. speed input: 1591.55 toks/s, output: 6047.33 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 353/528 [00:20<00:11, 15.75it/s, est. speed input: 1615.70 toks/s, output: 6179.00 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 361/528 [00:21<00:08, 19.81it/s, est. speed input: 1635.02 toks/s, output: 6451.62 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 369/528 [00:22<00:12, 12.83it/s, est. speed input: 1586.98 toks/s, output: 6243.64 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 385/528 [00:23<00:10, 13.15it/s, est. speed input: 1577.21 toks/s, output: 6190.20 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 401/528 [00:23<00:06, 18.95it/s, est. speed input: 1613.48 toks/s, output: 6388.90 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 409/528 [00:24<00:06, 18.63it/s, est. speed input: 1606.99 toks/s, output: 6484.55 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 417/528 [00:31<00:29,  3.76it/s, est. speed input: 1243.47 toks/s, output: 5206.71 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 425/528 [00:36<00:37,  2.76it/s, est. speed input: 1088.10 toks/s, output: 4704.13 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 432/528 [00:47<00:34,  2.76it/s, est. speed input: 1106.15 toks/s, output: 4904.99 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 433/528 [01:15<02:28,  1.56s/it, est. speed input: 541.24 toks/s, output: 2430.00 toks/s] [A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 457/528 [01:21<01:01,  1.16it/s, est. speed input: 550.74 toks/s, output: 3152.48 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 465/528 [01:23<00:45,  1.38it/s, est. speed input: 557.79 toks/s, output: 3389.80 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 473/528 [01:24<00:32,  1.70it/s, est. speed input: 559.99 toks/s, output: 3647.15 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 481/528 [01:24<00:22,  2.11it/s, est. speed input: 563.55 toks/s, output: 3897.03 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 489/528 [01:26<00:15,  2.53it/s, est. speed input: 568.55 toks/s, output: 4119.41 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 497/528 [01:27<00:10,  3.03it/s, est. speed input: 581.07 toks/s, output: 4342.94 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 505/528 [01:27<00:05,  4.10it/s, est. speed input: 592.23 toks/s, output: 4615.84 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 513/528 [01:28<00:02,  5.00it/s, est. speed input: 602.13 toks/s, output: 4856.48 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 521/528 [01:28<00:01,  6.64it/s, est. speed input: 610.14 toks/s, output: 5121.05 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528/528 [01:28<00:00,  5.95it/s, est. speed input: 616.47 toks/s, output: 5358.96 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/528 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528/528 [00:00<00:00, 12662.34it/s]
{'num_samples': 66, 'num_scores': 528, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 42.4, 'total_acc': 42.42424242424242, 'pass_at_k_percent': {'1': 42.4, '8': 42.4}, 'pass_at_k_valid_counts': {'1': 66, '8': 66}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2/math500/test_think-boxed_-1_seed0_t0.0_s0_e-1_part7.jsonl
[2025-12-03 22:52:53] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2/math500  acc=42.4 pass_at_k={'1': 42.4, '8': 42.4}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [06:10<00:00, 122.48s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [06:10<00:00, 123.35s/ds]
[2025-12-03 22:52:53] ‚úÖ ÂÆåÊàêÔºöB_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300Ôºàg1+g2 Áº∫Â§±Êï∞ÊçÆÈõÜÂ∑≤Ë°•ÂÖ®Ôºâ
[2025-12-03 22:52:53] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:52:53 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:52:53 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:52:53 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:52:59 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:53:06 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:53:06 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9f3d706200>
INFO 12-03 22:53:16 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:53:16 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:53:16 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:53:16 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:53:17 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:53:17 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:53:17 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:53:17 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:53:17 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:53:17 [core.py:396]     self._init_executor()
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:53:17 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:53:17 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:53:17 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:53:17 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:53:17 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:53:17 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:53:17 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:53:17 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:53:17 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:53:17 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:53:17 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:53:17 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:53:17 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:53:17 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:53:17 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:53:17 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:53:17 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:53:17 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:53:17 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:53:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:53:17 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:53:17 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2949697 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2949697 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa1f136c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fa1f1315b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fa1a1e8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fa1f178fb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fa1f179020e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fa1f17a6b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fa1f1792329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fa1e94864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fa1e8ba5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fa1e8ba64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55de6d02dc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55de6cfba2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55de6cfbabbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55de6cfbac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55de6d02db85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55de6d099c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55de6d09cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55de6cfbbc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55de6d0fac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55de6d120407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55de6d120634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55de6d120718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55de6d12075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55de6d120972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55de6d126f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55de6d1271ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55de6d127469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fa1f228bd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fa1f228be40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55de6d0922d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:53:18] ‚ö† Ê®°Âûã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:53:18] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:53:18 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:53:18 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:53:18 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:53:25 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:53:32 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:53:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0516f6a500>
INFO 12-03 22:53:53 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:53:53 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:53:53 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:53:53 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:53:53 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:53:53 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:53:53 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:53:53 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:53:53 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:53:53 [core.py:396]     self._init_executor()
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:53:53 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:53:53 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:53:53 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:53:53 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:53:53 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:53:53 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:53:53 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:53:53 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:53:53 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:53:53 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:53:53 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:53:53 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:53:53 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:53:53 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:53:53 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:53:53 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:53:53 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:53:53 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:53:53 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:53:53 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:53:53 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:53:53 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2950413 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2950413 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f07ca76c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f07ca715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f077b68e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f07cab6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f07cab6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f07cab82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f07cab6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f07c2c864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f07c23a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f07c23a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55bdf69edc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55bdf697a2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55bdf697abbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55bdf697ac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55bdf69edb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55bdf6a59c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55bdf6a5cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55bdf697bc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55bdf6abac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55bdf6ae0407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55bdf6ae0634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55bdf6ae0718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55bdf6ae075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55bdf6ae0972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55bdf6ae6f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55bdf6ae71ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55bdf6ae7469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f07cb926d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f07cb926e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55bdf6a522d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:53:55] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:53:55] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:53:55 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:53:55 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:53:55 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:54:01 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:54:09 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:54:09 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fccea1e9900>
INFO 12-03 22:54:20 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:54:20 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:54:20 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:54:20 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:54:21 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:54:21 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:54:21 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:54:21 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:54:21 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:54:21 [core.py:396]     self._init_executor()
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:54:21 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:54:21 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:54:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:54:21 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:54:21 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:54:21 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:54:21 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:54:21 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:54:21 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:54:21 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:54:21 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:54:21 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:54:21 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:54:21 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:54:21 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:54:21 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:54:21 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:54:21 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:54:21 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:54:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:54:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:54:21 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2951405 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2951405 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fcf9db6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fcf9db15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fcf4ea8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fcf9df6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fcf9df6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fcf9df82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fcf9df6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fcf960864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fcf957a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fcf957a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x56484e2fdc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56484e28a2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56484e28abbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56484e28ac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x56484e2fdb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x56484e369c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x56484e36cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x56484e28bc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x56484e3cac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x56484e3f0407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x56484e3f0634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x56484e3f0718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56484e3f075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x56484e3f0972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x56484e3f6f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56484e3f71ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56484e3f7469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fcf9ed4fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fcf9ed4fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x56484e3622d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:54:22] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:54:22] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:54:22 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:54:22 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:54:22 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:54:29 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:54:36 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:54:36 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5f1b6a6b30>
INFO 12-03 22:54:51 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:54:51 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:54:51 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:54:51 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:54:52 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:54:52 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:54:52 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:54:52 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:54:52 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:54:52 [core.py:396]     self._init_executor()
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:54:52 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:54:52 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:54:52 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:54:52 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:54:52 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:54:52 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:54:52 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:54:52 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:54:52 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:54:52 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:54:52 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:54:52 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:54:52 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:54:52 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:54:52 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:54:52 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:54:52 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:54:52 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:54:52 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:54:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:54:52 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:54:52 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2951975 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2951975 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f61cf36c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f61cf315b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f617fe8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f61cf73ab78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f61cf73b20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f61cf751b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f61cf73d329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f61c74864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f61c6ba5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f61c6ba64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5623ce50fc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5623ce49c2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5623ce49cbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5623ce49cc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5623ce50fb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5623ce57bc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5623ce57ef1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5623ce49dc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5623ce5dcc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5623ce602407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5623ce602634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5623ce602718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5623ce60275b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5623ce602972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5623ce608f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5623ce6091ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5623ce609469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f61d0236d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f61d0236e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5623ce5742d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:54:53] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:54:53] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:54:53 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:54:53 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:54:53 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:55:00 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:55:06 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:55:07 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe0f61bf070>
INFO 12-03 22:55:17 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:55:17 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:55:17 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:55:17 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:55:18 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:55:18 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:55:18 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:55:18 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:55:18 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:55:18 [core.py:396]     self._init_executor()
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:55:18 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:55:18 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:55:18 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:55:18 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:55:18 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:55:18 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:55:18 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:55:18 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:55:18 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:55:18 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:55:18 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:55:18 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:55:18 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:55:18 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:55:18 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:55:18 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:55:18 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:55:18 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:55:18 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:55:18 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:55:18 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:55:18 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2952821 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2952821 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe3a996c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fe3a9915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fe35a88e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fe3a9d6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fe3a9d6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fe3a9d82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fe3a9d6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fe3a1e864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fe3a15a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fe3a15a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55c7d20d2c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55c7d205f2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55c7d205fbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55c7d205fc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55c7d20d2b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55c7d213ec3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55c7d2141f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55c7d2060c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55c7d219fc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55c7d21c5407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55c7d21c5634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55c7d21c5718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55c7d21c575b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55c7d21c5972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55c7d21cbf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55c7d21cc1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55c7d21cc469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fe3aab69d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fe3aab69e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55c7d21372d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:55:19] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:55:19] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:55:19 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:55:19 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:55:19 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:55:25 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:55:31 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:55:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa69c2886a0>
INFO 12-03 22:55:42 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:55:42 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:55:42 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:55:42 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:55:43 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:55:43 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:55:43 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:55:43 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:55:43 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:55:43 [core.py:396]     self._init_executor()
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:55:43 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:55:43 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:55:43 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:55:43 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:55:43 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:55:43 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:55:43 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:55:43 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:55:43 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:55:43 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:55:43 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:55:43 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:55:43 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:55:43 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:55:43 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:55:43 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:55:43 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:55:43 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:55:43 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:55:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:55:43 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:55:43 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2953606 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2953606 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa94ff6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fa94ff15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fa900a8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fa950334b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fa95033520e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fa95034bb0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fa950337329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fa9480864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fa9477a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fa9477a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x56193474bc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5619346d82a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5619346d8bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5619346d8c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x56193474bb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5619347b7c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5619347baf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5619346d9c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x561934818c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x56193483e407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x56193483e634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x56193483e718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56193483e75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x56193483e972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x561934844f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5619348451ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x561934845469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fa950e30d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fa950e30e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5619347b02d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:55:44] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:55:44] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:55:44 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:55:44 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:55:44 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:55:51 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:55:57 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:55:57 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f51ff62abc0>
INFO 12-03 22:56:08 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:56:08 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:56:08 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:56:08 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:56:08 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:56:08 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:56:08 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:56:08 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:56:08 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:56:08 [core.py:396]     self._init_executor()
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:56:08 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:56:08 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:56:08 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:56:08 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:56:08 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:56:08 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:56:08 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:56:08 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:56:08 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:56:08 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:56:08 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:56:08 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:56:08 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:56:08 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:56:08 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:56:08 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:56:08 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:56:08 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:56:08 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:56:08 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:56:08 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:56:08 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2954291 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2954291 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f54b2f6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f54b2f15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f5463e8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f54b336bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f54b336c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f54b3382b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f54b336e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f54ab4864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f54aaba5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f54aaba64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55fba172cc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55fba16b92a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55fba16b9bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55fba16b9c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55fba172cb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55fba1798c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55fba179bf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55fba16bac98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55fba17f9c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55fba181f407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55fba181f634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55fba181f718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55fba181f75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55fba181f972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55fba1825f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55fba18261ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55fba1826469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f54b417ad90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f54b417ae40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55fba17912d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:56:10] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:56:10] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:56:10 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:56:10 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:56:10 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:56:16 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:56:22 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:56:22 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff63269e200>
INFO 12-03 22:56:33 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:56:33 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:56:33 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:56:33 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:56:33 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:56:33 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:56:33 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:56:33 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:56:33 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:56:33 [core.py:396]     self._init_executor()
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:56:33 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:56:33 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:56:33 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:56:33 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:56:33 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:56:33 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:56:33 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:56:33 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:56:33 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:56:33 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:56:33 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:56:33 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:56:33 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:56:33 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:56:33 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:56:33 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:56:33 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:56:33 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:56:33 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:56:33 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:56:33 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:56:33 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2954925 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2954925 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff8e616c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7ff8e6115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7ff896c8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7ff8e655eb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7ff8e655f20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7ff8e6575b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7ff8e6561329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7ff8de2864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7ff8dd9a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7ff8dd9a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x556376475c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5563764022a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x556376402bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x556376402c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x556376475b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5563764e1c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5563764e4f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x556376403c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x556376542c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x556376568407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x556376568634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x556376568718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55637656875b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x556376568972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55637656ef60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55637656f1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55637656f469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7ff8e705ad90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7ff8e705ae40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5563764da2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:56:35] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:56:35] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:56:35 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:56:35 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:56:35 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:56:41 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:56:47 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:56:47 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f796ab5c6a0>
INFO 12-03 22:56:58 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:56:58 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:56:58 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:56:58 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:56:58 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:56:58 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:56:58 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:56:58 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:56:58 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:56:58 [core.py:396]     self._init_executor()
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:56:58 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:56:58 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:56:58 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:56:58 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:56:58 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:56:58 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:56:58 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:56:58 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:56:58 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:56:58 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:56:58 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:56:58 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:56:58 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:56:58 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:56:58 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:56:58 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:56:58 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:56:58 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:56:58 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:56:58 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:56:58 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:56:58 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2955420 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2955420 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f7c1e56c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f7c1e515b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f7bcf48e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f7c1e945b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f7c1e94620e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f7c1e95cb0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f7c1e948329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f7c16a864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f7c161a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f7c161a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55f9f3f19c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55f9f3ea62a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55f9f3ea6bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55f9f3ea6c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55f9f3f19b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55f9f3f85c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55f9f3f88f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55f9f3ea7c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55f9f3fe6c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55f9f400c407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55f9f400c634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55f9f400c718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55f9f400c75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55f9f400c972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55f9f4012f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55f9f40131ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55f9f4013469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f7c1f6f0d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f7c1f6f0e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55f9f3f7e2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:56:59] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:56:59] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:56:59 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:56:59 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:56:59 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:57:05 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:57:11 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:57:12 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe63e9fa4d0>
INFO 12-03 22:57:22 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:57:22 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:57:22 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:57:22 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:57:23 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:57:23 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:57:23 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:57:23 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:57:23 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:57:23 [core.py:396]     self._init_executor()
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:57:23 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:57:23 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:57:23 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:57:23 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:57:23 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:57:23 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:57:23 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:57:23 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:57:23 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:57:23 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:57:23 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:57:23 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:57:23 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:57:23 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:57:23 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:57:23 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:57:23 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:57:23 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:57:23 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:57:23 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:57:23 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:57:23 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2956022 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2956022 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe8f216c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fe8f2115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fe8a308e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fe8f256bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fe8f256c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fe8f2582b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fe8f256e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fe8ea6864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fe8e9da5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fe8e9da64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55cf98d5bc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55cf98ce82a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55cf98ce8bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55cf98ce8c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55cf98d5bb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55cf98dc7c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55cf98dcaf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55cf98ce9c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55cf98e28c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55cf98e4e407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55cf98e4e634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55cf98e4e718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55cf98e4e75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55cf98e4e972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55cf98e54f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55cf98e551ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55cf98e55469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fe8f339fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fe8f339fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55cf98dc02d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:57:24] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:57:24] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:57:24 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:57:24 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:57:24 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:57:30 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:57:36 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:57:36 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb6db52d8a0>
INFO 12-03 22:57:47 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:57:47 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:57:47 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:57:47 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:57:47 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:57:47 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:57:47 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:57:47 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:57:47 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:57:47 [core.py:396]     self._init_executor()
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:57:47 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:57:47 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:57:47 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:57:47 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:57:47 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:57:47 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:57:47 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:57:47 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:57:47 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:57:47 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:57:47 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:57:47 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:57:47 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:57:47 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:57:47 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:57:47 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:57:47 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:57:47 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:57:47 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:57:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:57:47 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:57:47 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2956611 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2956611 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fb98f16c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fb98f115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fb93fc8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fb98f5a9b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fb98f5aa20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fb98f5c0b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fb98f5ac329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fb9872864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fb9869a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fb9869a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x559f105a4c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x559f105312a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x559f10531bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x559f10531c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x559f105a4b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x559f10610c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x559f10613f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x559f10532c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x559f10671c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x559f10697407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x559f10697634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x559f10697718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x559f1069775b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x559f10697972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x559f1069df60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x559f1069e1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x559f1069e469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fb9900a5d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fb9900a5e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x559f106092d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:57:49] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:57:49] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:57:49 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:57:49 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:57:49 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:57:55 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:58:01 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:58:01 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f774df3e500>
INFO 12-03 22:58:12 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:58:12 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:58:12 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:58:12 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:58:12 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:58:12 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:58:12 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:58:12 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:58:12 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:58:12 [core.py:396]     self._init_executor()
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:58:12 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:58:12 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:58:12 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:58:12 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:58:12 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:58:12 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:58:12 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:58:12 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:58:12 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:58:12 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:58:12 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:58:12 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:58:12 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:58:12 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:58:12 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:58:12 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:58:12 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:58:12 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:58:12 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:58:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:58:12 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:58:12 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2957305 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2957305 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f7a0176c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f7a01715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f79b268e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f7a01b5fb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f7a01b6020e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f7a01b76b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f7a01b62329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f79f9c864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f79f93a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f79f93a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5562d000ec04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5562cff9b2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5562cff9bbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5562cff9bc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5562d000eb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5562d007ac3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5562d007df1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5562cff9cc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5562d00dbc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5562d0101407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5562d0101634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5562d0101718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5562d010175b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5562d0101972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5562d0107f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5562d01081ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5562d0108469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f7a02915d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f7a02915e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5562d00732d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:58:14] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:58:14] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:58:14 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:58:14 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:58:14 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:58:20 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:58:26 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:58:26 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f39a4766b30>
INFO 12-03 22:58:37 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:58:37 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:58:37 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:58:37 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:58:38 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:58:38 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:58:38 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:58:38 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:58:38 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:58:38 [core.py:396]     self._init_executor()
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:58:38 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:58:38 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:58:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:58:38 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:58:38 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:58:38 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:58:38 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:58:38 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:58:38 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:58:38 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:58:38 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:58:38 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:58:38 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:58:38 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:58:38 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:58:38 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:58:38 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:58:38 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:58:38 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:58:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:58:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:58:38 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2957823 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2957823 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3c5816c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f3c58115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f3c0908e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f3c58545b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f3c5854620e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f3c5855cb0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f3c58548329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f3c506864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f3c4fda5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f3c4fda64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55806e6c9c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55806e6562a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55806e656bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55806e656c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55806e6c9b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55806e735c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55806e738f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55806e657c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55806e796c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55806e7bc407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55806e7bc634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55806e7bc718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55806e7bc75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55806e7bc972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55806e7c2f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55806e7c31ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55806e7c3469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f3c59303d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f3c59303e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55806e72e2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:58:40] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:58:40] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:58:40 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:58:40 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:58:40 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:58:46 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:58:53 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:58:53 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ef97a255ea0>
INFO 12-03 22:59:03 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:59:03 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:59:03 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:59:04 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:59:04 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:59:04 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:59:04 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:59:04 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:59:04 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:59:04 [core.py:396]     self._init_executor()
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:59:04 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:59:04 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:59:04 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:59:04 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:59:04 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:59:04 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:59:04 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:59:04 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:59:04 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:59:04 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:59:04 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:59:04 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:59:04 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:59:04 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:59:04 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:59:04 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:59:04 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:59:04 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:59:04 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:59:04 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:59:04 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:59:04 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2958538 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2958538 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-12-03 22:59:05] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:59:05] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:59:05 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:59:05 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:59:05 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:59:11 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:59:18 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:59:18 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f28b19164a0>
INFO 12-03 22:59:28 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:59:28 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:59:28 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:59:29 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:59:29 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:59:29 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:59:29 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:59:29 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:59:29 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:59:29 [core.py:396]     self._init_executor()
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:59:29 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:59:29 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:59:29 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:59:29 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:59:29 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:59:29 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:59:29 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:59:29 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:59:29 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:59:29 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:59:29 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:59:29 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:59:29 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:59:29 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:59:29 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:59:29 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:59:29 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:59:29 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:59:29 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:59:29 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:59:29 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:59:29 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2958992 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2958992 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2b6556c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f2b65515b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f2b1608e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f2b65999b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f2b6599a20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f2b659b0b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f2b6599c329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f2b5d6864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f2b5cda5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f2b5cda64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x56505e339c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56505e2c62a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56505e2c6bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56505e2c6c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x56505e339b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x56505e3a5c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x56505e3a8f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x56505e2c7c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x56505e406c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x56505e42c407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x56505e42c634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x56505e42c718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56505e42c75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x56505e42c972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x56505e432f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56505e4331ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56505e433469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f2b66495d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f2b66495e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x56505e39e2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:59:30] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:59:30] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:59:30 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:59:30 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:59:30 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:59:36 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:59:42 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:59:43 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f393cd0a200>
INFO 12-03 22:59:53 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:59:53 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:59:53 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:59:53 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:59:54 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:59:54 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:59:54 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:59:54 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:59:54 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:59:54 [core.py:396]     self._init_executor()
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:59:54 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:59:54 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:59:54 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:59:54 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:59:54 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:59:54 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:59:54 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:59:54 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:59:54 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:59:54 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:59:54 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:59:54 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:59:54 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:59:54 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:59:54 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:59:54 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:59:54 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:59:54 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:59:54 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:59:54 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:59:54 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:59:54 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2959683 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2959683 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3bf096c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f3bf0915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f3ba148e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f3bf0da0b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f3bf0da120e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f3bf0db7b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f3bf0da3329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f3be8a864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f3be81a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f3be81a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55bbd90fdc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55bbd908a2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55bbd908abbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55bbd908ac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55bbd90fdb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55bbd9169c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55bbd916cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55bbd908bc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55bbd91cac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55bbd91f0407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55bbd91f0634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55bbd91f0718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55bbd91f075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55bbd91f0972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55bbd91f6f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55bbd91f71ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55bbd91f7469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f3bf189cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f3bf189ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55bbd91622d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:59:55] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:59:55] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:59:55 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-03 22:59:55 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:59:55 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 23:00:01 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 23:00:07 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 23:00:08 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe71ac324a0>
INFO 12-03 23:00:18 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 23:00:18 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 23:00:18 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 23:00:18 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 23:00:19 [core.py:396] EngineCore failed to start.
ERROR 12-03 23:00:19 [core.py:396] Traceback (most recent call last):
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 23:00:19 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 23:00:19 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 23:00:19 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 23:00:19 [core.py:396]     self._init_executor()
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 23:00:19 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 23:00:19 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 23:00:19 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 23:00:19 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 23:00:19 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 23:00:19 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 23:00:19 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 23:00:19 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 23:00:19 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 23:00:19 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 23:00:19 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 23:00:19 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 23:00:19 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 23:00:19 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 23:00:19 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 23:00:19 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 23:00:19 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 23:00:19 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 23:00:19 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 23:00:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 23:00:19 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 23:00:19 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2960238 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 2934824 has 35.91 GiB memory in use. Process 2960238 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe9ce76c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fe9ce715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fe97f28e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fe9ceae0b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fe9ceae120e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fe9ceaf7b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fe9ceae3329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fe9c68864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fe9c5fa5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fe9c5fa64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x56472b8b7c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56472b8442a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56472b844bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56472b844c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x56472b8b7b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x56472b923c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x56472b926f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x56472b845c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x56472b984c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x56472b9aa407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x56472b9aa634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x56472b9aa718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56472b9aa75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x56472b9aa972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x56472b9b0f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56472b9b11ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56472b9b1469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fe9cf5dcd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fe9cf5dce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x56472b91c2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 23:00:20] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
