INFO 12-03 22:27:17 [__init__.py:239] Automatically detected platform cuda.
[2025-12-03 22:28:24] ÂèëÁé∞ 7 ‰∏™ run„ÄÇ
[2025-12-03 22:28:24] ‚è≠ Ë∑≥Ëøá base-onlyÔºöLlama-3.2-3B-Instruct
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_100
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_200
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_300
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_313
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_313
[2025-12-03 22:28:24] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100
INFO 12-03 22:28:30 [__init__.py:239] Automatically detected platform cuda.
[2025-12-03 22:28:39] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:29:01 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:29:01 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:29:01 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:29:11 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:29:19 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:29:24 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc090e06c80>
INFO 12-03 22:29:57 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:29:57 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:29:57 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:29:58 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:25<00:25, 25.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:26<00:00, 10.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:26<00:00, 13.12s/it]

INFO 12-03 22:30:26 [loader.py:458] Loading weights took 26.27 seconds
INFO 12-03 22:30:27 [gpu_model_runner.py:1347] Model loading took 6.0160 GiB and 28.723145 seconds
INFO 12-03 22:30:30 [kv_cache_utils.py:634] GPU KV cache size: 258,240 tokens
INFO 12-03 22:30:30 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.97x
INFO 12-03 22:30:30 [core.py:159] init engine (profile, create kv cache, warmup model) took 3.30 seconds
INFO 12-03 22:30:30 [core_client.py:439] Core engine process 0 ready.
[2025-12-03 22:30:30] ‚ÑπÔ∏è  ÂΩìÂâçÂ∑•‰ΩúËäÇÁÇπÂàÜÁâá: 1/8
[2025-12-03 22:30:30] ‚úì Ê®°ÂûãÂ∞±Áª™ÔºåÂºÄÂßãËØÑÊµã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200ÔºàÂÖ±‰∫´Âêå‰∏Ä LLMÔºå‰ªÖË°•Áº∫Êï∞ÊçÆÈõÜÔºâ
[2025-12-03 22:30:30] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1  ÂæÖËØÑÊµã=['aime25x8', 'amc23x8', 'aime24x8']  T=0.6  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 1/8 handling range [30:60]
==================================================
data: aime25x8  ,remain samples: 30
{'idx': 30, 'problem': 'Find the sum of all integer bases $b>9$ for which $17_{b}$ is a divisor of $97_{b}$.', 'answer': '70'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 328.72it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:06<26:51,  6.74s/it, est. speed input: 23.14 toks/s, output: 35.90 toks/s][A
Processed prompts:   1%|‚ñè         | 3/240 [00:07<08:20,  2.11s/it, est. speed input: 56.45 toks/s, output: 99.82 toks/s][A
Processed prompts:   2%|‚ñè         | 4/240 [00:07<05:43,  1.46s/it, est. speed input: 81.75 toks/s, output: 134.03 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:08<04:03,  1.04s/it, est. speed input: 91.88 toks/s, output: 167.89 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:09<04:26,  1.14s/it, est. speed input: 89.65 toks/s, output: 181.52 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:09<03:25,  1.13it/s, est. speed input: 100.65 toks/s, output: 213.14 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:10<03:10,  1.22it/s, est. speed input: 101.16 toks/s, output: 237.01 toks/s][A
Processed prompts:   4%|‚ñç         | 9/240 [00:10<02:42,  1.42it/s, est. speed input: 111.36 toks/s, output: 265.26 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:11<02:05,  1.84it/s, est. speed input: 119.53 toks/s, output: 298.74 toks/s][A
Processed prompts:   5%|‚ñå         | 12/240 [00:11<01:15,  3.00it/s, est. speed input: 150.47 toks/s, output: 369.13 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:11<00:55,  4.04it/s, est. speed input: 167.07 toks/s, output: 436.59 toks/s][A
Processed prompts:   6%|‚ñã         | 15/240 [00:11<00:53,  4.19it/s, est. speed input: 172.39 toks/s, output: 466.72 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:11<00:47,  4.70it/s, est. speed input: 179.84 toks/s, output: 499.39 toks/s][A
Processed prompts:   7%|‚ñã         | 17/240 [00:11<00:42,  5.23it/s, est. speed input: 186.07 toks/s, output: 531.87 toks/s][A
Processed prompts:   8%|‚ñä         | 18/240 [00:12<00:49,  4.47it/s, est. speed input: 194.02 toks/s, output: 556.23 toks/s][A
Processed prompts:   8%|‚ñä         | 19/240 [00:12<00:44,  4.98it/s, est. speed input: 212.38 toks/s, output: 587.74 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:12<00:35,  6.15it/s, est. speed input: 231.94 toks/s, output: 681.26 toks/s][A
Processed prompts:  11%|‚ñà         | 26/240 [00:13<00:24,  8.87it/s, est. speed input: 258.29 toks/s, output: 817.48 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:13<00:27,  7.63it/s, est. speed input: 263.40 toks/s, output: 841.15 toks/s][A
Processed prompts:  12%|‚ñà‚ñé        | 30/240 [00:13<00:30,  6.94it/s, est. speed input: 277.55 toks/s, output: 922.52 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:14<00:32,  6.48it/s, est. speed input: 283.19 toks/s, output: 946.72 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 34/240 [00:14<00:27,  7.46it/s, est. speed input: 311.95 toks/s, output: 1038.28 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 35/240 [00:14<00:27,  7.49it/s, est. speed input: 318.60 toks/s, output: 1066.85 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 36/240 [00:14<00:26,  7.82it/s, est. speed input: 322.23 toks/s, output: 1097.17 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 37/240 [00:14<00:28,  7.03it/s, est. speed input: 326.49 toks/s, output: 1120.53 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 38/240 [00:15<00:44,  4.53it/s, est. speed input: 321.82 toks/s, output: 1122.85 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 40/240 [00:15<00:34,  5.72it/s, est. speed input: 327.98 toks/s, output: 1182.57 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 42/240 [00:15<00:25,  7.66it/s, est. speed input: 352.35 toks/s, output: 1249.81 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 44/240 [00:15<00:22,  8.53it/s, est. speed input: 375.04 toks/s, output: 1310.21 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 46/240 [00:15<00:20,  9.56it/s, est. speed input: 387.36 toks/s, output: 1372.51 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 48/240 [00:16<00:21,  8.87it/s, est. speed input: 405.24 toks/s, output: 1425.67 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 50/240 [00:16<00:19,  9.82it/s, est. speed input: 412.65 toks/s, output: 1487.48 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 52/240 [00:16<00:29,  6.48it/s, est. speed input: 421.93 toks/s, output: 1514.03 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 53/240 [00:16<00:27,  6.88it/s, est. speed input: 428.65 toks/s, output: 1542.52 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 55/240 [00:17<00:29,  6.34it/s, est. speed input: 439.94 toks/s, output: 1585.02 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 56/240 [00:17<00:27,  6.77it/s, est. speed input: 445.53 toks/s, output: 1613.19 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 57/240 [00:17<00:27,  6.69it/s, est. speed input: 454.87 toks/s, output: 1636.64 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 59/240 [00:17<00:23,  7.86it/s, est. speed input: 471.50 toks/s, output: 1695.16 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 60/240 [00:18<00:45,  3.96it/s, est. speed input: 461.05 toks/s, output: 1667.96 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 62/240 [00:18<00:31,  5.67it/s, est. speed input: 472.05 toks/s, output: 1733.92 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 64/240 [00:18<00:26,  6.60it/s, est. speed input: 480.36 toks/s, output: 1789.94 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 66/240 [00:18<00:21,  8.15it/s, est. speed input: 491.48 toks/s, output: 1853.01 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 68/240 [00:19<00:19,  8.87it/s, est. speed input: 505.17 toks/s, output: 1910.78 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 70/240 [00:19<00:16, 10.23it/s, est. speed input: 513.02 toks/s, output: 1973.34 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 72/240 [00:19<00:16, 10.42it/s, est. speed input: 528.64 toks/s, output: 2029.92 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 74/240 [00:19<00:19,  8.73it/s, est. speed input: 533.38 toks/s, output: 2072.90 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 76/240 [00:19<00:15, 10.48it/s, est. speed input: 544.79 toks/s, output: 2137.72 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 78/240 [00:19<00:13, 11.61it/s, est. speed input: 567.93 toks/s, output: 2199.47 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:20<00:28,  5.65it/s, est. speed input: 556.93 toks/s, output: 2192.28 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 82/240 [00:21<00:28,  5.49it/s, est. speed input: 558.30 toks/s, output: 2227.16 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 84/240 [00:21<00:27,  5.73it/s, est. speed input: 569.79 toks/s, output: 2269.75 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 85/240 [00:22<00:42,  3.66it/s, est. speed input: 560.66 toks/s, output: 2233.04 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 88/240 [00:22<00:29,  5.19it/s, est. speed input: 572.55 toks/s, output: 2320.15 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 90/240 [00:22<00:23,  6.28it/s, est. speed input: 586.09 toks/s, output: 2379.78 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 93/240 [00:22<00:18,  8.03it/s, est. speed input: 606.62 toks/s, output: 2471.40 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 95/240 [00:23<00:34,  4.16it/s, est. speed input: 599.71 toks/s, output: 2431.41 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 96/240 [00:24<00:33,  4.33it/s, est. speed input: 598.73 toks/s, output: 2450.90 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:24<00:28,  4.91it/s, est. speed input: 605.19 toks/s, output: 2497.56 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 99/240 [00:24<00:29,  4.77it/s, est. speed input: 602.03 toks/s, output: 2511.61 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 100/240 [00:24<00:31,  4.43it/s, est. speed input: 598.55 toks/s, output: 2520.49 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 101/240 [00:25<00:29,  4.76it/s, est. speed input: 599.22 toks/s, output: 2542.64 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/240 [00:25<00:29,  4.62it/s, est. speed input: 602.88 toks/s, output: 2556.89 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 103/240 [00:25<00:34,  4.01it/s, est. speed input: 599.25 toks/s, output: 2560.79 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 104/240 [00:25<00:28,  4.76it/s, est. speed input: 600.81 toks/s, output: 2588.30 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:26<00:22,  6.01it/s, est. speed input: 620.46 toks/s, output: 2662.92 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 108/240 [00:26<00:27,  4.74it/s, est. speed input: 613.92 toks/s, output: 2662.33 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/240 [00:26<00:28,  4.64it/s, est. speed input: 618.93 toks/s, output: 2677.32 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 110/240 [00:27<00:29,  4.44it/s, est. speed input: 617.71 toks/s, output: 2689.91 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 111/240 [00:27<00:27,  4.68it/s, est. speed input: 617.43 toks/s, output: 2710.21 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 112/240 [00:27<00:44,  2.86it/s, est. speed input: 606.39 toks/s, output: 2678.37 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/240 [00:28<00:23,  5.26it/s, est. speed input: 619.58 toks/s, output: 2815.82 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 118/240 [00:28<00:26,  4.56it/s, est. speed input: 617.03 toks/s, output: 2817.51 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/240 [00:29<00:29,  4.08it/s, est. speed input: 617.10 toks/s, output: 2835.22 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:30<00:45,  2.60it/s, est. speed input: 596.68 toks/s, output: 2782.09 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 123/240 [00:31<00:42,  2.73it/s, est. speed input: 595.56 toks/s, output: 2794.98 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:31<00:38,  3.02it/s, est. speed input: 599.81 toks/s, output: 2816.42 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:32<00:49,  2.33it/s, est. speed input: 590.82 toks/s, output: 2787.66 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 126/240 [00:32<00:53,  2.14it/s, est. speed input: 584.25 toks/s, output: 2776.82 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 127/240 [00:33<00:51,  2.21it/s, est. speed input: 579.21 toks/s, output: 2780.88 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:34<01:14,  1.51it/s, est. speed input: 563.54 toks/s, output: 2720.31 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:34<01:03,  1.74it/s, est. speed input: 564.06 toks/s, output: 2731.90 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 130/240 [00:34<00:55,  1.97it/s, est. speed input: 560.95 toks/s, output: 2743.62 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 132/240 [00:35<00:50,  2.14it/s, est. speed input: 553.91 toks/s, output: 2756.66 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 134/240 [00:36<00:34,  3.11it/s, est. speed input: 558.07 toks/s, output: 2819.62 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:36<00:29,  3.53it/s, est. speed input: 563.50 toks/s, output: 2847.34 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 137/240 [00:36<00:29,  3.47it/s, est. speed input: 561.39 toks/s, output: 2878.62 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:37<00:33,  3.05it/s, est. speed input: 556.86 toks/s, output: 2880.85 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:37<00:34,  2.94it/s, est. speed input: 552.95 toks/s, output: 2890.70 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 140/240 [00:38<00:39,  2.55it/s, est. speed input: 549.16 toks/s, output: 2888.24 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:39<00:51,  1.91it/s, est. speed input: 539.37 toks/s, output: 2861.97 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 142/240 [00:39<00:43,  2.25it/s, est. speed input: 542.36 toks/s, output: 2883.61 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:40<01:17,  1.26it/s, est. speed input: 526.00 toks/s, output: 2803.74 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:41<00:58,  1.64it/s, est. speed input: 529.82 toks/s, output: 2832.98 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:41<00:45,  2.07it/s, est. speed input: 530.48 toks/s, output: 2860.32 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 146/240 [00:44<01:58,  1.26s/it, est. speed input: 496.04 toks/s, output: 2697.39 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [00:44<01:26,  1.07it/s, est. speed input: 500.53 toks/s, output: 2727.38 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 148/240 [00:45<01:17,  1.19it/s, est. speed input: 498.48 toks/s, output: 2729.78 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:45<01:08,  1.32it/s, est. speed input: 495.15 toks/s, output: 2735.50 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [00:50<02:53,  1.93s/it, est. speed input: 453.78 toks/s, output: 2520.38 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [00:51<02:31,  1.70s/it, est. speed input: 445.90 toks/s, output: 2502.87 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:51<01:47,  1.22s/it, est. speed input: 448.01 toks/s, output: 2536.60 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/240 [00:51<01:22,  1.06it/s, est. speed input: 447.63 toks/s, output: 2561.14 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 154/240 [00:54<02:05,  1.46s/it, est. speed input: 430.62 toks/s, output: 2475.84 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155/240 [00:57<02:39,  1.87s/it, est. speed input: 410.39 toks/s, output: 2391.76 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [01:00<03:14,  2.31s/it, est. speed input: 390.00 toks/s, output: 2299.21 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 157/240 [01:08<05:14,  3.79s/it, est. speed input: 349.45 toks/s, output: 2092.53 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 158/240 [01:14<06:04,  4.45s/it, est. speed input: 322.94 toks/s, output: 1960.89 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 160/240 [01:23<05:59,  4.50s/it, est. speed input: 290.03 toks/s, output: 1816.42 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:23<00:00,  2.88it/s, est. speed input: 435.10 toks/s, output: 4767.65 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 19085.98it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 0.0, 'total_acc': 0.4166666666666667, 'pass_at_k_percent': {'1': 0.4, '8': 3.3}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime25x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 22:31:55] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime25x8  acc=0.0 pass_at_k={'1': 0.4, '8': 3.3}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:24<02:49, 84.60s/ds][Info] Sharding enabled: Process 1/8 handling range [40:80]
==================================================
data: amc23x8  ,remain samples: 40
{'idx': 40, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}

  0%|          | 0/40 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:00<00:00, 442.93it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/320 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 2/320 [00:00<00:18, 17.45it/s, est. speed input: 1536.10 toks/s, output: 52.36 toks/s][A
Processed prompts:   1%|‚ñè         | 4/320 [00:04<06:58,  1.32s/it, est. speed input: 80.42 toks/s, output: 92.97 toks/s]  [A
Processed prompts:   2%|‚ñè         | 5/320 [00:04<05:12,  1.01it/s, est. speed input: 100.54 toks/s, output: 135.40 toks/s][A
Processed prompts:   2%|‚ñè         | 7/320 [00:04<02:57,  1.76it/s, est. speed input: 135.14 toks/s, output: 222.84 toks/s][A
Processed prompts:   2%|‚ñé         | 8/320 [00:05<02:30,  2.07it/s, est. speed input: 142.86 toks/s, output: 258.60 toks/s][A
Processed prompts:   3%|‚ñé         | 9/320 [00:05<02:16,  2.28it/s, est. speed input: 155.02 toks/s, output: 288.95 toks/s][A
Processed prompts:   3%|‚ñé         | 11/320 [00:05<01:25,  3.62it/s, est. speed input: 191.98 toks/s, output: 373.08 toks/s][A
Processed prompts:   4%|‚ñç         | 14/320 [00:05<01:06,  4.63it/s, est. speed input: 235.67 toks/s, output: 473.51 toks/s][A
Processed prompts:   5%|‚ñå         | 17/320 [00:06<00:52,  5.81it/s, est. speed input: 275.99 toks/s, output: 578.83 toks/s][A
Processed prompts:   6%|‚ñå         | 18/320 [00:06<00:48,  6.18it/s, est. speed input: 288.61 toks/s, output: 613.30 toks/s][A
Processed prompts:   6%|‚ñã         | 20/320 [00:06<00:40,  7.37it/s, est. speed input: 298.14 toks/s, output: 685.44 toks/s][A
Processed prompts:   7%|‚ñã         | 22/320 [00:07<00:59,  4.99it/s, est. speed input: 288.59 toks/s, output: 705.20 toks/s][A
Processed prompts:   7%|‚ñã         | 23/320 [00:07<00:54,  5.42it/s, est. speed input: 302.26 toks/s, output: 737.78 toks/s][A
Processed prompts:   8%|‚ñä         | 24/320 [00:07<00:53,  5.55it/s, est. speed input: 304.87 toks/s, output: 764.95 toks/s][A
Processed prompts:   8%|‚ñä         | 25/320 [00:07<00:57,  5.16it/s, est. speed input: 308.98 toks/s, output: 784.37 toks/s][A
Processed prompts:   8%|‚ñä         | 27/320 [00:07<00:43,  6.76it/s, est. speed input: 339.17 toks/s, output: 854.36 toks/s][A
Processed prompts:   9%|‚ñâ         | 30/320 [00:08<00:37,  7.66it/s, est. speed input: 361.22 toks/s, output: 945.73 toks/s][A
Processed prompts:  10%|‚ñà         | 32/320 [00:08<00:30,  9.35it/s, est. speed input: 382.78 toks/s, output: 1018.46 toks/s][A
Processed prompts:  11%|‚ñà         | 34/320 [00:08<00:34,  8.24it/s, est. speed input: 388.95 toks/s, output: 1065.28 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 37/320 [00:08<00:29,  9.60it/s, est. speed input: 412.39 toks/s, output: 1162.09 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 39/320 [00:09<00:28,  9.94it/s, est. speed input: 420.07 toks/s, output: 1221.98 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 41/320 [00:09<00:27, 10.20it/s, est. speed input: 436.28 toks/s, output: 1280.98 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 43/320 [00:09<00:27, 10.00it/s, est. speed input: 443.84 toks/s, output: 1294.14 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 45/320 [00:09<00:24, 11.33it/s, est. speed input: 454.58 toks/s, output: 1319.32 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 47/320 [00:09<00:21, 12.52it/s, est. speed input: 467.54 toks/s, output: 1303.75 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 50/320 [00:09<00:19, 13.66it/s, est. speed input: 485.14 toks/s, output: 1360.96 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 52/320 [00:10<00:22, 12.14it/s, est. speed input: 490.18 toks/s, output: 1413.78 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 54/320 [00:10<00:27,  9.63it/s, est. speed input: 487.44 toks/s, output: 1451.78 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 56/320 [00:10<00:27,  9.52it/s, est. speed input: 493.43 toks/s, output: 1503.00 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 58/320 [00:10<00:24, 10.57it/s, est. speed input: 498.24 toks/s, output: 1564.24 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 60/320 [00:11<00:27,  9.37it/s, est. speed input: 502.16 toks/s, output: 1605.97 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 63/320 [00:11<00:28,  9.14it/s, est. speed input: 516.60 toks/s, output: 1675.95 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 64/320 [00:11<00:28,  8.96it/s, est. speed input: 517.41 toks/s, output: 1697.94 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 67/320 [00:11<00:21, 11.94it/s, est. speed input: 539.38 toks/s, output: 1798.52 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 70/320 [00:11<00:17, 14.39it/s, est. speed input: 555.11 toks/s, output: 1897.04 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 74/320 [00:12<00:16, 15.30it/s, est. speed input: 594.31 toks/s, output: 2016.71 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 77/320 [00:12<00:13, 17.82it/s, est. speed input: 614.48 toks/s, output: 2117.85 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 80/320 [00:12<00:16, 14.62it/s, est. speed input: 630.91 toks/s, output: 2186.67 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 83/320 [00:12<00:16, 14.53it/s, est. speed input: 643.41 toks/s, output: 2245.84 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 86/320 [00:13<00:20, 11.42it/s, est. speed input: 643.18 toks/s, output: 2273.17 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 88/320 [00:13<00:24,  9.40it/s, est. speed input: 641.78 toks/s, output: 2269.19 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 90/320 [00:13<00:23,  9.70it/s, est. speed input: 648.24 toks/s, output: 2295.53 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 94/320 [00:13<00:17, 12.96it/s, est. speed input: 675.63 toks/s, output: 2425.89 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 96/320 [00:13<00:18, 11.97it/s, est. speed input: 676.83 toks/s, output: 2467.63 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 98/320 [00:14<00:18, 12.16it/s, est. speed input: 681.16 toks/s, output: 2518.83 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 101/320 [00:14<00:15, 14.54it/s, est. speed input: 702.29 toks/s, output: 2588.81 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 104/320 [00:14<00:14, 14.53it/s, est. speed input: 716.10 toks/s, output: 2648.47 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 107/320 [00:14<00:14, 14.59it/s, est. speed input: 737.66 toks/s, output: 2711.97 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 109/320 [00:14<00:13, 15.50it/s, est. speed input: 746.44 toks/s, output: 2771.76 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 111/320 [00:14<00:12, 16.34it/s, est. speed input: 756.54 toks/s, output: 2805.79 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 113/320 [00:14<00:14, 14.60it/s, est. speed input: 765.90 toks/s, output: 2851.12 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 115/320 [00:15<00:17, 11.83it/s, est. speed input: 768.66 toks/s, output: 2881.97 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 121/320 [00:15<00:10, 18.22it/s, est. speed input: 810.78 toks/s, output: 3015.11 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 123/320 [00:15<00:11, 17.70it/s, est. speed input: 815.80 toks/s, output: 3049.08 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 125/320 [00:16<00:18, 10.52it/s, est. speed input: 807.57 toks/s, output: 3020.03 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 127/320 [00:16<00:16, 11.87it/s, est. speed input: 824.17 toks/s, output: 3079.94 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 131/320 [00:16<00:12, 15.36it/s, est. speed input: 844.58 toks/s, output: 3183.81 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 133/320 [00:16<00:12, 15.50it/s, est. speed input: 852.72 toks/s, output: 3238.31 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 135/320 [00:16<00:16, 11.32it/s, est. speed input: 853.78 toks/s, output: 3239.40 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 137/320 [00:16<00:14, 12.24it/s, est. speed input: 859.38 toks/s, output: 3279.61 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 140/320 [00:17<00:13, 13.12it/s, est. speed input: 871.11 toks/s, output: 3359.08 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 142/320 [00:17<00:15, 11.74it/s, est. speed input: 875.85 toks/s, output: 3372.84 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 147/320 [00:17<00:12, 14.19it/s, est. speed input: 904.59 toks/s, output: 3500.85 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 149/320 [00:17<00:16, 10.55it/s, est. speed input: 901.22 toks/s, output: 3486.97 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 151/320 [00:18<00:18,  9.34it/s, est. speed input: 899.15 toks/s, output: 3509.01 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 154/320 [00:18<00:15, 11.06it/s, est. speed input: 915.42 toks/s, output: 3594.42 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 156/320 [00:18<00:20,  8.02it/s, est. speed input: 905.97 toks/s, output: 3564.44 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 158/320 [00:18<00:17,  9.22it/s, est. speed input: 918.27 toks/s, output: 3620.18 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 160/320 [00:19<00:18,  8.66it/s, est. speed input: 918.35 toks/s, output: 3634.51 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 163/320 [00:19<00:13, 11.34it/s, est. speed input: 940.01 toks/s, output: 3719.27 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 167/320 [00:19<00:10, 14.41it/s, est. speed input: 954.55 toks/s, output: 3788.79 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 169/320 [00:19<00:14, 10.26it/s, est. speed input: 947.97 toks/s, output: 3772.57 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 171/320 [00:20<00:20,  7.36it/s, est. speed input: 941.98 toks/s, output: 3747.49 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 174/320 [00:20<00:16,  8.75it/s, est. speed input: 946.26 toks/s, output: 3808.61 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 176/320 [00:21<00:22,  6.34it/s, est. speed input: 932.61 toks/s, output: 3783.08 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 177/320 [00:21<00:22,  6.41it/s, est. speed input: 930.27 toks/s, output: 3797.08 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 181/320 [00:21<00:13, 10.20it/s, est. speed input: 946.22 toks/s, output: 3918.91 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 183/320 [00:21<00:14,  9.15it/s, est. speed input: 945.73 toks/s, output: 3946.53 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 185/320 [00:21<00:14,  9.21it/s, est. speed input: 949.96 toks/s, output: 3987.49 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 187/320 [00:22<00:24,  5.38it/s, est. speed input: 925.90 toks/s, output: 3928.74 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 190/320 [00:22<00:18,  7.10it/s, est. speed input: 934.80 toks/s, output: 4000.75 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 192/320 [00:23<00:16,  7.57it/s, est. speed input: 938.93 toks/s, output: 4023.78 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 196/320 [00:23<00:12,  9.93it/s, est. speed input: 952.27 toks/s, output: 4125.32 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 198/320 [00:23<00:16,  7.19it/s, est. speed input: 942.11 toks/s, output: 4081.66 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 200/320 [00:24<00:15,  7.64it/s, est. speed input: 941.38 toks/s, output: 4089.41 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 201/320 [00:24<00:17,  6.97it/s, est. speed input: 936.18 toks/s, output: 4076.11 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 203/320 [00:24<00:15,  7.38it/s, est. speed input: 943.61 toks/s, output: 4103.80 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 205/320 [00:24<00:15,  7.51it/s, est. speed input: 947.20 toks/s, output: 4125.78 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 208/320 [00:25<00:11,  9.73it/s, est. speed input: 953.45 toks/s, output: 4201.99 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 210/320 [00:25<00:11,  9.43it/s, est. speed input: 955.39 toks/s, output: 4244.80 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 212/320 [00:25<00:14,  7.30it/s, est. speed input: 953.79 toks/s, output: 4254.06 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 214/320 [00:25<00:14,  7.52it/s, est. speed input: 955.42 toks/s, output: 4280.02 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 215/320 [00:26<00:13,  7.62it/s, est. speed input: 955.13 toks/s, output: 4300.62 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 217/320 [00:26<00:11,  8.70it/s, est. speed input: 957.71 toks/s, output: 4340.01 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 218/320 [00:27<00:28,  3.56it/s, est. speed input: 929.08 toks/s, output: 4222.35 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 220/320 [00:27<00:23,  4.29it/s, est. speed input: 926.85 toks/s, output: 4259.56 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 221/320 [00:27<00:22,  4.40it/s, est. speed input: 922.78 toks/s, output: 4269.16 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 222/320 [00:28<00:37,  2.61it/s, est. speed input: 895.66 toks/s, output: 4172.57 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 223/320 [00:28<00:30,  3.17it/s, est. speed input: 895.11 toks/s, output: 4198.91 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 224/320 [00:28<00:25,  3.83it/s, est. speed input: 895.13 toks/s, output: 4225.27 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 225/320 [00:29<00:24,  3.87it/s, est. speed input: 891.15 toks/s, output: 4230.02 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 226/320 [00:29<00:33,  2.81it/s, est. speed input: 875.31 toks/s, output: 4169.49 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 228/320 [00:30<00:42,  2.15it/s, est. speed input: 854.43 toks/s, output: 4088.05 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 229/320 [00:31<00:53,  1.69it/s, est. speed input: 832.66 toks/s, output: 4003.15 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 230/320 [00:32<00:49,  1.82it/s, est. speed input: 825.19 toks/s, output: 3991.49 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 231/320 [00:33<00:52,  1.70it/s, est. speed input: 815.39 toks/s, output: 3942.78 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 233/320 [00:33<00:36,  2.38it/s, est. speed input: 812.29 toks/s, output: 3967.94 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 234/320 [00:34<00:44,  1.92it/s, est. speed input: 794.08 toks/s, output: 3911.33 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 235/320 [00:35<01:06,  1.28it/s, est. speed input: 766.68 toks/s, output: 3777.68 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 236/320 [00:37<01:15,  1.12it/s, est. speed input: 743.57 toks/s, output: 3695.97 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 237/320 [00:37<01:04,  1.29it/s, est. speed input: 737.18 toks/s, output: 3693.74 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 239/320 [00:40<01:22,  1.01s/it, est. speed input: 693.37 toks/s, output: 3523.06 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 240/320 [00:40<01:10,  1.14it/s, est. speed input: 691.88 toks/s, output: 3521.18 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 241/320 [00:40<00:53,  1.46it/s, est. speed input: 691.78 toks/s, output: 3554.08 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 242/320 [00:41<00:46,  1.69it/s, est. speed input: 688.70 toks/s, output: 3567.63 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 243/320 [00:42<00:59,  1.30it/s, est. speed input: 674.58 toks/s, output: 3501.09 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 244/320 [00:45<01:55,  1.52s/it, est. speed input: 625.88 toks/s, output: 3281.31 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 245/320 [00:46<01:36,  1.28s/it, est. speed input: 620.21 toks/s, output: 3274.70 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 246/320 [00:48<01:42,  1.39s/it, est. speed input: 602.22 toks/s, output: 3204.30 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 247/320 [00:50<02:00,  1.65s/it, est. speed input: 577.26 toks/s, output: 3102.25 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 248/320 [01:05<06:50,  5.70s/it, est. speed input: 444.72 toks/s, output: 2418.29 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 249/320 [01:12<07:01,  5.94s/it, est. speed input: 405.72 toks/s, output: 2241.93 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 250/320 [01:14<05:45,  4.94s/it, est. speed input: 392.63 toks/s, output: 2205.23 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 313/320 [01:17<00:01,  4.40it/s, est. speed input: 477.19 toks/s, output: 4595.79 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 314/320 [01:18<00:01,  4.35it/s, est. speed input: 476.12 toks/s, output: 4614.55 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 315/320 [01:18<00:01,  4.15it/s, est. speed input: 473.54 toks/s, output: 4618.25 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 316/320 [01:19<00:00,  4.24it/s, est. speed input: 474.78 toks/s, output: 4650.34 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 317/320 [01:19<00:00,  4.26it/s, est. speed input: 475.41 toks/s, output: 4676.31 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 318/320 [01:19<00:00,  4.00it/s, est. speed input: 473.89 toks/s, output: 4688.78 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 319/320 [01:20<00:00,  3.70it/s, est. speed input: 472.48 toks/s, output: 4699.73 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:20<00:00,  3.74it/s, est. speed input: 471.98 toks/s, output: 4723.82 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:20<00:00,  3.98it/s, est. speed input: 471.98 toks/s, output: 4723.82 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/320 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [00:00<00:00, 19982.99it/s]
{'num_samples': 40, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 22.5, 'total_acc': 18.75, 'pass_at_k_percent': {'1': 18.8, '8': 47.5}, 'pass_at_k_valid_counts': {'1': 40, '8': 40}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/amc23x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 22:33:16] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/amc23x8  acc=22.5 pass_at_k={'1': 18.8, '8': 47.5}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [02:46<01:22, 82.84s/ds][Info] Sharding enabled: Process 1/8 handling range [30:60]
==================================================
data: aime24x8  ,remain samples: 30
{'idx': 30, 'id': 60, 'problem': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.', 'solution': '$\\frac{9}{s} + t = 4$ in hours and $\\frac{9}{s+2} + t = 2.4$ in hours.\nSubtracting the second equation from the first, we get, \n$\\frac{9}{s} - \\frac{9}{s+2} = 1.6$\nMultiplying by $(s)(s+2)$, we get \n$9s+18-9s=18=1.6s^{2} + 3.2s$\nMultiplying by 5/2 on both sides, we get\n$0 = 4s^{2} + 8s - 45$\nFactoring gives us \n$(2s-5)(2s+9) = 0$, of which the solution we want is $s=2.5$.\nSubstituting this back to the first equation, we can find that $t = 0.4$ hours.\nLastly, $s + \\frac{1}{2} = 3$ kilometers per hour, so\n$\\frac{9}{3} + 0.4 = 3.4$ hours, or $\\framebox{204}$ minutes\n-Failure.net\nThe amount of hours spent while walking on the first travel is $\\frac{240-t}{6}$. Thus, we have the equation $(240-t)(s) = 540$, and by the same logic, the second equation yields $(144-t)(s+2) = 540$. We have $240s-st = 540$, and $288+144s-2t-st = 540$. We subtract the two equations to get $96s+2t-288 = 0$, so we have $48s+t = 144$, so $t = 144-48s$, and now we have $(96+48s)(s) = 540$. The numerator of $s$ must evenly divide 540, however, $s$ must be less than 3. We can guess that $s = 2.5$. Now, $2.5+0.5 = 3$. Taking $\\frac{9}{3} = 3$, we find that it will take three hours for the 9 kilometers to be traveled. The t minutes spent at the coffeeshop can be written as $144-48(2.5)$, so t = 24. $180 + 24 = 204$. -sepehr2010', 'answer': '204', 'url': 'https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems/Problem_1', 'question': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 458.07it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:02<09:30,  2.39s/it, est. speed input: 37.31 toks/s, output: 47.37 toks/s][A
Processed prompts:   1%|          | 2/240 [00:04<08:13,  2.07s/it, est. speed input: 47.19 toks/s, output: 73.14 toks/s][A
Processed prompts:   1%|‚ñè         | 3/240 [00:05<06:08,  1.55s/it, est. speed input: 76.55 toks/s, output: 105.93 toks/s][A
Processed prompts:   2%|‚ñè         | 4/240 [00:05<04:33,  1.16s/it, est. speed input: 93.85 toks/s, output: 141.65 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:07<03:43,  1.05it/s, est. speed input: 99.29 toks/s, output: 192.80 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:07<02:50,  1.37it/s, est. speed input: 123.72 toks/s, output: 233.69 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:07<02:19,  1.66it/s, est. speed input: 136.52 toks/s, output: 270.18 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:07<01:25,  2.68it/s, est. speed input: 179.52 toks/s, output: 352.43 toks/s][A
Processed prompts:   5%|‚ñå         | 12/240 [00:08<00:58,  3.90it/s, est. speed input: 203.52 toks/s, output: 434.12 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:08<00:43,  5.22it/s, est. speed input: 239.38 toks/s, output: 514.41 toks/s][A
Processed prompts:   6%|‚ñã         | 15/240 [00:08<00:49,  4.58it/s, est. speed input: 240.74 toks/s, output: 539.09 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:08<00:46,  4.80it/s, est. speed input: 248.81 toks/s, output: 572.55 toks/s][A
Processed prompts:   7%|‚ñã         | 17/240 [00:09<01:04,  3.44it/s, est. speed input: 244.07 toks/s, output: 582.52 toks/s][A
Processed prompts:   8%|‚ñä         | 19/240 [00:09<00:42,  5.21it/s, est. speed input: 260.46 toks/s, output: 663.94 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:09<00:27,  7.97it/s, est. speed input: 288.18 toks/s, output: 783.94 toks/s][A
Processed prompts:  10%|‚ñà         | 24/240 [00:10<00:42,  5.03it/s, est. speed input: 306.10 toks/s, output: 814.59 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:10<00:40,  5.26it/s, est. speed input: 310.15 toks/s, output: 846.16 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:10<00:44,  4.84it/s, est. speed input: 329.95 toks/s, output: 893.82 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 28/240 [00:10<00:42,  4.98it/s, est. speed input: 333.00 toks/s, output: 922.63 toks/s][A
Processed prompts:  12%|‚ñà‚ñé        | 30/240 [00:11<00:32,  6.41it/s, est. speed input: 353.79 toks/s, output: 996.01 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 32/240 [00:11<00:25,  8.31it/s, est. speed input: 376.40 toks/s, output: 1073.42 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 35/240 [00:11<00:17, 11.44it/s, est. speed input: 401.88 toks/s, output: 1190.23 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 37/240 [00:11<00:24,  8.34it/s, est. speed input: 403.19 toks/s, output: 1234.46 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 39/240 [00:11<00:23,  8.74it/s, est. speed input: 415.25 toks/s, output: 1299.21 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 42/240 [00:12<00:19, 10.04it/s, est. speed input: 457.44 toks/s, output: 1402.64 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 44/240 [00:12<00:24,  8.09it/s, est. speed input: 486.78 toks/s, output: 1445.46 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 46/240 [00:12<00:20,  9.67it/s, est. speed input: 505.52 toks/s, output: 1519.32 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 49/240 [00:12<00:15, 12.44it/s, est. speed input: 533.41 toks/s, output: 1632.11 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 51/240 [00:13<00:16, 11.25it/s, est. speed input: 540.98 toks/s, output: 1688.86 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñé       | 54/240 [00:13<00:13, 13.35it/s, est. speed input: 559.04 toks/s, output: 1797.00 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 58/240 [00:13<00:12, 14.33it/s, est. speed input: 598.03 toks/s, output: 1932.62 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 60/240 [00:13<00:12, 14.72it/s, est. speed input: 611.19 toks/s, output: 1999.92 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 62/240 [00:13<00:12, 13.94it/s, est. speed input: 627.00 toks/s, output: 2060.12 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 65/240 [00:13<00:13, 12.89it/s, est. speed input: 643.38 toks/s, output: 2146.68 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 69/240 [00:14<00:10, 15.65it/s, est. speed input: 674.08 toks/s, output: 2288.84 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 71/240 [00:14<00:13, 12.60it/s, est. speed input: 678.50 toks/s, output: 2330.43 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 73/240 [00:14<00:16, 10.19it/s, est. speed input: 679.31 toks/s, output: 2364.03 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà‚ñè      | 75/240 [00:15<00:19,  8.57it/s, est. speed input: 682.82 toks/s, output: 2394.77 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 78/240 [00:15<00:17,  9.12it/s, est. speed input: 688.45 toks/s, output: 2474.59 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:15<00:16,  9.95it/s, est. speed input: 702.05 toks/s, output: 2535.51 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 83/240 [00:15<00:14, 10.71it/s, est. speed input: 712.98 toks/s, output: 2622.16 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 85/240 [00:15<00:14, 10.97it/s, est. speed input: 718.72 toks/s, output: 2678.37 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 87/240 [00:16<00:15, 10.11it/s, est. speed input: 721.54 toks/s, output: 2722.51 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 89/240 [00:16<00:16,  8.97it/s, est. speed input: 722.70 toks/s, output: 2758.70 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 91/240 [00:16<00:15,  9.32it/s, est. speed input: 736.16 toks/s, output: 2810.55 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 93/240 [00:16<00:15,  9.31it/s, est. speed input: 741.94 toks/s, output: 2858.30 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 96/240 [00:17<00:16,  8.70it/s, est. speed input: 742.28 toks/s, output: 2919.54 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 97/240 [00:17<00:17,  8.13it/s, est. speed input: 740.19 toks/s, output: 2933.50 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:17<00:21,  6.56it/s, est. speed input: 739.29 toks/s, output: 2928.05 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 99/240 [00:17<00:20,  6.86it/s, est. speed input: 740.52 toks/s, output: 2950.30 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/240 [00:17<00:14,  9.65it/s, est. speed input: 759.32 toks/s, output: 3048.55 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 104/240 [00:18<00:14,  9.24it/s, est. speed input: 763.26 toks/s, output: 3092.66 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 106/240 [00:18<00:14,  9.32it/s, est. speed input: 771.95 toks/s, output: 3140.91 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:18<00:15,  8.51it/s, est. speed input: 771.48 toks/s, output: 3155.25 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/240 [00:18<00:14,  8.82it/s, est. speed input: 773.09 toks/s, output: 3203.60 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 110/240 [00:19<00:20,  6.27it/s, est. speed input: 764.53 toks/s, output: 3187.10 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 112/240 [00:19<00:24,  5.25it/s, est. speed input: 759.70 toks/s, output: 3190.26 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 114/240 [00:19<00:18,  6.77it/s, est. speed input: 771.90 toks/s, output: 3253.69 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 115/240 [00:20<00:21,  5.94it/s, est. speed input: 766.01 toks/s, output: 3254.67 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 116/240 [00:20<00:25,  4.86it/s, est. speed input: 757.07 toks/s, output: 3242.74 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/240 [00:20<00:24,  4.95it/s, est. speed input: 754.19 toks/s, output: 3255.03 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 118/240 [00:20<00:21,  5.62it/s, est. speed input: 761.56 toks/s, output: 3280.55 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/240 [00:21<00:37,  3.19it/s, est. speed input: 741.08 toks/s, output: 3211.89 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 121/240 [00:21<00:34,  3.44it/s, est. speed input: 738.41 toks/s, output: 3222.96 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:22<00:29,  3.98it/s, est. speed input: 739.10 toks/s, output: 3246.56 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 123/240 [00:22<00:26,  4.45it/s, est. speed input: 739.11 toks/s, output: 3267.12 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:22<00:28,  4.07it/s, est. speed input: 733.27 toks/s, output: 3265.90 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:22<00:30,  3.73it/s, est. speed input: 727.52 toks/s, output: 3262.15 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 126/240 [00:22<00:26,  4.38it/s, est. speed input: 741.29 toks/s, output: 3286.42 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:23<00:21,  5.16it/s, est. speed input: 738.77 toks/s, output: 3328.71 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:24<00:38,  2.88it/s, est. speed input: 717.77 toks/s, output: 3257.31 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 132/240 [00:24<00:20,  5.30it/s, est. speed input: 741.65 toks/s, output: 3371.18 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:24<00:15,  6.93it/s, est. speed input: 746.11 toks/s, output: 3464.86 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 137/240 [00:24<00:12,  8.07it/s, est. speed input: 751.78 toks/s, output: 3530.23 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:25<00:15,  6.38it/s, est. speed input: 745.84 toks/s, output: 3548.85 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:25<00:16,  6.17it/s, est. speed input: 744.22 toks/s, output: 3585.83 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 142/240 [00:25<00:14,  6.58it/s, est. speed input: 748.45 toks/s, output: 3614.49 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:25<00:14,  6.71it/s, est. speed input: 752.31 toks/s, output: 3660.47 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:25<00:14,  6.73it/s, est. speed input: 752.58 toks/s, output: 3683.19 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [00:26<00:12,  7.33it/s, est. speed input: 757.35 toks/s, output: 3737.30 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 148/240 [00:26<00:15,  5.97it/s, est. speed input: 752.86 toks/s, output: 3739.67 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:26<00:16,  5.67it/s, est. speed input: 753.76 toks/s, output: 3754.02 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [00:26<00:13,  6.57it/s, est. speed input: 756.92 toks/s, output: 3808.84 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:27<00:13,  6.45it/s, est. speed input: 755.44 toks/s, output: 3829.09 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/240 [00:27<00:14,  6.15it/s, est. speed input: 756.42 toks/s, output: 3846.40 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 154/240 [00:27<00:18,  4.63it/s, est. speed input: 750.40 toks/s, output: 3838.00 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [00:28<00:22,  3.72it/s, est. speed input: 746.40 toks/s, output: 3831.42 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 158/240 [00:29<00:37,  2.21it/s, est. speed input: 716.71 toks/s, output: 3721.41 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 159/240 [00:31<00:51,  1.59it/s, est. speed input: 691.21 toks/s, output: 3611.19 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 161/240 [00:31<00:38,  2.08it/s, est. speed input: 698.14 toks/s, output: 3644.97 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 163/240 [00:31<00:27,  2.75it/s, est. speed input: 703.31 toks/s, output: 3700.19 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 165/240 [00:32<00:22,  3.34it/s, est. speed input: 701.67 toks/s, output: 3748.63 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 167/240 [00:33<00:25,  2.84it/s, est. speed input: 688.89 toks/s, output: 3731.22 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 168/240 [00:33<00:31,  2.28it/s, est. speed input: 677.92 toks/s, output: 3686.76 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 169/240 [00:35<00:53,  1.32it/s, est. speed input: 646.06 toks/s, output: 3530.97 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 170/240 [00:36<00:47,  1.48it/s, est. speed input: 645.11 toks/s, output: 3535.44 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 172/240 [00:37<00:39,  1.74it/s, est. speed input: 646.26 toks/s, output: 3543.66 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 173/240 [00:38<00:53,  1.25it/s, est. speed input: 622.24 toks/s, output: 3443.93 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 174/240 [00:39<00:55,  1.20it/s, est. speed input: 611.65 toks/s, output: 3406.95 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 175/240 [00:40<01:00,  1.07it/s, est. speed input: 595.73 toks/s, output: 3349.66 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 176/240 [00:42<01:16,  1.20s/it, est. speed input: 571.14 toks/s, output: 3245.43 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 177/240 [00:47<02:11,  2.09s/it, est. speed input: 519.98 toks/s, output: 2985.76 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 178/240 [00:52<03:03,  2.96s/it, est. speed input: 471.31 toks/s, output: 2736.84 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 179/240 [00:52<02:13,  2.18s/it, est. speed input: 472.25 toks/s, output: 2769.00 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 180/240 [01:03<04:50,  4.84s/it, est. speed input: 391.38 toks/s, output: 2324.19 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 181/240 [01:09<04:59,  5.08s/it, est. speed input: 362.00 toks/s, output: 2179.51 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:09<00:00,  3.44it/s, est. speed input: 480.02 toks/s, output: 4778.07 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 20375.95it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 3.3, 'total_acc': 6.666666666666667, 'pass_at_k_percent': {'1': 6.7, '8': 23.3}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime24x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 22:34:27] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime24x8  acc=3.3 pass_at_k={'1': 6.7, '8': 23.3}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:57<00:00, 77.39s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:57<00:00, 79.04s/ds]
[2025-12-03 22:34:27] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2  ÂæÖËØÑÊµã=['minerva_math', 'olympiadbench', 'math500']  T=0.0  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 1/8 handling range [34:68]
==================================================
data: minerva_math  ,remain samples: 34
{'problem': 'The Hubble Space telescope has an effective diameter of $2.5 \\mathrm{~m}$, and a typical wavelength used for observation by the Hubble might be $0.6 \\mu \\mathrm{m}$, or 600 nanometers (typical optical wavelength). Based on this information, compute an estimate for the angular resolution of the Hubble Space telescope in arcseconds.', 'solution': 'Using the formula for angular resolution $\\theta$ in terms of the effective size $d$ and the wavelength $\\lambda$, namely $\\theta = \\lambda/d$, gives \\boxed{0.05} arcseconds.', 'type': 'Introduction to Astronomy (8.282J Spring 2006)', 'idx': 34}

  0%|          | 0/34 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:00<00:00, 13409.15it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/272 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/272 [00:02<13:06,  2.90s/it, est. speed input: 83.73 toks/s, output: 47.20 toks/s][A
Processed prompts:   3%|‚ñé         | 9/272 [00:03<01:25,  3.06it/s, est. speed input: 541.71 toks/s, output: 342.27 toks/s][A
Processed prompts:   6%|‚ñã         | 17/272 [00:04<00:52,  4.84it/s, est. speed input: 561.55 toks/s, output: 583.12 toks/s][A
Processed prompts:   9%|‚ñâ         | 25/272 [00:08<01:15,  3.26it/s, est. speed input: 452.32 toks/s, output: 565.70 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 33/272 [00:08<00:46,  5.12it/s, est. speed input: 564.82 toks/s, output: 889.35 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 41/272 [00:08<00:30,  7.51it/s, est. speed input: 638.43 toks/s, output: 1108.90 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 49/272 [00:08<00:21, 10.19it/s, est. speed input: 834.89 toks/s, output: 1427.17 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 57/272 [00:09<00:18, 11.92it/s, est. speed input: 902.76 toks/s, output: 1700.16 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 65/272 [00:09<00:12, 16.17it/s, est. speed input: 1017.68 toks/s, output: 2028.29 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 73/272 [00:09<00:13, 15.22it/s, est. speed input: 1198.45 toks/s, output: 2238.86 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 80/272 [00:10<00:17, 11.27it/s, est. speed input: 1157.21 toks/s, output: 2309.05 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 88/272 [00:10<00:12, 15.30it/s, est. speed input: 1221.70 toks/s, output: 2633.89 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 96/272 [00:11<00:08, 19.88it/s, est. speed input: 1295.63 toks/s, output: 2935.15 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 104/272 [00:11<00:09, 17.30it/s, est. speed input: 1302.91 toks/s, output: 3016.50 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 112/272 [00:13<00:14, 10.88it/s, est. speed input: 1232.67 toks/s, output: 3022.79 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 128/272 [00:13<00:08, 17.41it/s, est. speed input: 1384.93 toks/s, output: 3650.37 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 132/272 [00:13<00:09, 14.21it/s, est. speed input: 1352.39 toks/s, output: 3671.93 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 137/272 [00:15<00:16,  8.29it/s, est. speed input: 1251.56 toks/s, output: 3499.00 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 145/272 [00:16<00:14,  8.47it/s, est. speed input: 1285.52 toks/s, output: 3644.96 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 153/272 [00:17<00:16,  7.06it/s, est. speed input: 1251.09 toks/s, output: 3662.54 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 161/272 [00:18<00:13,  8.34it/s, est. speed input: 1294.67 toks/s, output: 3902.95 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 169/272 [00:20<00:14,  7.10it/s, est. speed input: 1238.69 toks/s, output: 3949.15 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 177/272 [00:20<00:11,  7.95it/s, est. speed input: 1252.19 toks/s, output: 4167.47 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 185/272 [00:21<00:10,  8.60it/s, est. speed input: 1281.87 toks/s, output: 4380.38 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 193/272 [00:26<00:21,  3.68it/s, est. speed input: 1073.84 toks/s, output: 3859.04 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 199/272 [01:10<02:24,  1.98s/it, est. speed input: 415.61 toks/s, output: 1597.21 toks/s] [AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [01:10<00:00,  3.88it/s, est. speed input: 647.25 toks/s, output: 4790.09 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/272 [00:00<?, ?it/s][A
Evaluate:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 225/272 [00:01<00:00, 177.31it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [00:01<00:00, 200.26it/s]
{'num_samples': 34, 'num_scores': 272, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 8.8, 'total_acc': 8.823529411764707, 'pass_at_k_percent': {'1': 8.8, '8': 8.8}, 'pass_at_k_valid_counts': {'1': 34, '8': 34}, 'type_acc': {'Differential Equations (18.03 Spring 2010)': 28.6, 'Ecology I (1.018J Fall 2009)': 20.0, 'Information and Entropy (6.050J Spring 2008)': 0.0, 'Introduction to Astronomy (8.282J Spring 2006)': 0.0}, 'type_pass_at_k_percent': {'Differential Equations (18.03 Spring 2010)': {'1': 28.6, '8': 28.6}, 'Ecology I (1.018J Fall 2009)': {'1': 20.0, '8': 20.0}, 'Information and Entropy (6.050J Spring 2008)': {'1': 0.0, '8': 0.0}, 'Introduction to Astronomy (8.282J Spring 2006)': {'1': 0.0, '8': 0.0}}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/minerva_math/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 22:35:39] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/minerva_math  acc=8.8 pass_at_k={'1': 8.8, '8': 8.8}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:11<02:23, 71.86s/ds][Info] Sharding enabled: Process 1/8 handling range [84:168]
==================================================
data: olympiadbench  ,remain samples: 84
{'idx': 84, 'id': 2006, 'subfield': 'Combinatorics', 'context': None, 'question': 'In each square of a garden shaped like a $2022 \\times 2022$ board, there is initially a tree of height 0 . A gardener and a lumberjack alternate turns playing the following game, with the gardener taking the first turn:\n\n- The gardener chooses a square in the garden. Each tree on that square and all the surrounding squares (of which there are at most eight) then becomes one unit taller.\n- The lumberjack then chooses four different squares on the board. Each tree of positive height on those squares then becomes one unit shorter.\n\nWe say that a tree is majestic if its height is at least $10^{6}$. Determine the largest number $K$ such that the gardener can ensure there are eventually $K$ majestic trees on the board, no matter how the lumberjack plays.', 'solution': ["We solve the problem for a general $3 N \\times 3 N$ board. First, we prove that the lumberjack has a strategy to ensure there are never more than $5 N^{2}$ majestic trees. Giving the squares of the board coordinates in the natural manner, colour each square where at least one of its coordinates are divisible by 3 , shown below for a $9 \\times 9$ board:\n\n<img_3271>\n\nThen, as each $3 \\times 3$ square on the board contains exactly 5 coloured squares, each move of the gardener will cause at most 4 trees on non-coloured squares to grow. The lumberjack may therefore cut those trees, ensuring no tree on a non-coloured square has positive height after his turn. Hence there cannot ever be more majestic trees than coloured squares, which is $5 N^{2}$.\n\nNext, we prove the gardener may ensure there are $5 N^{2}$ majestic trees. In fact, we prove this statement in a modified game which is more difficult for the gardener: on the lumberjack's turn in the modified game, he may decrement the height of all trees on the board except those the gardener did not just grow, in addition to four of the trees the gardener just grew. Clearly, a sequence of moves for the gardener which ensures that there are $K$ majestic trees in the modified game also ensures this in the original game.\n\n\n\nLet $M=\\left(\\begin{array}{l}9 \\\\ 5\\end{array}\\right)$; we say that a $m a p$ is one of the $M$ possible ways to mark 5 squares on a $3 \\times 3$ board. In the modified game, after the gardener chooses a $3 \\times 3$ subboard on the board, the lumberjack chooses a map in this subboard, and the total result of the two moves is that each tree marked on the map increases its height by 1, each tree in the subboard which is not in the map remains unchanged, and each tree outside the subboard decreases its height by 1 . Also note that if the gardener chooses a $3 \\times 3$ subboard $M l$ times, the lumberjack will have to choose some map at least $l$ times, so there will be at least 5 trees which each have height $\\geqslant l$.\n\nThe strategy for the gardener will be to divide the board into $N^{2}$ disjoint $3 \\times 3$ subboards, number them $0, \\ldots, N^{2}-1$ in some order. Then, for $b=N^{2}-1, \\ldots, 0$ in order, he plays $10^{6} M(M+1)^{b}$ times on subboard number $b$. Hence, on subboard number $b$, the moves on that subboard will first ensure 5 of its trees grows by at least $10^{6}(M+1)^{b}$, and then each move after that will decrease their heights by 1 . (As the trees on subboard $b$ had height 0 before the gardener started playing there, no move made on subboards $\\geqslant b$ decreased their heights.) As the gardener makes $10^{6} M(M+1)^{b-1}+\\ldots=10^{6}\\left((M+1)^{b}-1\\right)$ moves after he finishes playing on subboard $b$, this means that on subboard $b$, there will be 5 trees of height at least $10^{6}(M+1)^{b}-10^{6}\\left((M+1)^{b}-1\\right)=10^{6}$, hence each of the subboard has 5 majestic trees, which was what we wanted."], 'final_answer': ['2271380'], 'is_multiple_answer': False, 'unit': None, 'answer_type': 'Numerical', 'error': None}

  0%|          | 0/84 [00:00<?, ?it/s][A
 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 37/84 [00:00<00:00, 349.71it/s][A
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 78/84 [00:00<00:00, 376.43it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84/84 [00:00<00:00, 370.15it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/672 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/672 [00:00<03:01,  3.69it/s, est. speed input: 1129.25 toks/s, output: 11.07 toks/s][A
Processed prompts:   3%|‚ñé         | 17/672 [00:09<06:12,  1.76it/s, est. speed input: 380.80 toks/s, output: 48.08 toks/s][A
Processed prompts:   4%|‚ñé         | 25/672 [00:10<03:54,  2.76it/s, est. speed input: 467.58 toks/s, output: 375.11 toks/s][A
Processed prompts:   5%|‚ñç         | 33/672 [00:11<02:58,  3.59it/s, est. speed input: 532.91 toks/s, output: 650.65 toks/s][A
Processed prompts:   6%|‚ñå         | 41/672 [00:12<02:34,  4.08it/s, est. speed input: 606.52 toks/s, output: 878.47 toks/s][A
Processed prompts:   8%|‚ñä         | 57/672 [00:12<01:20,  7.64it/s, est. speed input: 744.23 toks/s, output: 1516.01 toks/s][A
Processed prompts:  10%|‚ñâ         | 65/672 [00:13<01:22,  7.39it/s, est. speed input: 737.13 toks/s, output: 1693.63 toks/s][A
Processed prompts:  11%|‚ñà         | 73/672 [00:15<01:28,  6.76it/s, est. speed input: 746.75 toks/s, output: 1833.88 toks/s][A
Processed prompts:  11%|‚ñà         | 75/672 [00:16<01:55,  5.16it/s, est. speed input: 715.11 toks/s, output: 1765.80 toks/s][A
Processed prompts:  12%|‚ñà‚ñé        | 84/672 [00:17<01:19,  7.37it/s, est. speed input: 833.47 toks/s, output: 2088.63 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 86/672 [00:17<01:22,  7.11it/s, est. speed input: 830.73 toks/s, output: 2123.98 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 93/672 [00:19<01:38,  5.89it/s, est. speed input: 812.98 toks/s, output: 2204.89 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 101/672 [00:21<02:05,  4.56it/s, est. speed input: 790.98 toks/s, output: 2232.96 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 109/672 [00:22<01:44,  5.37it/s, est. speed input: 803.12 toks/s, output: 2432.09 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 117/672 [00:22<01:18,  7.06it/s, est. speed input: 849.88 toks/s, output: 2690.68 toks/s][A
Processed prompts:  19%|‚ñà‚ñä        | 125/672 [00:23<01:05,  8.39it/s, est. speed input: 898.17 toks/s, output: 2900.83 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 133/672 [00:23<00:46, 11.55it/s, est. speed input: 922.60 toks/s, output: 2993.16 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 137/672 [00:24<00:51, 10.47it/s, est. speed input: 919.84 toks/s, output: 2946.03 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 145/672 [00:24<00:42, 12.35it/s, est. speed input: 971.54 toks/s, output: 3069.25 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 153/672 [00:26<01:00,  8.57it/s, est. speed input: 958.81 toks/s, output: 3174.98 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 161/672 [00:27<01:10,  7.30it/s, est. speed input: 965.19 toks/s, output: 3260.25 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 165/672 [00:29<01:36,  5.24it/s, est. speed input: 918.35 toks/s, output: 3097.47 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 169/672 [00:29<01:34,  5.32it/s, est. speed input: 906.81 toks/s, output: 3069.58 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 173/672 [00:30<01:16,  6.48it/s, est. speed input: 911.24 toks/s, output: 3089.92 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 176/672 [00:30<01:27,  5.70it/s, est. speed input: 897.62 toks/s, output: 3074.77 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 184/672 [00:30<00:52,  9.33it/s, est. speed input: 926.25 toks/s, output: 3327.81 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 188/672 [00:31<00:45, 10.72it/s, est. speed input: 931.71 toks/s, output: 3363.17 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 191/672 [00:31<00:49,  9.75it/s, est. speed input: 928.31 toks/s, output: 3354.73 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 199/672 [00:35<02:02,  3.87it/s, est. speed input: 849.59 toks/s, output: 3065.52 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 202/672 [00:36<02:21,  3.32it/s, est. speed input: 823.18 toks/s, output: 2986.94 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 206/672 [00:38<02:29,  3.12it/s, est. speed input: 799.76 toks/s, output: 2918.74 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà‚ñè      | 210/672 [00:38<02:06,  3.64it/s, est. speed input: 800.41 toks/s, output: 2943.44 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 218/672 [00:39<01:15,  6.03it/s, est. speed input: 828.37 toks/s, output: 3187.68 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñé      | 226/672 [00:39<00:58,  7.65it/s, est. speed input: 853.18 toks/s, output: 3287.77 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 234/672 [00:39<00:42, 10.33it/s, est. speed input: 867.34 toks/s, output: 3351.18 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 238/672 [00:40<00:39, 10.94it/s, est. speed input: 866.85 toks/s, output: 3361.95 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 242/672 [00:40<00:33, 12.98it/s, est. speed input: 870.39 toks/s, output: 3391.50 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 246/672 [00:44<02:05,  3.38it/s, est. speed input: 800.16 toks/s, output: 3168.47 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 254/672 [00:44<01:19,  5.27it/s, est. speed input: 811.89 toks/s, output: 3390.43 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 257/672 [00:44<01:11,  5.78it/s, est. speed input: 815.20 toks/s, output: 3399.82 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 261/672 [00:48<02:17,  2.99it/s, est. speed input: 768.34 toks/s, output: 3219.86 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 263/672 [00:48<02:09,  3.16it/s, est. speed input: 766.61 toks/s, output: 3216.88 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 270/672 [00:52<02:58,  2.25it/s, est. speed input: 720.10 toks/s, output: 3037.44 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 274/672 [00:52<02:13,  2.98it/s, est. speed input: 724.64 toks/s, output: 3072.80 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 275/672 [01:10<02:13,  2.98it/s, est. speed input: 725.18 toks/s, output: 3079.12 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 276/672 [01:42<27:37,  4.19s/it, est. speed input: 377.22 toks/s, output: 1623.17 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 288/672 [01:42<11:29,  1.79s/it, est. speed input: 399.62 toks/s, output: 1977.19 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 328/672 [01:42<02:47,  2.06it/s, est. speed input: 456.10 toks/s, output: 3164.53 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 359/672 [01:43<01:25,  3.68it/s, est. speed input: 497.08 toks/s, output: 4079.25 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 367/672 [01:43<01:13,  4.16it/s, est. speed input: 504.42 toks/s, output: 4064.26 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 373/672 [01:45<01:15,  3.94it/s, est. speed input: 500.04 toks/s, output: 4159.50 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 378/672 [01:46<01:13,  4.00it/s, est. speed input: 499.92 toks/s, output: 4181.85 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 382/672 [01:47<01:14,  3.91it/s, est. speed input: 498.45 toks/s, output: 4147.14 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 387/672 [01:48<01:00,  4.68it/s, est. speed input: 500.70 toks/s, output: 4149.97 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 391/672 [01:48<00:59,  4.73it/s, est. speed input: 499.45 toks/s, output: 4125.47 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 399/672 [01:49<00:39,  6.84it/s, est. speed input: 504.15 toks/s, output: 4133.36 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 402/672 [01:49<00:37,  7.12it/s, est. speed input: 504.69 toks/s, output: 4135.19 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 406/672 [01:49<00:34,  7.79it/s, est. speed input: 507.38 toks/s, output: 4133.75 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 409/672 [01:49<00:29,  8.92it/s, est. speed input: 510.48 toks/s, output: 4139.33 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 411/672 [01:52<01:16,  3.40it/s, est. speed input: 500.31 toks/s, output: 4053.30 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 413/672 [01:52<01:10,  3.65it/s, est. speed input: 500.15 toks/s, output: 4049.98 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 416/672 [01:52<00:52,  4.85it/s, est. speed input: 501.89 toks/s, output: 4061.40 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 418/672 [01:53<00:47,  5.35it/s, est. speed input: 502.77 toks/s, output: 4107.46 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 421/672 [01:53<00:35,  7.11it/s, est. speed input: 505.08 toks/s, output: 4184.55 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 423/672 [01:53<00:31,  8.00it/s, est. speed input: 506.32 toks/s, output: 4233.40 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 425/672 [01:53<00:35,  7.00it/s, est. speed input: 506.24 toks/s, output: 4250.91 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 427/672 [01:53<00:29,  8.31it/s, est. speed input: 509.77 toks/s, output: 4258.42 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 429/672 [01:54<00:35,  6.90it/s, est. speed input: 509.17 toks/s, output: 4249.09 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 436/672 [01:55<00:27,  8.56it/s, est. speed input: 511.02 toks/s, output: 4247.75 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 438/672 [01:55<00:40,  5.80it/s, est. speed input: 509.53 toks/s, output: 4225.71 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 447/672 [01:55<00:19, 11.84it/s, est. speed input: 518.26 toks/s, output: 4270.68 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 451/672 [01:56<00:25,  8.72it/s, est. speed input: 517.08 toks/s, output: 4264.55 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 457/672 [01:57<00:20, 10.71it/s, est. speed input: 520.52 toks/s, output: 4286.59 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 460/672 [01:58<00:33,  6.25it/s, est. speed input: 518.91 toks/s, output: 4257.75 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 462/672 [01:58<00:31,  6.57it/s, est. speed input: 519.20 toks/s, output: 4255.27 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 468/672 [01:59<00:25,  8.08it/s, est. speed input: 520.81 toks/s, output: 4254.25 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 470/672 [01:59<00:26,  7.65it/s, est. speed input: 521.34 toks/s, output: 4293.83 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 472/672 [01:59<00:24,  8.20it/s, est. speed input: 522.58 toks/s, output: 4339.16 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 475/672 [01:59<00:20,  9.51it/s, est. speed input: 524.70 toks/s, output: 4409.14 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 477/672 [02:02<01:03,  3.05it/s, est. speed input: 517.45 toks/s, output: 4359.30 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 478/672 [02:02<00:58,  3.34it/s, est. speed input: 518.55 toks/s, output: 4362.51 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 479/672 [02:03<01:39,  1.93it/s, est. speed input: 512.69 toks/s, output: 4309.51 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 487/672 [02:04<00:41,  4.46it/s, est. speed input: 519.82 toks/s, output: 4339.87 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 489/672 [02:04<00:35,  5.19it/s, est. speed input: 522.14 toks/s, output: 4352.42 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 492/672 [02:05<00:40,  4.41it/s, est. speed input: 521.72 toks/s, output: 4340.84 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 494/672 [02:05<00:37,  4.72it/s, est. speed input: 522.51 toks/s, output: 4343.03 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 495/672 [02:05<00:38,  4.59it/s, est. speed input: 522.14 toks/s, output: 4338.69 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 498/672 [02:07<01:04,  2.68it/s, est. speed input: 516.28 toks/s, output: 4286.80 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 501/672 [02:10<01:28,  1.93it/s, est. speed input: 509.46 toks/s, output: 4223.03 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 502/672 [02:10<01:28,  1.92it/s, est. speed input: 508.60 toks/s, output: 4210.32 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 503/672 [02:13<02:29,  1.13it/s, est. speed input: 498.99 toks/s, output: 4140.00 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 512/672 [02:14<00:53,  2.99it/s, est. speed input: 514.22 toks/s, output: 4185.18 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 513/672 [02:16<01:20,  1.98it/s, est. speed input: 507.46 toks/s, output: 4143.80 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 515/672 [02:16<01:06,  2.37it/s, est. speed input: 508.70 toks/s, output: 4181.11 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 516/672 [02:16<00:59,  2.61it/s, est. speed input: 509.32 toks/s, output: 4199.72 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 519/672 [02:16<00:40,  3.76it/s, est. speed input: 511.88 toks/s, output: 4261.26 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 521/672 [02:18<01:08,  2.22it/s, est. speed input: 507.00 toks/s, output: 4233.65 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 522/672 [02:19<01:04,  2.33it/s, est. speed input: 507.13 toks/s, output: 4234.70 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 523/672 [02:20<01:26,  1.72it/s, est. speed input: 504.17 toks/s, output: 4204.86 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 524/672 [02:20<01:15,  1.97it/s, est. speed input: 503.97 toks/s, output: 4207.08 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 531/672 [02:22<00:53,  2.64it/s, est. speed input: 500.43 toks/s, output: 4216.38 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 534/672 [02:23<00:38,  3.57it/s, est. speed input: 501.89 toks/s, output: 4277.05 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 537/672 [02:23<00:28,  4.69it/s, est. speed input: 503.77 toks/s, output: 4322.36 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 539/672 [02:23<00:24,  5.34it/s, est. speed input: 504.38 toks/s, output: 4359.56 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 541/672 [02:23<00:21,  6.20it/s, est. speed input: 506.37 toks/s, output: 4373.61 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 548/672 [02:24<00:20,  6.10it/s, est. speed input: 511.00 toks/s, output: 4402.25 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 550/672 [02:27<00:50,  2.43it/s, est. speed input: 502.79 toks/s, output: 4352.72 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 554/672 [02:27<00:33,  3.56it/s, est. speed input: 507.13 toks/s, output: 4432.44 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 557/672 [02:28<00:31,  3.67it/s, est. speed input: 508.24 toks/s, output: 4461.69 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 565/672 [02:30<00:26,  4.08it/s, est. speed input: 512.54 toks/s, output: 4501.47 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 570/672 [02:30<00:17,  5.69it/s, est. speed input: 518.20 toks/s, output: 4590.07 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 573/672 [02:30<00:14,  6.76it/s, est. speed input: 521.13 toks/s, output: 4647.25 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 576/672 [02:35<00:45,  2.09it/s, est. speed input: 508.04 toks/s, output: 4556.22 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 578/672 [02:37<00:58,  1.60it/s, est. speed input: 501.22 toks/s, output: 4511.77 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 580/672 [02:39<00:57,  1.60it/s, est. speed input: 498.80 toks/s, output: 4506.58 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 582/672 [02:39<00:44,  2.00it/s, est. speed input: 499.38 toks/s, output: 4541.32 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 584/672 [02:42<01:04,  1.36it/s, est. speed input: 491.57 toks/s, output: 4496.92 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 588/672 [02:45<01:03,  1.33it/s, est. speed input: 485.05 toks/s, output: 4486.72 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 590/672 [02:51<01:45,  1.29s/it, est. speed input: 468.85 toks/s, output: 4359.49 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 593/672 [02:51<01:09,  1.13it/s, est. speed input: 471.14 toks/s, output: 4408.96 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 594/672 [02:52<01:11,  1.09it/s, est. speed input: 468.87 toks/s, output: 4397.70 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 597/672 [02:52<00:44,  1.68it/s, est. speed input: 471.08 toks/s, output: 4448.23 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 599/672 [02:53<00:33,  2.18it/s, est. speed input: 472.38 toks/s, output: 4480.31 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 601/672 [02:53<00:28,  2.51it/s, est. speed input: 472.78 toks/s, output: 4503.75 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 603/672 [02:53<00:21,  3.17it/s, est. speed input: 473.62 toks/s, output: 4533.79 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 605/672 [02:54<00:18,  3.72it/s, est. speed input: 474.20 toks/s, output: 4561.23 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 608/672 [02:54<00:15,  4.25it/s, est. speed input: 474.83 toks/s, output: 4599.84 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 610/672 [02:57<00:33,  1.83it/s, est. speed input: 468.42 toks/s, output: 4561.27 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 612/672 [02:58<00:35,  1.68it/s, est. speed input: 466.37 toks/s, output: 4559.05 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 615/672 [03:00<00:29,  1.91it/s, est. speed input: 466.98 toks/s, output: 4579.51 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 618/672 [03:00<00:19,  2.75it/s, est. speed input: 470.26 toks/s, output: 4625.87 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 619/672 [03:00<00:19,  2.72it/s, est. speed input: 470.52 toks/s, output: 4632.88 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 622/672 [03:01<00:15,  3.27it/s, est. speed input: 472.46 toks/s, output: 4668.12 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 623/672 [03:01<00:15,  3.19it/s, est. speed input: 472.53 toks/s, output: 4675.94 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 626/672 [03:01<00:09,  4.93it/s, est. speed input: 475.21 toks/s, output: 4723.99 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 628/672 [03:02<00:10,  4.33it/s, est. speed input: 475.66 toks/s, output: 4741.89 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 631/672 [03:04<00:17,  2.39it/s, est. speed input: 473.08 toks/s, output: 4735.01 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 634/672 [03:06<00:20,  1.84it/s, est. speed input: 469.77 toks/s, output: 4724.90 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 651/672 [03:07<00:03,  5.46it/s, est. speed input: 476.79 toks/s, output: 4979.82 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 653/672 [03:07<00:03,  5.86it/s, est. speed input: 478.85 toks/s, output: 5008.74 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 655/672 [03:08<00:02,  6.46it/s, est. speed input: 480.98 toks/s, output: 5038.49 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 657/672 [03:08<00:02,  6.19it/s, est. speed input: 481.21 toks/s, output: 5060.62 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 660/672 [03:08<00:01,  6.37it/s, est. speed input: 481.95 toks/s, output: 5097.83 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 664/672 [03:09<00:00,  8.40it/s, est. speed input: 484.21 toks/s, output: 5158.30 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 668/672 [03:09<00:00,  9.14it/s, est. speed input: 486.66 toks/s, output: 5213.46 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 670/672 [03:09<00:00,  8.06it/s, est. speed input: 487.40 toks/s, output: 5235.48 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 672/672 [03:11<00:00,  4.43it/s, est. speed input: 486.04 toks/s, output: 5234.30 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 672/672 [03:11<00:00,  3.52it/s, est. speed input: 486.04 toks/s, output: 5234.30 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/672 [00:00<?, ?it/s][A
Evaluate:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 651/672 [00:00<00:00, 6494.14it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 672/672 [00:00<00:00, 6520.25it/s]
{'num_samples': 84, 'num_scores': 672, 'timeout_samples': 0, 'empty_samples': 6, 'acc': 16.7, 'total_acc': 16.666666666666664, 'pass_at_k_percent': {'1': 16.7, '8': 21.4}, 'pass_at_k_valid_counts': {'1': 84, '8': 84}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/olympiadbench/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 22:38:53] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/olympiadbench  acc=16.7 pass_at_k={'1': 16.7, '8': 21.4}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [04:25<02:23, 143.42s/ds][Info] Sharding enabled: Process 1/8 handling range [62:124]
==================================================
data: math500  ,remain samples: 62
{'idx': 62, 'problem': 'Find the smallest positive real number $C$ for which\n\\[\\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\bold{v} \\right\\| \\le C \\|\\bold{v}\\|\\]for all two-dimensional vectors $\\bold{v}.$\n\nNote that for a two-dimensional vector $\\mathbf{a},$ $\\|\\mathbf{a}\\|$ is the magnitude of $\\mathbf{a}.$', 'solution': 'Let $\\bold{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.  Then\n\\[\\|\\bold{v}\\| = \\left\\| \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right\\| = \\sqrt{x^2 + y^2},\\]and\n\\begin{align*}\n\\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\bold{v} \\right\\| &= \\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right\\| \\\\\n&= \\left\\| \\begin{pmatrix} 2x + 3y \\\\ -2y \\end{pmatrix} \\right\\| \\\\\n&= \\sqrt{(2x + 3y)^2 + (-2y)^2} \\\\\n&= \\sqrt{4x^2 + 12xy + 13y^2},\n\\end{align*}so the given inequality becomes\n\\[\\sqrt{4x^2 + 12xy + 13y^2} \\le C \\sqrt{x^2 + y^2},\\]or\n\\[\\sqrt{\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2}} \\le C.\\]Thus, we can think of $C$ as the maximum value of the expression in the left-hand side.\n\nMaximizing the expression in the left-hand side is equivalent to maximizing its square, namely\n\\[\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2}.\\]Let $k$ be a possible value of this expression, which means the equation\n\\[\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2} = k\\]has a solution in $x$ and $y$.  We can re-write this equation as\n\\[(4 - k) x^2 + 12xy + (13 - k) y^2 = 0.\\]For this quadratic expression to have a solution in $x$ and $y$, its discriminant must be nonnegative.  In other words,\n\\[12^2 - 4 (4 - k)(13 - k) \\ge 0,\\]or $4k^2 - 68k + 64 \\le 0$.  This inequality factors as $4(k - 1)(k - 16) \\le 0$.  The largest value of $k$ that satisfies this inequality is 16, so the value of $C$ we seek is $\\sqrt{16} = \\boxed{4}$.  Note that equality occurs for\n\\[\\bold{v} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\\]', 'answer': '4', 'subject': 'Precalculus', 'level': 5, 'unique_id': 'test/precalculus/675.json'}

  0%|          | 0/62 [00:00<?, ?it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 23/62 [00:00<00:00, 223.14it/s][A
 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 46/62 [00:00<00:00, 217.27it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62/62 [00:00<00:00, 222.69it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/496 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   2%|‚ñè         | 9/496 [00:02<01:55,  4.21it/s, est. speed input: 318.86 toks/s, output: 62.27 toks/s][A
Processed prompts:   5%|‚ñå         | 25/496 [00:02<00:45, 10.40it/s, est. speed input: 627.09 toks/s, output: 691.38 toks/s][A
Processed prompts:   7%|‚ñã         | 33/496 [00:03<00:35, 13.06it/s, est. speed input: 750.12 toks/s, output: 992.72 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 57/496 [00:03<00:18, 24.25it/s, est. speed input: 1083.74 toks/s, output: 1951.01 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 65/496 [00:03<00:18, 23.90it/s, est. speed input: 1133.78 toks/s, output: 2134.28 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 73/496 [00:04<00:17, 24.04it/s, est. speed input: 1199.28 toks/s, output: 2331.02 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 81/496 [00:04<00:16, 25.23it/s, est. speed input: 1238.35 toks/s, output: 2556.75 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 89/496 [00:05<00:19, 20.44it/s, est. speed input: 1255.70 toks/s, output: 2601.23 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 97/496 [00:05<00:17, 23.40it/s, est. speed input: 1320.39 toks/s, output: 2866.48 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 105/496 [00:05<00:22, 17.63it/s, est. speed input: 1253.73 toks/s, output: 2854.42 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 113/496 [00:06<00:21, 17.60it/s, est. speed input: 1234.13 toks/s, output: 2983.90 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 121/496 [00:06<00:19, 19.39it/s, est. speed input: 1242.22 toks/s, output: 3048.88 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 129/496 [00:07<00:22, 16.05it/s, est. speed input: 1191.76 toks/s, output: 3103.65 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 145/496 [00:07<00:16, 21.23it/s, est. speed input: 1263.77 toks/s, output: 3623.80 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 160/496 [00:08<00:13, 24.73it/s, est. speed input: 1376.01 toks/s, output: 3973.54 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 168/496 [00:08<00:11, 27.83it/s, est. speed input: 1430.76 toks/s, output: 4255.34 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 176/496 [00:08<00:10, 29.56it/s, est. speed input: 1497.49 toks/s, output: 4483.43 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 184/496 [00:09<00:14, 21.57it/s, est. speed input: 1564.49 toks/s, output: 4369.37 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñä      | 192/496 [00:09<00:14, 21.69it/s, est. speed input: 1553.30 toks/s, output: 4419.36 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 200/496 [00:09<00:11, 24.88it/s, est. speed input: 1601.93 toks/s, output: 4669.13 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 204/496 [00:10<00:19, 14.89it/s, est. speed input: 1505.97 toks/s, output: 4389.31 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 209/496 [00:11<00:18, 15.48it/s, est. speed input: 1518.27 toks/s, output: 4371.35 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 225/496 [00:11<00:12, 22.47it/s, est. speed input: 1708.41 toks/s, output: 4755.54 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 230/496 [00:12<00:22, 11.77it/s, est. speed input: 1560.93 toks/s, output: 4469.20 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 238/496 [00:12<00:16, 15.61it/s, est. speed input: 1619.19 toks/s, output: 4747.72 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 245/496 [00:13<00:19, 13.20it/s, est. speed input: 1561.97 toks/s, output: 4588.91 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 248/496 [00:14<00:21, 11.28it/s, est. speed input: 1519.42 toks/s, output: 4473.79 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 253/496 [00:14<00:20, 11.58it/s, est. speed input: 1499.26 toks/s, output: 4455.03 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 261/496 [00:14<00:15, 15.42it/s, est. speed input: 1518.75 toks/s, output: 4706.03 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 286/496 [00:15<00:08, 25.60it/s, est. speed input: 1583.27 toks/s, output: 4971.74 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 294/496 [00:15<00:08, 24.97it/s, est. speed input: 1577.08 toks/s, output: 4983.48 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 297/496 [00:16<00:11, 17.70it/s, est. speed input: 1531.70 toks/s, output: 4853.18 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 302/496 [00:16<00:10, 17.99it/s, est. speed input: 1529.02 toks/s, output: 4884.84 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 310/496 [00:16<00:08, 21.63it/s, est. speed input: 1576.55 toks/s, output: 5166.10 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 318/496 [00:17<00:07, 22.71it/s, est. speed input: 1710.95 toks/s, output: 5409.47 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/496 [00:17<00:10, 16.03it/s, est. speed input: 1679.64 toks/s, output: 5317.44 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 327/496 [00:18<00:13, 12.46it/s, est. speed input: 1647.53 toks/s, output: 5224.93 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 332/496 [00:19<00:20,  8.14it/s, est. speed input: 1569.72 toks/s, output: 4969.08 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 335/496 [00:19<00:18,  8.61it/s, est. speed input: 1563.66 toks/s, output: 4977.26 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 344/496 [00:19<00:11, 13.66it/s, est. speed input: 1589.58 toks/s, output: 5286.72 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 351/496 [00:20<00:10, 13.46it/s, est. speed input: 1584.63 toks/s, output: 5310.16 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 359/496 [00:20<00:07, 18.44it/s, est. speed input: 1615.40 toks/s, output: 5446.68 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 367/496 [00:20<00:06, 20.52it/s, est. speed input: 1673.07 toks/s, output: 5646.41 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 375/496 [00:22<00:10, 11.06it/s, est. speed input: 1620.43 toks/s, output: 5541.28 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 383/496 [00:28<00:36,  3.12it/s, est. speed input: 1324.29 toks/s, output: 4425.48 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 391/496 [00:43<01:20,  1.31it/s, est. speed input: 917.95 toks/s, output: 3210.25 toks/s] [A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 399/496 [00:51<01:22,  1.18it/s, est. speed input: 793.19 toks/s, output: 2943.70 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 406/496 [01:07<01:16,  1.18it/s, est. speed input: 809.84 toks/s, output: 3205.74 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 407/496 [01:16<02:18,  1.56s/it, est. speed input: 542.63 toks/s, output: 2180.67 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 449/496 [01:18<00:24,  1.95it/s, est. speed input: 579.84 toks/s, output: 3753.10 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 457/496 [01:20<00:18,  2.15it/s, est. speed input: 579.11 toks/s, output: 3976.20 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 465/496 [01:20<00:11,  2.60it/s, est. speed input: 591.31 toks/s, output: 4269.21 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 473/496 [01:22<00:07,  2.93it/s, est. speed input: 592.57 toks/s, output: 4493.46 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 481/496 [01:23<00:04,  3.48it/s, est. speed input: 600.87 toks/s, output: 4741.47 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 488/496 [01:23<00:01,  4.40it/s, est. speed input: 604.58 toks/s, output: 4992.94 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 491/496 [01:24<00:01,  4.12it/s, est. speed input: 603.57 toks/s, output: 5038.53 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 496/496 [01:24<00:00,  5.87it/s, est. speed input: 614.14 toks/s, output: 5215.73 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/496 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 496/496 [00:00<00:00, 12847.21it/s]
{'num_samples': 62, 'num_scores': 496, 'timeout_samples': 0, 'empty_samples': 4, 'acc': 43.5, 'total_acc': 44.556451612903224, 'pass_at_k_percent': {'1': 44.6, '8': 45.2}, 'pass_at_k_valid_counts': {'1': 62, '8': 62}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/math500/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 22:40:19] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/math500  acc=43.5 pass_at_k={'1': 44.6, '8': 45.2}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:51<00:00, 117.36s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:51<00:00, 117.24s/ds]
[2025-12-03 22:40:19] ‚úÖ ÂÆåÊàêÔºöB_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200Ôºàg1+g2 Áº∫Â§±Êï∞ÊçÆÈõÜÂ∑≤Ë°•ÂÖ®Ôºâ
[2025-12-03 22:40:19] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:40:19 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:40:19 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:40:19 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:40:25 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:40:32 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:40:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff18a5c8670>
INFO 12-03 22:41:02 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:41:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:41:02 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:41:02 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:41:03 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:41:03 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:41:03 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:41:03 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:41:03 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:41:03 [core.py:396]     self._init_executor()
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:41:03 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:41:03 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:41:03 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:41:03 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:41:03 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:41:03 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:41:03 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:41:03 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:41:03 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:41:03 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:41:03 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:41:03 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:41:03 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:41:03 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:41:03 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:41:03 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:41:03 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:41:03 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:41:03 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:41:03 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:41:03 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:41:03 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2932393 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2932393 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff43dd6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7ff43dd15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7ff3eec8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7ff43e16bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7ff43e16c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7ff43e182b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7ff43e16e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7ff4362864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7ff4359a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7ff4359a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55a9cf7bfc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55a9cf74c2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55a9cf74cbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55a9cf74cc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55a9cf7bfb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55a9cf82bc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55a9cf82ef1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55a9cf74dc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55a9cf88cc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55a9cf8b2407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55a9cf8b2634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55a9cf8b2718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55a9cf8b275b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55a9cf8b2972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55a9cf8b8f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55a9cf8b91ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55a9cf8b9469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7ff43ef7fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7ff43ef7fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55a9cf8242d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:41:05] ‚ö† Ê®°Âûã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:41:05] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:41:05 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:41:05 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:41:05 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:41:11 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:41:17 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:41:17 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff0483fec80>
INFO 12-03 22:41:38 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:41:38 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:41:38 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:41:38 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:41:38 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:41:38 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:41:38 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:41:38 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:41:38 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:41:38 [core.py:396]     self._init_executor()
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:41:38 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:41:38 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:41:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:41:38 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:41:38 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:41:38 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:41:38 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:41:38 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:41:38 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:41:38 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:41:38 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:41:38 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:41:38 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:41:38 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:41:38 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:41:38 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:41:38 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:41:38 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:41:38 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:41:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:41:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:41:38 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2933797 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2933797 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff2fbf6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7ff2fbf15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7ff2aca8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7ff2fc2edb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7ff2fc2ee20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7ff2fc304b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7ff2fc2f0329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7ff2f40864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7ff2f37a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7ff2f37a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5654e3b18c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5654e3aa52a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5654e3aa5bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5654e3aa5c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5654e3b18b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5654e3b84c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5654e3b87f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5654e3aa6c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5654e3be5c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5654e3c0b407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5654e3c0b634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5654e3c0b718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5654e3c0b75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5654e3c0b972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5654e3c11f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5654e3c121ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5654e3c12469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7ff2fcde9d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7ff2fcde9e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5654e3b7d2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:41:39] ‚ö† Ê®°Âûã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:41:39] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:41:39 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:41:39 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:41:39 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:41:45 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:41:51 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:41:52 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6ce4fb2500>
INFO 12-03 22:42:27 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:42:27 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:42:27 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:42:27 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:42:27 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:42:27 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:42:27 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:42:27 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:42:27 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:42:27 [core.py:396]     self._init_executor()
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:42:27 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:42:27 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:42:27 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:42:27 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:42:27 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:42:27 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:42:27 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:42:27 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:42:27 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:42:27 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:42:27 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:42:27 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:42:27 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:42:27 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:42:27 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:42:27 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:42:27 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:42:27 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:42:27 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:42:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:42:27 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:42:27 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2935067 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2935067 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6f9896c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f6f98915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f6f4988e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f6f98d6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f6f98d6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f6f98d82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f6f98d6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f6f90e864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f6f905a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f6f905a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5648528a0c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56485282d2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56485282dbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56485282dc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5648528a0b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x56485290cc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x56485290ff1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x56485282ec98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x56485296dc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x564852993407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x564852993634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x564852993718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56485299375b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x564852993972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x564852999f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56485299a1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56485299a469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f6f99b29d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f6f99b29e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5648529052d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:42:29] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:42:29] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:42:29 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:42:29 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:42:29 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:42:35 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:42:41 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:42:41 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f150d6218d0>
INFO 12-03 22:42:51 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:42:51 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:42:51 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:42:52 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:42:52 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:42:52 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:42:52 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:42:52 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:42:52 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:42:52 [core.py:396]     self._init_executor()
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:42:52 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:42:52 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:42:52 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:42:52 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:42:52 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:42:52 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:42:52 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:42:52 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:42:52 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:42:52 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:42:52 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:42:52 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:42:52 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:42:52 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:42:52 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:42:52 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:42:52 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:42:52 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:42:52 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:42:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:42:52 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:42:52 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2936781 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2936781 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f17c0f6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f17c0f15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f1771e8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f17c136bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f17c136c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f17c1382b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f17c136e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f17b94864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f17b8ba5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f17b8ba64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55f649212c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55f64919f2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55f64919fbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55f64919fc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55f649212b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55f64927ec3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55f649281f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55f6491a0c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55f6492dfc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55f649305407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55f649305634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55f649305718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55f64930575b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55f649305972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55f64930bf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55f64930c1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55f64930c469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f17c21c9d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f17c21c9e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55f6492772d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:42:53] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:42:53] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:42:53 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:42:53 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:42:53 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:42:59 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:43:06 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:43:06 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f3b04d506a0>
INFO 12-03 22:43:17 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:43:17 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:43:17 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:43:17 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:43:17 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:43:17 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:43:17 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:43:17 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:43:17 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:43:17 [core.py:396]     self._init_executor()
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:43:17 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:43:17 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:43:17 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:43:17 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:43:17 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:43:17 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:43:17 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:43:17 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:43:17 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:43:17 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:43:17 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:43:17 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:43:17 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:43:17 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:43:17 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:43:17 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:43:17 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:43:17 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:43:17 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:43:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:43:17 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:43:17 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2937304 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2937304 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3db896c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f3db8915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f3d6948e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f3db8dcfb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f3db8dd020e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f3db8de6b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f3db8dd2329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f3db0a864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f3db01a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f3db01a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55fc54e06c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55fc54d932a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55fc54d93bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55fc54d93c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55fc54e06b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55fc54e72c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55fc54e75f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55fc54d94c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55fc54ed3c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55fc54ef9407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55fc54ef9634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55fc54ef9718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55fc54ef975b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55fc54ef9972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55fc54efff60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55fc54f001ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55fc54f00469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f3db98cbd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f3db98cbe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55fc54e6b2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:43:19] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:43:19] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:43:19 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:43:19 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:43:19 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:43:25 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:43:31 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:43:31 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f00274e9840>
INFO 12-03 22:43:42 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:43:42 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:43:42 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:43:42 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:43:42 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:43:42 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:43:42 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:43:42 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:43:42 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:43:42 [core.py:396]     self._init_executor()
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:43:42 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:43:42 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:43:42 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:43:42 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:43:42 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:43:42 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:43:42 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:43:42 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:43:42 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:43:42 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:43:42 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:43:42 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:43:42 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:43:42 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:43:42 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:43:42 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:43:42 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:43:42 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:43:42 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:43:42 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:43:42 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:43:42 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2937892 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2937892 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f02db16c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f02db115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f028bc8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f02db57db78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f02db57e20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f02db594b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f02db580329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f02d32864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f02d29a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f02d29a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55b94da71c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55b94d9fe2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55b94d9febbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55b94d9fec83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55b94da71b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55b94daddc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55b94dae0f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55b94d9ffc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55b94db3ec30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55b94db64407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55b94db64634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55b94db64718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55b94db6475b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55b94db64972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55b94db6af60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55b94db6b1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55b94db6b469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f02dc079d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f02dc079e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55b94dad62d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:43:43] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:43:43] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:43:43 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:43:43 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:43:43 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:43:49 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:43:55 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:43:56 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f7743ccac80>
INFO 12-03 22:44:06 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:44:06 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:44:06 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:44:06 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:44:06 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:44:06 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:44:06 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:44:06 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:44:06 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:44:06 [core.py:396]     self._init_executor()
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:44:06 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:44:06 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:44:06 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:44:06 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:44:06 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:44:06 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:44:06 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:44:06 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:44:06 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:44:06 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:44:06 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:44:06 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:44:06 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:44:06 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:44:06 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:44:06 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:44:06 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:44:06 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:44:06 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:44:06 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:44:06 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:44:06 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2938421 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2938421 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f79f796c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f79f7915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f79a848e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f79f7d5ab78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f79f7d5b20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f79f7d71b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f79f7d5d329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f79efa864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f79ef1a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f79ef1a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x560e8e17cc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x560e8e1092a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x560e8e109bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x560e8e109c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x560e8e17cb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x560e8e1e8c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x560e8e1ebf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x560e8e10ac98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x560e8e249c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x560e8e26f407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x560e8e26f634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x560e8e26f718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x560e8e26f75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x560e8e26f972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x560e8e275f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x560e8e2761ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x560e8e276469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f79f8856d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f79f8856e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x560e8e1e12d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:44:08] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:44:08] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:44:08 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:44:08 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:44:08 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:44:14 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:44:20 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:44:20 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc3ff1f46a0>
INFO 12-03 22:44:31 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:44:31 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:44:31 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:44:31 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:44:31 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:44:31 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:44:31 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:44:31 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:44:31 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:44:31 [core.py:396]     self._init_executor()
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:44:31 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:44:31 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:44:31 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:44:31 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:44:31 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:44:31 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:44:31 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:44:31 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:44:31 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:44:31 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:44:31 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:44:31 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:44:31 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:44:31 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:44:31 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:44:31 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:44:31 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:44:31 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:44:31 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:44:31 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:44:31 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:44:31 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2939172 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2939172 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc6b2b6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fc6b2b15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fc663a8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fc6b2f6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fc6b2f6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fc6b2f82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fc6b2f6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fc6ab0864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fc6aa7a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fc6aa7a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55ee52eb5c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55ee52e422a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55ee52e42bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55ee52e42c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55ee52eb5b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55ee52f21c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55ee52f24f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55ee52e43c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55ee52f82c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55ee52fa8407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55ee52fa8634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55ee52fa8718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55ee52fa875b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55ee52fa8972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55ee52faef60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55ee52faf1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55ee52faf469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fc6b3d54d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fc6b3d54e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55ee52f1a2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:44:32] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:44:32] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:44:32 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:44:32 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:44:32 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:44:38 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:44:44 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:44:45 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5792602bc0>
INFO 12-03 22:44:55 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:44:55 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:44:55 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:44:55 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:44:56 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:44:56 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:44:56 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:44:56 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:44:56 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:44:56 [core.py:396]     self._init_executor()
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:44:56 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:44:56 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:44:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:44:56 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:44:56 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:44:56 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:44:56 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:44:56 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:44:56 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:44:56 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:44:56 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:44:56 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:44:56 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:44:56 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:44:56 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:44:56 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:44:56 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:44:56 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:44:56 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:44:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:44:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:44:56 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2939698 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2939698 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-12-03 22:44:57] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:44:57] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:44:57 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:44:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:44:57 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:45:03 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:45:10 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:45:10 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd7b4679ba0>
INFO 12-03 22:45:21 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:45:21 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:45:21 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:45:21 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:45:21 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:45:21 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:45:21 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:45:21 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:45:21 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:45:21 [core.py:396]     self._init_executor()
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:45:21 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:45:21 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:45:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:45:21 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:45:21 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:45:21 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:45:21 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:45:21 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:45:21 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:45:21 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:45:21 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:45:21 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:45:21 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:45:21 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:45:21 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:45:21 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:45:21 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:45:21 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:45:21 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:45:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:45:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:45:21 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2940392 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2940392 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fda6836c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fda68315b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fda18e8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fda68732b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fda6873320e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fda68749b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fda68735329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fda604864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fda5fba5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fda5fba64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55e41442dc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55e4143ba2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55e4143babbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55e4143bac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55e41442db85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55e414499c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55e41449cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55e4143bbc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55e4144fac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55e414520407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55e414520634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55e414520718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55e41452075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55e414520972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55e414526f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55e4145271ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55e414527469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fda6922ed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fda6922ee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55e4144922d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:45:22] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:45:22] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:45:22 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:45:22 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:45:22 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:45:28 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:45:35 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:45:35 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f993f556b90>
INFO 12-03 22:45:46 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:45:46 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:45:46 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:45:46 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:45:46 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:45:46 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:45:46 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:45:46 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:45:46 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:45:46 [core.py:396]     self._init_executor()
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:45:46 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:45:46 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:45:46 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:45:46 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:45:46 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:45:46 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:45:46 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:45:46 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:45:46 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:45:46 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:45:46 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:45:46 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:45:46 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:45:46 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:45:46 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:45:46 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:45:46 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:45:46 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:45:46 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:45:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:45:46 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:45:46 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2940834 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2940834 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9bf316c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f9bf3115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f9ba3c8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f9bf35c1b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f9bf35c220e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f9bf35d8b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f9bf35c4329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f9beb2864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f9bea9a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f9bea9a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x563165190c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56316511d2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56316511dbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56316511dc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x563165190b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5631651fcc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5631651fff1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x56316511ec98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x56316525dc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x563165283407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x563165283634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x563165283718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56316528375b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x563165283972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x563165289f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56316528a1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56316528a469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f9bf40bdd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f9bf40bde40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5631651f52d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:45:48] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:45:48] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:45:48 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:45:48 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:45:48 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:45:54 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:46:00 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:46:00 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4b64402200>
INFO 12-03 22:46:11 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:46:11 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:46:11 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:46:11 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:46:11 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:46:11 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:46:11 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:46:11 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:46:11 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:46:11 [core.py:396]     self._init_executor()
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:46:11 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:46:11 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:46:11 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:46:11 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:46:11 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:46:11 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:46:11 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:46:11 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:46:11 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:46:11 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:46:11 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:46:11 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:46:11 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:46:11 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:46:11 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:46:11 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:46:11 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:46:11 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:46:11 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:46:11 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:46:11 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:46:11 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2941595 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2941595 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f4e17d6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f4e17d15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f4dc8c8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f4e1816bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f4e1816c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f4e18182b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f4e1816e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f4e102864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f4e0f9a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f4e0f9a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x56188b6fec04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56188b68b2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56188b68bbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56188b68bc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x56188b6feb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x56188b76ac3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x56188b76df1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x56188b68cc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x56188b7cbc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x56188b7f1407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x56188b7f1634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x56188b7f1718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56188b7f175b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x56188b7f1972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x56188b7f7f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56188b7f81ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56188b7f8469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f4e18faad90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f4e18faae40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x56188b7632d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:46:13] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:46:13] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:46:13 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:46:13 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:46:13 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:46:19 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:46:25 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:46:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faa656c64a0>
INFO 12-03 22:46:36 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:46:36 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:46:36 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:46:36 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:46:36 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:46:36 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:46:36 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:46:36 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:46:36 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:46:36 [core.py:396]     self._init_executor()
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:46:36 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:46:36 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:46:36 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:46:36 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:46:36 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:46:36 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:46:36 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:46:36 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:46:36 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:46:36 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:46:36 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:46:36 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:46:36 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:46:36 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:46:36 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:46:36 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:46:36 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:46:36 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:46:36 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:46:36 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:46:36 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:46:36 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2942151 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2942151 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fad1936c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fad19315b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7facc9e8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fad1974cb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fad1974d20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fad19763b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fad1974f329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fad114864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fad10ba5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fad10ba64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5610045eac04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5610045772a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x561004577bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x561004577c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5610045eab85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x561004656c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x561004659f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x561004578c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5610046b7c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5610046dd407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5610046dd634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5610046dd718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5610046dd75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5610046dd972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5610046e3f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5610046e41ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5610046e4469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fad1a248d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fad1a248e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x56100464f2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:46:38] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:46:38] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:46:38 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:46:38 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:46:38 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:46:44 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:46:50 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:46:50 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4672a42b30>
INFO 12-03 22:47:01 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:47:01 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:47:01 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:47:01 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:47:01 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:47:01 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:47:01 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:47:01 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:47:01 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:47:01 [core.py:396]     self._init_executor()
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:47:01 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:47:01 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:47:01 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:47:01 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:47:01 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:47:01 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:47:01 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:47:01 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:47:01 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:47:01 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:47:01 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:47:01 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:47:01 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:47:01 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:47:01 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:47:01 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:47:01 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:47:01 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:47:01 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:47:01 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:47:01 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:47:01 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2942814 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2942814 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f492656c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f4926515b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f48d708e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f492691cb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f492691d20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f4926933b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f492691f329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f491e6864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f491dda5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f491dda64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x56134e014c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56134dfa12a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56134dfa1bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56134dfa1c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x56134e014b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x56134e080c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x56134e083f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x56134dfa2c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x56134e0e1c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x56134e107407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x56134e107634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x56134e107718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56134e10775b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x56134e107972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x56134e10df60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56134e10e1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56134e10e469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f4927418d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f4927418e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x56134e0792d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:47:03] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:47:03] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 22:47:03 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:47:03 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:47:03 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:47:09 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:47:15 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:47:16 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc60d3e9ea0>
INFO 12-03 22:47:26 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:47:26 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:47:26 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:47:26 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:47:27 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:47:27 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:47:27 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:47:27 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:47:27 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:47:27 [core.py:396]     self._init_executor()
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:47:27 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:47:27 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:47:27 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:47:27 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:47:27 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:47:27 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:47:27 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:47:27 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:47:27 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:47:27 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:47:27 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:47:27 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:47:27 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:47:27 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:47:27 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:47:27 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:47:27 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:47:27 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:47:27 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:47:27 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:47:27 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:47:27 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2943377 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2943377 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc8c0b6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fc8c0b15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fc871a8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fc8c0f6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fc8c0f6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fc8c0f82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fc8c0f6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fc8b90864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fc8b87a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fc8b87a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5635cd57bc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5635cd5082a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5635cd508bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5635cd508c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5635cd57bb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5635cd5e7c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5635cd5eaf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5635cd509c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5635cd648c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5635cd66e407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5635cd66e634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5635cd66e718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5635cd66e75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5635cd66e972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5635cd674f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5635cd6751ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5635cd675469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fc8c1d94d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fc8c1d94e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5635cd5e02d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:47:28] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:47:28] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 22:47:28 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:47:28 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:47:28 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:47:34 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:47:40 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:47:41 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2ac1485900>
INFO 12-03 22:47:51 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:47:51 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:47:51 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:47:51 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:47:52 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:47:52 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:47:52 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:47:52 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:47:52 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:47:52 [core.py:396]     self._init_executor()
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:47:52 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:47:52 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:47:52 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:47:52 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:47:52 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:47:52 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:47:52 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:47:52 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:47:52 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:47:52 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:47:52 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:47:52 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:47:52 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:47:52 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:47:52 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:47:52 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:47:52 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:47:52 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:47:52 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:47:52 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:47:52 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:47:52 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2944029 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2944029 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2d7516c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f2d75115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f2d25c8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f2d75536b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f2d7553720e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f2d7554db0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f2d75539329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f2d6d2864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f2d6c9a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f2d6c9a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55cf6fa41c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55cf6f9ce2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55cf6f9cebbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55cf6f9cec83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55cf6fa41b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55cf6faadc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55cf6fab0f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55cf6f9cfc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55cf6fb0ec30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55cf6fb34407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55cf6fb34634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55cf6fb34718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55cf6fb3475b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55cf6fb34972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55cf6fb3af60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55cf6fb3b1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55cf6fb3b469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f2d76032d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f2d76032e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55cf6faa62d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:47:53] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:47:53] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 22:47:53 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:47:53 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:47:53 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:48:00 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:48:06 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:48:06 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f81f3d76200>
INFO 12-03 22:48:17 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:48:17 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:48:17 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:48:17 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:48:17 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:48:17 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:48:17 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:48:17 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:48:17 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:48:17 [core.py:396]     self._init_executor()
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:48:17 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:48:17 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:48:17 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:48:17 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:48:17 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:48:17 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:48:17 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:48:17 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:48:17 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:48:17 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:48:17 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:48:17 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:48:17 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:48:17 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:48:17 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:48:17 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:48:17 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:48:17 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:48:17 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:48:17 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:48:17 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:48:17 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2944541 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2944541 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f84a776c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f84a7715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f845868e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f84a7b45b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f84a7b4620e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f84a7b5cb0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f84a7b48329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f849fc864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f849f3a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f849f3a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55cf16e0dc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55cf16d9a2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55cf16d9abbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55cf16d9ac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55cf16e0db85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55cf16e79c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55cf16e7cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55cf16d9bc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55cf16edac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55cf16f00407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55cf16f00634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55cf16f00718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55cf16f0075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55cf16f00972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55cf16f06f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55cf16f071ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55cf16f07469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f84a88f5d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f84a88f5e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55cf16e722d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:48:19] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 22:48:19] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 22:48:19 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-03 22:48:19 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 22:48:19 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 22:48:25 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 22:48:31 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 22:48:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1aeaf85900>
INFO 12-03 22:48:42 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 22:48:42 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 22:48:42 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 22:48:42 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 22:48:43 [core.py:396] EngineCore failed to start.
ERROR 12-03 22:48:43 [core.py:396] Traceback (most recent call last):
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 22:48:43 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 22:48:43 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 22:48:43 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 22:48:43 [core.py:396]     self._init_executor()
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 22:48:43 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 22:48:43 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 22:48:43 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 22:48:43 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 22:48:43 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 22:48:43 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 22:48:43 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 22:48:43 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 22:48:43 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 22:48:43 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 22:48:43 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 22:48:43 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 22:48:43 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 22:48:43 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 22:48:43 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 22:48:43 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 22:48:43 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 22:48:43 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 22:48:43 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 22:48:43 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 22:48:43 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 22:48:43 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2945125 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 92.81 MiB is free. Process 2919299 has 35.83 GiB memory in use. Process 2945125 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f1d9e96c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f1d9e915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f1d4f88e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f1d9ed45b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f1d9ed4620e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f1d9ed5cb0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f1d9ed48329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f1d96e864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f1d965a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f1d965a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x558ac2fbdc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x558ac2f4a2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x558ac2f4abbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x558ac2f4ac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x558ac2fbdb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x558ac3029c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x558ac302cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x558ac2f4bc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x558ac308ac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x558ac30b0407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x558ac30b0634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x558ac30b0718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x558ac30b075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x558ac30b0972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x558ac30b6f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x558ac30b71ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x558ac30b7469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f1d9faf7d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f1d9faf7e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x558ac30222d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 22:48:44] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
