INFO 12-03 19:37:24 [__init__.py:239] Automatically detected platform cuda.
[2025-12-03 19:38:37] ÂèëÁé∞ 7 ‰∏™ run„ÄÇ
[2025-12-03 19:38:38] ‚è≠ Ë∑≥Ëøá base-onlyÔºöLlama-3.2-3B-Instruct
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_100
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_200
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_300
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöA_rb_oracle_grpo_Llama-3.2-3B-Instruct__global_step_313
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300
[2025-12-03 19:38:38] ‚è≠ Ë∑≥ËøáÔºöB_rb_manual_algo1_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_313
INFO 12-03 19:38:45 [__init__.py:239] Automatically detected platform cuda.
[2025-12-03 19:43:16] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 19:43:27 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 19:43:27 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 19:43:27 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 19:43:37 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 19:43:44 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 19:43:51 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f48f4c4ab30>
INFO 12-03 19:44:23 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 19:44:23 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 19:44:23 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 19:44:23 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_100...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.12s/it]

INFO 12-03 19:44:25 [loader.py:458] Loading weights took 2.30 seconds
INFO 12-03 19:44:25 [gpu_model_runner.py:1347] Model loading took 6.0160 GiB and 2.452224 seconds
INFO 12-03 19:44:26 [kv_cache_utils.py:634] GPU KV cache size: 258,240 tokens
INFO 12-03 19:44:26 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.97x
INFO 12-03 19:44:26 [core.py:159] init engine (profile, create kv cache, warmup model) took 0.72 seconds
INFO 12-03 19:44:26 [core_client.py:439] Core engine process 0 ready.
[2025-12-03 19:44:26] ‚ÑπÔ∏è  ÂΩìÂâçÂ∑•‰ΩúËäÇÁÇπÂàÜÁâá: 1/8
[2025-12-03 19:44:26] ‚úì Ê®°ÂûãÂ∞±Áª™ÔºåÂºÄÂßãËØÑÊµã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100ÔºàÂÖ±‰∫´Âêå‰∏Ä LLMÔºå‰ªÖË°•Áº∫Êï∞ÊçÆÈõÜÔºâ
[2025-12-03 19:44:26] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1  ÂæÖËØÑÊµã=['aime25x8', 'amc23x8', 'aime24x8']  T=0.6  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 1/8 handling range [30:60]
==================================================
data: aime25x8  ,remain samples: 30
{'idx': 30, 'problem': 'Find the sum of all integer bases $b>9$ for which $17_{b}$ is a divisor of $97_{b}$.', 'answer': '70'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 329.17it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:03<15:03,  3.78s/it, est. speed input: 25.66 toks/s, output: 46.57 toks/s][A
Processed prompts:   1%|          | 2/240 [00:03<06:32,  1.65s/it, est. speed input: 41.16 toks/s, output: 91.47 toks/s][A
Processed prompts:   2%|‚ñè         | 4/240 [00:04<02:44,  1.43it/s, est. speed input: 121.78 toks/s, output: 175.54 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:04<02:22,  1.64it/s, est. speed input: 144.93 toks/s, output: 206.54 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:06<03:21,  1.16it/s, est. speed input: 127.47 toks/s, output: 203.19 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:06<02:47,  1.39it/s, est. speed input: 135.85 toks/s, output: 236.58 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:07<02:36,  1.48it/s, est. speed input: 133.92 toks/s, output: 262.28 toks/s][A
Processed prompts:   4%|‚ñç         | 9/240 [00:07<02:29,  1.55it/s, est. speed input: 136.44 toks/s, output: 286.98 toks/s][A
Processed prompts:   5%|‚ñç         | 11/240 [00:07<01:33,  2.45it/s, est. speed input: 175.47 toks/s, output: 365.70 toks/s][A
Processed prompts:   5%|‚ñå         | 12/240 [00:08<01:18,  2.91it/s, est. speed input: 203.11 toks/s, output: 403.47 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:08<01:18,  2.90it/s, est. speed input: 212.50 toks/s, output: 456.73 toks/s][A
Processed prompts:   6%|‚ñã         | 15/240 [00:08<01:11,  3.14it/s, est. speed input: 238.24 toks/s, output: 489.13 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:09<01:07,  3.30it/s, est. speed input: 245.04 toks/s, output: 519.25 toks/s][A
Processed prompts:   7%|‚ñã         | 17/240 [00:09<01:03,  3.52it/s, est. speed input: 252.88 toks/s, output: 550.48 toks/s][A
Processed prompts:   8%|‚ñä         | 20/240 [00:09<00:50,  4.36it/s, est. speed input: 284.44 toks/s, output: 647.63 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:10<00:38,  5.61it/s, est. speed input: 315.14 toks/s, output: 724.15 toks/s][A
Processed prompts:  10%|‚ñâ         | 23/240 [00:10<00:46,  4.66it/s, est. speed input: 310.35 toks/s, output: 742.33 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:10<00:36,  5.88it/s, est. speed input: 330.39 toks/s, output: 815.73 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:10<00:30,  6.96it/s, est. speed input: 344.74 toks/s, output: 887.96 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 29/240 [00:10<00:24,  8.49it/s, est. speed input: 377.20 toks/s, output: 963.52 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:11<00:20, 10.37it/s, est. speed input: 402.12 toks/s, output: 1040.64 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 34/240 [00:11<00:17, 11.64it/s, est. speed input: 431.21 toks/s, output: 1148.83 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 36/240 [00:11<00:16, 12.03it/s, est. speed input: 444.91 toks/s, output: 1219.14 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 38/240 [00:11<00:17, 11.24it/s, est. speed input: 450.99 toks/s, output: 1283.09 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 40/240 [00:11<00:19, 10.02it/s, est. speed input: 471.17 toks/s, output: 1340.83 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 43/240 [00:12<00:17, 11.03it/s, est. speed input: 489.37 toks/s, output: 1442.54 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 45/240 [00:12<00:20,  9.58it/s, est. speed input: 499.37 toks/s, output: 1494.25 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 47/240 [00:12<00:22,  8.76it/s, est. speed input: 512.66 toks/s, output: 1546.25 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 51/240 [00:12<00:14, 13.35it/s, est. speed input: 544.77 toks/s, output: 1703.71 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñé       | 54/240 [00:12<00:11, 16.23it/s, est. speed input: 567.22 toks/s, output: 1817.71 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 57/240 [00:13<00:13, 13.49it/s, est. speed input: 607.53 toks/s, output: 1902.68 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 59/240 [00:13<00:13, 13.45it/s, est. speed input: 615.68 toks/s, output: 1966.05 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 61/240 [00:13<00:16, 10.77it/s, est. speed input: 635.95 toks/s, output: 2007.51 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 64/240 [00:13<00:13, 12.79it/s, est. speed input: 656.80 toks/s, output: 2112.06 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 66/240 [00:14<00:15, 11.18it/s, est. speed input: 663.49 toks/s, output: 2159.14 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 68/240 [00:14<00:17,  9.84it/s, est. speed input: 665.21 toks/s, output: 2202.33 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 70/240 [00:14<00:15, 11.02it/s, est. speed input: 688.28 toks/s, output: 2267.70 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 72/240 [00:14<00:19,  8.45it/s, est. speed input: 690.89 toks/s, output: 2294.10 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 74/240 [00:14<00:17,  9.45it/s, est. speed input: 699.36 toks/s, output: 2355.47 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 77/240 [00:15<00:13, 12.30it/s, est. speed input: 727.26 toks/s, output: 2462.21 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 81/240 [00:15<00:11, 14.26it/s, est. speed input: 745.41 toks/s, output: 2594.29 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 83/240 [00:15<00:13, 11.76it/s, est. speed input: 758.45 toks/s, output: 2633.26 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 85/240 [00:15<00:16,  9.18it/s, est. speed input: 759.88 toks/s, output: 2657.43 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 87/240 [00:16<00:16,  9.34it/s, est. speed input: 763.61 toks/s, output: 2707.98 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 89/240 [00:16<00:20,  7.45it/s, est. speed input: 770.64 toks/s, output: 2723.55 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 92/240 [00:17<00:26,  5.62it/s, est. speed input: 766.53 toks/s, output: 2727.07 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 93/240 [00:17<00:26,  5.54it/s, est. speed input: 762.89 toks/s, output: 2738.58 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 95/240 [00:17<00:21,  6.90it/s, est. speed input: 787.94 toks/s, output: 2803.14 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:17<00:16,  8.81it/s, est. speed input: 803.62 toks/s, output: 2898.10 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 101/240 [00:17<00:12, 11.46it/s, est. speed input: 826.85 toks/s, output: 3004.26 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 103/240 [00:18<00:13, 10.13it/s, est. speed input: 825.21 toks/s, output: 3044.15 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/240 [00:18<00:12, 10.91it/s, est. speed input: 839.77 toks/s, output: 3104.00 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:18<00:11, 11.20it/s, est. speed input: 846.17 toks/s, output: 3160.02 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/240 [00:18<00:14,  9.03it/s, est. speed input: 852.89 toks/s, output: 3188.62 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 113/240 [00:18<00:09, 13.58it/s, est. speed input: 890.06 toks/s, output: 3337.27 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 115/240 [00:19<00:10, 11.58it/s, est. speed input: 903.86 toks/s, output: 3377.16 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/240 [00:19<00:09, 12.87it/s, est. speed input: 925.34 toks/s, output: 3442.87 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 119/240 [00:19<00:09, 13.08it/s, est. speed input: 930.06 toks/s, output: 3501.12 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 121/240 [00:19<00:11, 10.40it/s, est. speed input: 942.08 toks/s, output: 3533.03 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 123/240 [00:20<00:15,  7.48it/s, est. speed input: 932.83 toks/s, output: 3537.68 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 126/240 [00:20<00:12,  9.14it/s, est. speed input: 947.82 toks/s, output: 3628.05 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:20<00:09, 12.02it/s, est. speed input: 959.63 toks/s, output: 3736.78 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 132/240 [00:20<00:07, 13.60it/s, est. speed input: 975.53 toks/s, output: 3834.28 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 134/240 [00:21<00:16,  6.60it/s, est. speed input: 951.31 toks/s, output: 3776.99 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 136/240 [00:21<00:17,  6.01it/s, est. speed input: 951.11 toks/s, output: 3789.71 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:22<00:15,  6.50it/s, est. speed input: 955.78 toks/s, output: 3834.33 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:22<00:14,  6.74it/s, est. speed input: 955.65 toks/s, output: 3856.82 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 140/240 [00:22<00:16,  5.89it/s, est. speed input: 948.79 toks/s, output: 3854.89 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:22<00:18,  5.31it/s, est. speed input: 944.22 toks/s, output: 3854.36 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 142/240 [00:23<00:20,  4.89it/s, est. speed input: 938.47 toks/s, output: 3854.48 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:23<00:30,  3.21it/s, est. speed input: 919.02 toks/s, output: 3794.48 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:23<00:30,  3.19it/s, est. speed input: 912.75 toks/s, output: 3787.41 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:24<00:24,  3.82it/s, est. speed input: 919.64 toks/s, output: 3811.70 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [00:24<00:19,  4.88it/s, est. speed input: 925.53 toks/s, output: 3857.22 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:24<00:13,  6.51it/s, est. speed input: 931.39 toks/s, output: 3922.01 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [00:24<00:14,  6.26it/s, est. speed input: 934.93 toks/s, output: 3936.81 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [00:24<00:16,  5.30it/s, est. speed input: 927.81 toks/s, output: 3935.98 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:25<00:16,  5.34it/s, est. speed input: 930.39 toks/s, output: 3951.30 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/240 [00:25<00:26,  3.34it/s, est. speed input: 912.31 toks/s, output: 3901.56 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 154/240 [00:25<00:22,  3.82it/s, est. speed input: 917.23 toks/s, output: 3921.08 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155/240 [00:26<00:27,  3.12it/s, est. speed input: 906.14 toks/s, output: 3895.57 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [00:26<00:21,  3.88it/s, est. speed input: 907.81 toks/s, output: 3924.63 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 157/240 [00:27<00:42,  1.95it/s, est. speed input: 874.61 toks/s, output: 3807.64 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 158/240 [00:28<00:44,  1.84it/s, est. speed input: 865.14 toks/s, output: 3768.27 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 159/240 [00:28<00:38,  2.10it/s, est. speed input: 860.19 toks/s, output: 3771.86 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 161/240 [00:28<00:23,  3.30it/s, est. speed input: 861.96 toks/s, output: 3834.45 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 162/240 [00:29<00:27,  2.85it/s, est. speed input: 851.41 toks/s, output: 3813.99 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 163/240 [00:29<00:23,  3.27it/s, est. speed input: 850.45 toks/s, output: 3835.39 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 165/240 [00:30<00:23,  3.13it/s, est. speed input: 836.69 toks/s, output: 3837.58 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 166/240 [00:30<00:22,  3.34it/s, est. speed input: 834.76 toks/s, output: 3852.59 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 167/240 [00:30<00:21,  3.44it/s, est. speed input: 832.01 toks/s, output: 3863.96 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 168/240 [00:31<00:30,  2.34it/s, est. speed input: 812.72 toks/s, output: 3809.54 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 169/240 [00:31<00:24,  2.92it/s, est. speed input: 815.12 toks/s, output: 3840.01 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 170/240 [00:31<00:21,  3.32it/s, est. speed input: 817.86 toks/s, output: 3861.02 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 171/240 [00:32<00:30,  2.29it/s, est. speed input: 803.74 toks/s, output: 3814.34 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 173/240 [00:34<00:42,  1.56it/s, est. speed input: 770.33 toks/s, output: 3704.24 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 174/240 [00:34<00:38,  1.72it/s, est. speed input: 763.96 toks/s, output: 3706.89 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 175/240 [00:35<00:36,  1.80it/s, est. speed input: 757.09 toks/s, output: 3701.18 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 176/240 [00:36<00:56,  1.14it/s, est. speed input: 724.66 toks/s, output: 3570.95 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 177/240 [00:39<01:21,  1.29s/it, est. speed input: 687.24 toks/s, output: 3401.41 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 178/240 [00:41<01:33,  1.51s/it, est. speed input: 656.19 toks/s, output: 3277.35 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 179/240 [00:42<01:27,  1.44s/it, est. speed input: 642.53 toks/s, output: 3226.26 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 180/240 [00:43<01:18,  1.30s/it, est. speed input: 631.19 toks/s, output: 3200.70 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 181/240 [00:44<01:06,  1.12s/it, est. speed input: 624.37 toks/s, output: 3196.66 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 182/240 [00:49<02:17,  2.37s/it, est. speed input: 561.77 toks/s, output: 2900.32 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 183/240 [00:50<01:45,  1.85s/it, est. speed input: 557.33 toks/s, output: 2910.31 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 184/240 [00:50<01:23,  1.49s/it, est. speed input: 553.04 toks/s, output: 2919.27 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 185/240 [00:53<01:33,  1.70s/it, est. speed input: 531.39 toks/s, output: 2844.52 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 186/240 [00:53<01:07,  1.25s/it, est. speed input: 531.60 toks/s, output: 2879.74 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 188/240 [00:53<00:37,  1.39it/s, est. speed input: 534.29 toks/s, output: 2961.58 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 189/240 [00:53<00:32,  1.58it/s, est. speed input: 532.85 toks/s, output: 2987.40 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 190/240 [00:55<00:44,  1.13it/s, est. speed input: 520.44 toks/s, output: 2947.92 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 191/240 [00:55<00:32,  1.49it/s, est. speed input: 520.81 toks/s, output: 2988.62 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 192/240 [01:06<02:48,  3.52s/it, est. speed input: 436.69 toks/s, output: 2546.57 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:06<00:00,  3.61it/s, est. speed input: 545.03 toks/s, output: 4763.60 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 18224.22it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 0, 'acc': 3.3, 'total_acc': 0.8333333333333334, 'pass_at_k_percent': {'1': 0.8, '8': 6.7}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1/aime25x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 19:45:33] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1/aime25x8  acc=3.3 pass_at_k={'1': 0.8, '8': 6.7}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:07<02:14, 67.36s/ds][Info] Sharding enabled: Process 1/8 handling range [40:80]
==================================================
data: amc23x8  ,remain samples: 40
{'idx': 40, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}

  0%|          | 0/40 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:00<00:00, 447.22it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/320 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 3/320 [00:00<00:51,  6.11it/s, est. speed input: 703.23 toks/s, output: 57.07 toks/s][A
Processed prompts:   1%|‚ñè         | 4/320 [00:02<04:21,  1.21it/s, est. speed input: 162.70 toks/s, output: 57.06 toks/s][A
Processed prompts:   2%|‚ñè         | 5/320 [00:03<03:31,  1.49it/s, est. speed input: 188.44 toks/s, output: 97.33 toks/s][A
Processed prompts:   2%|‚ñè         | 6/320 [00:03<03:31,  1.48it/s, est. speed input: 173.30 toks/s, output: 125.82 toks/s][A
Processed prompts:   2%|‚ñè         | 7/320 [00:04<03:28,  1.50it/s, est. speed input: 174.61 toks/s, output: 152.90 toks/s][A
Processed prompts:   2%|‚ñé         | 8/320 [00:04<02:38,  1.97it/s, est. speed input: 184.12 toks/s, output: 193.41 toks/s][A
Processed prompts:   3%|‚ñé         | 9/320 [00:04<02:19,  2.23it/s, est. speed input: 203.49 toks/s, output: 226.24 toks/s][A
Processed prompts:   3%|‚ñé         | 11/320 [00:05<01:40,  3.06it/s, est. speed input: 230.18 toks/s, output: 297.37 toks/s][A
Processed prompts:   4%|‚ñç         | 12/320 [00:05<01:43,  2.97it/s, est. speed input: 235.02 toks/s, output: 322.92 toks/s][A
Processed prompts:   4%|‚ñç         | 13/320 [00:05<01:26,  3.54it/s, est. speed input: 250.32 toks/s, output: 360.05 toks/s][A
Processed prompts:   4%|‚ñç         | 14/320 [00:05<01:19,  3.84it/s, est. speed input: 260.59 toks/s, output: 392.49 toks/s][A
Processed prompts:   5%|‚ñç         | 15/320 [00:06<01:06,  4.60it/s, est. speed input: 266.94 toks/s, output: 429.76 toks/s][A
Processed prompts:   6%|‚ñå         | 18/320 [00:06<00:39,  7.62it/s, est. speed input: 300.23 toks/s, output: 548.14 toks/s][A
Processed prompts:   6%|‚ñã         | 20/320 [00:06<00:35,  8.39it/s, est. speed input: 317.99 toks/s, output: 618.12 toks/s][A
Processed prompts:   7%|‚ñã         | 22/320 [00:06<00:30,  9.62it/s, est. speed input: 338.01 toks/s, output: 691.04 toks/s][A
Processed prompts:   8%|‚ñä         | 24/320 [00:06<00:27, 10.94it/s, est. speed input: 364.48 toks/s, output: 764.39 toks/s][A
Processed prompts:   8%|‚ñä         | 27/320 [00:06<00:25, 11.62it/s, est. speed input: 382.54 toks/s, output: 866.34 toks/s][A
Processed prompts:   9%|‚ñâ         | 29/320 [00:07<00:22, 12.90it/s, est. speed input: 400.03 toks/s, output: 938.54 toks/s][A
Processed prompts:  10%|‚ñà         | 33/320 [00:07<00:17, 16.44it/s, est. speed input: 445.71 toks/s, output: 1087.82 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 37/320 [00:07<00:20, 14.08it/s, est. speed input: 491.60 toks/s, output: 1202.70 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 41/320 [00:08<00:27, 10.19it/s, est. speed input: 494.80 toks/s, output: 1275.77 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 43/320 [00:08<00:24, 11.29it/s, est. speed input: 527.39 toks/s, output: 1343.34 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 46/320 [00:08<00:19, 13.88it/s, est. speed input: 553.42 toks/s, output: 1327.93 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 48/320 [00:08<00:18, 14.83it/s, est. speed input: 570.56 toks/s, output: 1353.55 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 50/320 [00:09<00:35,  7.60it/s, est. speed input: 546.51 toks/s, output: 1295.94 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 52/320 [00:09<00:29,  8.98it/s, est. speed input: 558.72 toks/s, output: 1363.01 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 56/320 [00:09<00:20, 12.78it/s, est. speed input: 583.64 toks/s, output: 1506.68 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 59/320 [00:09<00:20, 12.80it/s, est. speed input: 596.06 toks/s, output: 1591.21 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 61/320 [00:09<00:20, 12.76it/s, est. speed input: 602.73 toks/s, output: 1646.37 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 63/320 [00:09<00:19, 13.26it/s, est. speed input: 632.40 toks/s, output: 1704.95 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 65/320 [00:10<00:22, 11.55it/s, est. speed input: 643.98 toks/s, output: 1744.98 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 67/320 [00:10<00:23, 10.96it/s, est. speed input: 650.64 toks/s, output: 1790.14 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 70/320 [00:10<00:17, 14.22it/s, est. speed input: 682.21 toks/s, output: 1892.80 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 73/320 [00:10<00:14, 17.24it/s, est. speed input: 719.81 toks/s, output: 1973.66 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 76/320 [00:10<00:12, 19.93it/s, est. speed input: 752.40 toks/s, output: 2075.02 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 79/320 [00:10<00:15, 15.56it/s, est. speed input: 768.16 toks/s, output: 2141.67 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 81/320 [00:10<00:14, 16.34it/s, est. speed input: 780.22 toks/s, output: 2176.28 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 83/320 [00:11<00:18, 12.81it/s, est. speed input: 783.64 toks/s, output: 2206.86 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 85/320 [00:11<00:17, 13.44it/s, est. speed input: 794.15 toks/s, output: 2262.20 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 87/320 [00:11<00:17, 13.30it/s, est. speed input: 798.35 toks/s, output: 2312.03 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 89/320 [00:11<00:15, 14.61it/s, est. speed input: 808.03 toks/s, output: 2348.49 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 91/320 [00:11<00:22,  9.98it/s, est. speed input: 798.83 toks/s, output: 2357.58 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 93/320 [00:12<00:25,  8.89it/s, est. speed input: 793.56 toks/s, output: 2382.22 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 95/320 [00:12<00:23,  9.43it/s, est. speed input: 809.09 toks/s, output: 2427.66 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 97/320 [00:12<00:25,  8.86it/s, est. speed input: 812.19 toks/s, output: 2458.21 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 99/320 [00:12<00:20, 10.58it/s, est. speed input: 820.54 toks/s, output: 2518.49 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 101/320 [00:12<00:18, 11.70it/s, est. speed input: 837.32 toks/s, output: 2573.53 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 103/320 [00:13<00:16, 13.31it/s, est. speed input: 846.68 toks/s, output: 2615.47 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 105/320 [00:13<00:15, 13.97it/s, est. speed input: 849.71 toks/s, output: 2670.19 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 107/320 [00:13<00:25,  8.48it/s, est. speed input: 848.44 toks/s, output: 2659.92 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 109/320 [00:13<00:25,  8.29it/s, est. speed input: 844.05 toks/s, output: 2671.36 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 112/320 [00:13<00:18, 11.11it/s, est. speed input: 863.32 toks/s, output: 2748.57 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 115/320 [00:14<00:22,  8.94it/s, est. speed input: 860.96 toks/s, output: 2760.11 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 117/320 [00:14<00:27,  7.40it/s, est. speed input: 864.48 toks/s, output: 2763.41 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 119/320 [00:15<00:25,  7.93it/s, est. speed input: 871.56 toks/s, output: 2805.66 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 121/320 [00:15<00:22,  8.89it/s, est. speed input: 882.39 toks/s, output: 2856.94 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 123/320 [00:15<00:18, 10.52it/s, est. speed input: 886.06 toks/s, output: 2917.84 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 125/320 [00:15<00:16, 12.15it/s, est. speed input: 895.01 toks/s, output: 2954.01 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 127/320 [00:15<00:15, 12.43it/s, est. speed input: 902.61 toks/s, output: 3004.73 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 129/320 [00:15<00:13, 13.97it/s, est. speed input: 928.74 toks/s, output: 3052.40 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 132/320 [00:15<00:11, 15.81it/s, est. speed input: 936.94 toks/s, output: 3118.35 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 134/320 [00:15<00:12, 14.33it/s, est. speed input: 943.32 toks/s, output: 3163.98 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 139/320 [00:16<00:09, 19.01it/s, est. speed input: 961.96 toks/s, output: 3307.26 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 142/320 [00:16<00:08, 20.33it/s, est. speed input: 980.96 toks/s, output: 3377.66 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 145/320 [00:16<00:08, 21.40it/s, est. speed input: 990.20 toks/s, output: 3447.66 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 148/320 [00:16<00:09, 18.36it/s, est. speed input: 995.68 toks/s, output: 3498.45 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 152/320 [00:16<00:09, 17.76it/s, est. speed input: 1006.47 toks/s, output: 3589.76 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 154/320 [00:16<00:09, 17.54it/s, est. speed input: 1023.34 toks/s, output: 3644.55 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 156/320 [00:17<00:12, 13.60it/s, est. speed input: 1032.07 toks/s, output: 3659.58 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 158/320 [00:17<00:11, 14.28it/s, est. speed input: 1046.11 toks/s, output: 3689.02 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 161/320 [00:17<00:09, 16.82it/s, est. speed input: 1065.37 toks/s, output: 3769.49 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 164/320 [00:17<00:10, 15.08it/s, est. speed input: 1072.89 toks/s, output: 3820.49 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 166/320 [00:17<00:09, 15.49it/s, est. speed input: 1089.57 toks/s, output: 3863.86 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 170/320 [00:18<00:08, 18.21it/s, est. speed input: 1111.82 toks/s, output: 3935.11 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 172/320 [00:18<00:15,  9.46it/s, est. speed input: 1090.00 toks/s, output: 3895.28 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 175/320 [00:18<00:12, 11.90it/s, est. speed input: 1110.04 toks/s, output: 3990.13 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 177/320 [00:18<00:11, 12.38it/s, est. speed input: 1114.68 toks/s, output: 4026.98 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 180/320 [00:19<00:10, 13.46it/s, est. speed input: 1121.73 toks/s, output: 4092.16 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 182/320 [00:19<00:13,  9.90it/s, est. speed input: 1109.74 toks/s, output: 4074.88 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 184/320 [00:19<00:15,  8.81it/s, est. speed input: 1102.60 toks/s, output: 4093.09 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 186/320 [00:19<00:13,  9.84it/s, est. speed input: 1104.85 toks/s, output: 4127.68 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 188/320 [00:20<00:13,  9.51it/s, est. speed input: 1106.42 toks/s, output: 4161.11 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 190/320 [00:20<00:11, 10.92it/s, est. speed input: 1116.15 toks/s, output: 4218.38 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 192/320 [00:20<00:15,  8.49it/s, est. speed input: 1109.80 toks/s, output: 4209.20 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 194/320 [00:20<00:12,  9.87it/s, est. speed input: 1116.18 toks/s, output: 4236.19 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 197/320 [00:20<00:09, 13.26it/s, est. speed input: 1135.22 toks/s, output: 4329.89 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 199/320 [00:21<00:14,  8.49it/s, est. speed input: 1125.53 toks/s, output: 4315.33 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 201/320 [00:21<00:13,  9.06it/s, est. speed input: 1124.53 toks/s, output: 4359.98 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 203/320 [00:21<00:13,  8.59it/s, est. speed input: 1119.60 toks/s, output: 4388.77 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 205/320 [00:22<00:20,  5.50it/s, est. speed input: 1094.99 toks/s, output: 4318.13 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 208/320 [00:22<00:14,  7.67it/s, est. speed input: 1103.89 toks/s, output: 4397.60 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 210/320 [00:22<00:12,  8.96it/s, est. speed input: 1109.91 toks/s, output: 4429.40 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 212/320 [00:22<00:10, 10.29it/s, est. speed input: 1114.61 toks/s, output: 4488.88 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 214/320 [00:22<00:10,  9.72it/s, est. speed input: 1117.29 toks/s, output: 4508.75 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 216/320 [00:23<00:15,  6.85it/s, est. speed input: 1103.43 toks/s, output: 4467.85 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 218/320 [00:23<00:13,  7.59it/s, est. speed input: 1104.29 toks/s, output: 4514.66 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 220/320 [00:23<00:14,  7.08it/s, est. speed input: 1098.57 toks/s, output: 4522.75 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 221/320 [00:24<00:14,  6.81it/s, est. speed input: 1097.01 toks/s, output: 4519.96 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 223/320 [00:24<00:11,  8.30it/s, est. speed input: 1100.71 toks/s, output: 4578.81 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 225/320 [00:24<00:18,  5.16it/s, est. speed input: 1079.15 toks/s, output: 4520.41 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 226/320 [00:25<00:17,  5.45it/s, est. speed input: 1077.59 toks/s, output: 4537.89 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 227/320 [00:25<00:20,  4.56it/s, est. speed input: 1065.95 toks/s, output: 4517.18 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 229/320 [00:25<00:15,  5.99it/s, est. speed input: 1068.61 toks/s, output: 4559.27 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 231/320 [00:25<00:11,  7.74it/s, est. speed input: 1071.87 toks/s, output: 4623.44 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 233/320 [00:26<00:17,  4.99it/s, est. speed input: 1050.24 toks/s, output: 4573.98 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 234/320 [00:26<00:20,  4.21it/s, est. speed input: 1040.56 toks/s, output: 4538.72 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 236/320 [00:27<00:18,  4.63it/s, est. speed input: 1035.42 toks/s, output: 4565.24 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 238/320 [00:27<00:14,  5.58it/s, est. speed input: 1033.91 toks/s, output: 4615.45 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 239/320 [00:28<00:32,  2.48it/s, est. speed input: 990.00 toks/s, output: 4444.36 toks/s] [A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 240/320 [00:28<00:29,  2.75it/s, est. speed input: 985.33 toks/s, output: 4453.43 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 241/320 [00:29<00:32,  2.47it/s, est. speed input: 969.71 toks/s, output: 4415.72 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 242/320 [00:30<00:44,  1.76it/s, est. speed input: 938.77 toks/s, output: 4294.53 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 243/320 [00:31<00:41,  1.86it/s, est. speed input: 928.82 toks/s, output: 4275.90 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 244/320 [00:31<00:36,  2.05it/s, est. speed input: 922.12 toks/s, output: 4271.63 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 245/320 [00:31<00:34,  2.20it/s, est. speed input: 916.48 toks/s, output: 4265.42 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 246/320 [00:32<00:33,  2.23it/s, est. speed input: 907.76 toks/s, output: 4242.39 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 247/320 [00:32<00:29,  2.48it/s, est. speed input: 901.65 toks/s, output: 4248.06 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 249/320 [00:32<00:21,  3.29it/s, est. speed input: 897.93 toks/s, output: 4288.07 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 250/320 [00:33<00:23,  3.03it/s, est. speed input: 891.86 toks/s, output: 4279.65 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 251/320 [00:33<00:29,  2.37it/s, est. speed input: 876.46 toks/s, output: 4238.12 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 253/320 [00:35<00:35,  1.89it/s, est. speed input: 847.57 toks/s, output: 4163.71 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 254/320 [00:35<00:33,  1.96it/s, est. speed input: 840.12 toks/s, output: 4155.65 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 255/320 [00:35<00:28,  2.28it/s, est. speed input: 838.50 toks/s, output: 4175.45 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 256/320 [00:37<00:52,  1.21it/s, est. speed input: 798.29 toks/s, output: 4010.22 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 257/320 [00:40<01:30,  1.44s/it, est. speed input: 741.00 toks/s, output: 3753.35 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 258/320 [00:41<01:16,  1.23s/it, est. speed input: 730.33 toks/s, output: 3736.02 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 259/320 [00:43<01:28,  1.44s/it, est. speed input: 700.00 toks/s, output: 3613.45 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 260/320 [00:45<01:28,  1.47s/it, est. speed input: 677.96 toks/s, output: 3537.15 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 261/320 [00:47<01:46,  1.81s/it, est. speed input: 642.79 toks/s, output: 3388.15 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 262/320 [00:48<01:35,  1.64s/it, est. speed input: 628.83 toks/s, output: 3341.96 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 263/320 [01:00<04:22,  4.60s/it, est. speed input: 509.32 toks/s, output: 2746.56 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 264/320 [01:08<05:10,  5.54s/it, est. speed input: 453.21 toks/s, output: 2480.17 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 313/320 [01:11<00:02,  3.24it/s, est. speed input: 524.14 toks/s, output: 4497.30 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 314/320 [01:11<00:01,  3.14it/s, est. speed input: 520.28 toks/s, output: 4497.00 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 316/320 [01:12<00:01,  3.32it/s, est. speed input: 521.57 toks/s, output: 4573.52 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 318/320 [01:12<00:00,  3.48it/s, est. speed input: 522.09 toks/s, output: 4639.05 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 319/320 [01:12<00:00,  3.33it/s, est. speed input: 519.71 toks/s, output: 4649.15 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:13<00:00,  3.27it/s, est. speed input: 518.64 toks/s, output: 4668.19 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:13<00:00,  4.37it/s, est. speed input: 518.64 toks/s, output: 4668.19 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/320 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [00:00<00:00, 18968.02it/s]
{'num_samples': 40, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 17.5, 'total_acc': 20.9375, 'pass_at_k_percent': {'1': 20.9, '8': 55.0}, 'pass_at_k_valid_counts': {'1': 40, '8': 40}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1/amc23x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 19:46:48] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1/amc23x8  acc=17.5 pass_at_k={'1': 20.9, '8': 55.0}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [02:21<01:11, 71.43s/ds][Info] Sharding enabled: Process 1/8 handling range [30:60]
==================================================
data: aime24x8  ,remain samples: 30
{'idx': 30, 'id': 60, 'problem': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.', 'solution': '$\\frac{9}{s} + t = 4$ in hours and $\\frac{9}{s+2} + t = 2.4$ in hours.\nSubtracting the second equation from the first, we get, \n$\\frac{9}{s} - \\frac{9}{s+2} = 1.6$\nMultiplying by $(s)(s+2)$, we get \n$9s+18-9s=18=1.6s^{2} + 3.2s$\nMultiplying by 5/2 on both sides, we get\n$0 = 4s^{2} + 8s - 45$\nFactoring gives us \n$(2s-5)(2s+9) = 0$, of which the solution we want is $s=2.5$.\nSubstituting this back to the first equation, we can find that $t = 0.4$ hours.\nLastly, $s + \\frac{1}{2} = 3$ kilometers per hour, so\n$\\frac{9}{3} + 0.4 = 3.4$ hours, or $\\framebox{204}$ minutes\n-Failure.net\nThe amount of hours spent while walking on the first travel is $\\frac{240-t}{6}$. Thus, we have the equation $(240-t)(s) = 540$, and by the same logic, the second equation yields $(144-t)(s+2) = 540$. We have $240s-st = 540$, and $288+144s-2t-st = 540$. We subtract the two equations to get $96s+2t-288 = 0$, so we have $48s+t = 144$, so $t = 144-48s$, and now we have $(96+48s)(s) = 540$. The numerator of $s$ must evenly divide 540, however, $s$ must be less than 3. We can guess that $s = 2.5$. Now, $2.5+0.5 = 3$. Taking $\\frac{9}{3} = 3$, we find that it will take three hours for the 9 kilometers to be traveled. The t minutes spent at the coffeeshop can be written as $144-48(2.5)$, so t = 24. $180 + 24 = 204$. -sepehr2010', 'answer': '204', 'url': 'https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems/Problem_1', 'question': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 470.34it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:00<02:23,  1.66it/s, est. speed input: 219.17 toks/s, output: 44.83 toks/s][A
Processed prompts:   1%|          | 2/240 [00:03<07:51,  1.98s/it, est. speed input: 83.92 toks/s, output: 55.20 toks/s] [A
Processed prompts:   1%|‚ñè         | 3/240 [00:03<04:41,  1.19s/it, est. speed input: 113.95 toks/s, output: 99.18 toks/s][A
Processed prompts:   2%|‚ñè         | 4/240 [00:03<03:05,  1.27it/s, est. speed input: 138.13 toks/s, output: 142.16 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:04<02:11,  1.79it/s, est. speed input: 163.78 toks/s, output: 184.40 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:04<01:31,  2.54it/s, est. speed input: 223.70 toks/s, output: 255.75 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:04<01:30,  2.56it/s, est. speed input: 225.66 toks/s, output: 283.03 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:05<00:58,  3.91it/s, est. speed input: 261.62 toks/s, output: 367.56 toks/s][A
Processed prompts:   5%|‚ñå         | 12/240 [00:05<00:40,  5.57it/s, est. speed input: 298.76 toks/s, output: 453.01 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:05<00:46,  4.89it/s, est. speed input: 336.15 toks/s, output: 506.57 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:05<00:38,  5.86it/s, est. speed input: 362.66 toks/s, output: 581.17 toks/s][A
Processed prompts:   8%|‚ñä         | 18/240 [00:06<00:32,  6.74it/s, est. speed input: 392.59 toks/s, output: 653.45 toks/s][A
Processed prompts:   8%|‚ñä         | 19/240 [00:07<01:07,  3.27it/s, est. speed input: 361.52 toks/s, output: 608.98 toks/s][A
Processed prompts:   9%|‚ñâ         | 21/240 [00:07<00:55,  3.97it/s, est. speed input: 394.65 toks/s, output: 674.50 toks/s][A
Processed prompts:   9%|‚ñâ         | 22/240 [00:07<00:50,  4.35it/s, est. speed input: 399.06 toks/s, output: 707.88 toks/s][A
Processed prompts:  10%|‚ñâ         | 23/240 [00:07<00:53,  4.05it/s, est. speed input: 398.29 toks/s, output: 726.07 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:08<00:40,  5.33it/s, est. speed input: 424.31 toks/s, output: 799.57 toks/s][A
Processed prompts:  11%|‚ñà         | 26/240 [00:08<00:39,  5.48it/s, est. speed input: 426.99 toks/s, output: 829.10 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 28/240 [00:08<00:31,  6.73it/s, est. speed input: 441.66 toks/s, output: 901.12 toks/s][A
Processed prompts:  12%|‚ñà‚ñé        | 30/240 [00:08<00:25,  8.26it/s, est. speed input: 462.75 toks/s, output: 976.96 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:08<00:27,  7.69it/s, est. speed input: 472.30 toks/s, output: 1003.90 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 34/240 [00:08<00:19, 10.50it/s, est. speed input: 510.18 toks/s, output: 1120.49 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 36/240 [00:09<00:40,  4.98it/s, est. speed input: 484.26 toks/s, output: 1108.92 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 37/240 [00:10<00:43,  4.71it/s, est. speed input: 479.25 toks/s, output: 1124.66 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 40/240 [00:10<00:33,  6.03it/s, est. speed input: 508.24 toks/s, output: 1223.01 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 44/240 [00:10<00:21,  9.33it/s, est. speed input: 550.61 toks/s, output: 1384.56 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 48/240 [00:10<00:17, 11.27it/s, est. speed input: 583.71 toks/s, output: 1530.32 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 51/240 [00:11<00:17, 11.03it/s, est. speed input: 624.42 toks/s, output: 1622.73 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 53/240 [00:11<00:25,  7.47it/s, est. speed input: 612.94 toks/s, output: 1630.67 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 55/240 [00:11<00:23,  8.00it/s, est. speed input: 626.37 toks/s, output: 1692.73 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 58/240 [00:12<00:28,  6.29it/s, est. speed input: 625.82 toks/s, output: 1730.03 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 59/240 [00:12<00:27,  6.50it/s, est. speed input: 632.43 toks/s, output: 1757.51 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 60/240 [00:12<00:26,  6.74it/s, est. speed input: 636.96 toks/s, output: 1784.98 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 63/240 [00:12<00:18,  9.35it/s, est. speed input: 701.80 toks/s, output: 1896.92 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 65/240 [00:13<00:24,  7.22it/s, est. speed input: 700.66 toks/s, output: 1921.92 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 68/240 [00:13<00:17,  9.57it/s, est. speed input: 718.27 toks/s, output: 2032.76 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 71/240 [00:13<00:14, 11.79it/s, est. speed input: 743.80 toks/s, output: 2142.66 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 73/240 [00:13<00:13, 12.24it/s, est. speed input: 751.98 toks/s, output: 2208.19 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 77/240 [00:13<00:11, 14.32it/s, est. speed input: 776.47 toks/s, output: 2348.92 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:14<00:11, 14.26it/s, est. speed input: 793.31 toks/s, output: 2444.53 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 82/240 [00:14<00:11, 14.26it/s, est. speed input: 806.65 toks/s, output: 2508.39 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 84/240 [00:14<00:11, 13.14it/s, est. speed input: 808.55 toks/s, output: 2563.25 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 86/240 [00:14<00:16,  9.51it/s, est. speed input: 802.95 toks/s, output: 2585.75 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 88/240 [00:15<00:15,  9.80it/s, est. speed input: 805.18 toks/s, output: 2640.81 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 90/240 [00:15<00:19,  7.73it/s, est. speed input: 803.47 toks/s, output: 2659.52 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 91/240 [00:15<00:20,  7.17it/s, est. speed input: 804.47 toks/s, output: 2671.27 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 92/240 [00:15<00:24,  6.16it/s, est. speed input: 797.94 toks/s, output: 2671.42 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 95/240 [00:16<00:20,  7.02it/s, est. speed input: 800.63 toks/s, output: 2743.14 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 97/240 [00:16<00:18,  7.81it/s, est. speed input: 810.27 toks/s, output: 2799.05 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 99/240 [00:16<00:17,  8.24it/s, est. speed input: 818.71 toks/s, output: 2850.48 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 100/240 [00:16<00:22,  6.33it/s, est. speed input: 807.45 toks/s, output: 2839.04 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/240 [00:17<00:18,  7.54it/s, est. speed input: 812.22 toks/s, output: 2898.89 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/240 [00:17<00:14,  9.40it/s, est. speed input: 831.63 toks/s, output: 2994.18 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:18<00:25,  5.13it/s, est. speed input: 823.00 toks/s, output: 2942.47 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 108/240 [00:18<00:23,  5.50it/s, est. speed input: 840.14 toks/s, output: 2967.34 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 110/240 [00:18<00:18,  6.88it/s, est. speed input: 849.26 toks/s, output: 3032.10 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 115/240 [00:18<00:15,  8.24it/s, est. speed input: 861.07 toks/s, output: 3165.70 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/240 [00:19<00:13,  9.09it/s, est. speed input: 865.82 toks/s, output: 3228.94 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 119/240 [00:19<00:20,  5.87it/s, est. speed input: 844.82 toks/s, output: 3201.28 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/240 [00:19<00:19,  6.12it/s, est. speed input: 844.90 toks/s, output: 3224.92 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:20<00:18,  6.34it/s, est. speed input: 860.96 toks/s, output: 3265.44 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:20<00:15,  7.70it/s, est. speed input: 866.15 toks/s, output: 3331.87 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 126/240 [00:20<00:16,  6.96it/s, est. speed input: 875.35 toks/s, output: 3362.73 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 127/240 [00:20<00:17,  6.46it/s, est. speed input: 872.24 toks/s, output: 3373.34 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:21<00:17,  6.53it/s, est. speed input: 871.19 toks/s, output: 3393.88 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:21<00:16,  6.62it/s, est. speed input: 870.03 toks/s, output: 3414.76 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 130/240 [00:21<00:15,  6.92it/s, est. speed input: 869.89 toks/s, output: 3438.84 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 134/240 [00:21<00:10, 10.14it/s, est. speed input: 885.85 toks/s, output: 3571.99 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:21<00:11,  9.47it/s, est. speed input: 888.35 toks/s, output: 3593.10 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 136/240 [00:21<00:13,  7.60it/s, est. speed input: 884.03 toks/s, output: 3598.36 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:22<00:13,  7.45it/s, est. speed input: 885.40 toks/s, output: 3641.59 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:22<00:11,  8.64it/s, est. speed input: 891.90 toks/s, output: 3728.99 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:22<00:12,  8.02it/s, est. speed input: 890.17 toks/s, output: 3769.29 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:23<00:11,  7.93it/s, est. speed input: 893.09 toks/s, output: 3815.55 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 146/240 [00:23<00:12,  7.79it/s, est. speed input: 891.57 toks/s, output: 3837.21 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [00:23<00:15,  6.17it/s, est. speed input: 885.99 toks/s, output: 3833.25 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 148/240 [00:23<00:16,  5.73it/s, est. speed input: 884.90 toks/s, output: 3842.59 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:23<00:16,  5.54it/s, est. speed input: 885.07 toks/s, output: 3855.30 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [00:24<00:14,  6.07it/s, est. speed input: 888.83 toks/s, output: 3880.81 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:24<00:19,  4.43it/s, est. speed input: 873.80 toks/s, output: 3872.05 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/240 [00:24<00:20,  4.28it/s, est. speed input: 868.19 toks/s, output: 3876.43 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155/240 [00:25<00:13,  6.08it/s, est. speed input: 871.60 toks/s, output: 3947.73 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [00:25<00:18,  4.63it/s, est. speed input: 863.14 toks/s, output: 3931.08 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 157/240 [00:25<00:18,  4.42it/s, est. speed input: 863.53 toks/s, output: 3936.59 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 161/240 [00:27<00:24,  3.16it/s, est. speed input: 837.74 toks/s, output: 3888.94 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 162/240 [00:27<00:28,  2.74it/s, est. speed input: 823.03 toks/s, output: 3850.36 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 164/240 [00:30<00:51,  1.48it/s, est. speed input: 761.00 toks/s, output: 3607.31 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 165/240 [00:31<00:53,  1.41it/s, est. speed input: 744.73 toks/s, output: 3556.03 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 166/240 [00:32<00:57,  1.29it/s, est. speed input: 725.76 toks/s, output: 3490.77 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 167/240 [00:32<00:50,  1.44it/s, est. speed input: 721.35 toks/s, output: 3489.35 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 168/240 [00:33<00:43,  1.66it/s, est. speed input: 718.30 toks/s, output: 3501.36 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 169/240 [00:33<00:45,  1.54it/s, est. speed input: 705.04 toks/s, output: 3467.96 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 170/240 [00:34<00:37,  1.88it/s, est. speed input: 704.26 toks/s, output: 3491.60 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 171/240 [00:34<00:41,  1.67it/s, est. speed input: 693.39 toks/s, output: 3460.54 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 172/240 [00:35<00:36,  1.88it/s, est. speed input: 692.79 toks/s, output: 3471.01 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 173/240 [00:35<00:29,  2.26it/s, est. speed input: 691.02 toks/s, output: 3495.59 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 174/240 [00:36<00:32,  2.06it/s, est. speed input: 683.00 toks/s, output: 3485.00 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 175/240 [00:37<00:47,  1.37it/s, est. speed input: 662.17 toks/s, output: 3409.25 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 176/240 [00:43<02:28,  2.33s/it, est. speed input: 573.26 toks/s, output: 2976.27 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 178/240 [00:45<01:52,  1.81s/it, est. speed input: 550.01 toks/s, output: 2911.15 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 179/240 [00:46<01:36,  1.58s/it, est. speed input: 543.69 toks/s, output: 2903.96 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 180/240 [00:47<01:14,  1.24s/it, est. speed input: 542.98 toks/s, output: 2931.88 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 181/240 [00:50<01:54,  1.94s/it, est. speed input: 504.59 toks/s, output: 2758.62 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 182/240 [00:52<01:53,  1.96s/it, est. speed input: 488.64 toks/s, output: 2700.47 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 183/240 [00:54<01:39,  1.75s/it, est. speed input: 479.77 toks/s, output: 2685.71 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 184/240 [01:04<04:03,  4.34s/it, est. speed input: 402.46 toks/s, output: 2286.84 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 185/240 [01:07<03:40,  4.01s/it, est. speed input: 385.98 toks/s, output: 2224.37 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:08<00:00,  3.53it/s, est. speed input: 491.88 toks/s, output: 4706.49 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 21326.97it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 10.0, 'total_acc': 6.25, 'pass_at_k_percent': {'1': 6.2, '8': 23.3}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1/aime24x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 19:47:57] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1/aime24x8  acc=10.0 pass_at_k={'1': 6.2, '8': 23.3}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:30<00:00, 70.28s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:30<00:00, 70.18s/ds]
[2025-12-03 19:47:57] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2  ÂæÖËØÑÊµã=['minerva_math', 'olympiadbench', 'math500']  T=0.0  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 1/8 handling range [34:68]
==================================================
data: minerva_math  ,remain samples: 34
{'problem': 'The Hubble Space telescope has an effective diameter of $2.5 \\mathrm{~m}$, and a typical wavelength used for observation by the Hubble might be $0.6 \\mu \\mathrm{m}$, or 600 nanometers (typical optical wavelength). Based on this information, compute an estimate for the angular resolution of the Hubble Space telescope in arcseconds.', 'solution': 'Using the formula for angular resolution $\\theta$ in terms of the effective size $d$ and the wavelength $\\lambda$, namely $\\theta = \\lambda/d$, gives \\boxed{0.05} arcseconds.', 'type': 'Introduction to Astronomy (8.282J Spring 2006)', 'idx': 34}

  0%|          | 0/34 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:00<00:00, 13632.19it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/272 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/272 [00:01<08:23,  1.86s/it, est. speed input: 74.79 toks/s, output: 47.88 toks/s][A
Processed prompts:   3%|‚ñé         | 9/272 [00:02<01:06,  3.94it/s, est. speed input: 489.95 toks/s, output: 305.54 toks/s][A
Processed prompts:   9%|‚ñâ         | 25/272 [00:04<00:33,  7.38it/s, est. speed input: 990.14 toks/s, output: 706.46 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 33/272 [00:04<00:27,  8.83it/s, est. speed input: 1015.54 toks/s, output: 835.22 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 41/272 [00:04<00:18, 12.41it/s, est. speed input: 1311.39 toks/s, output: 1191.18 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 49/272 [00:05<00:14, 15.01it/s, est. speed input: 1374.77 toks/s, output: 1486.22 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 57/272 [00:05<00:10, 20.05it/s, est. speed input: 1451.38 toks/s, output: 1830.22 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 65/272 [00:05<00:11, 18.69it/s, est. speed input: 1422.52 toks/s, output: 2025.98 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 81/272 [00:06<00:10, 18.07it/s, est. speed input: 1496.31 toks/s, output: 2415.66 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 89/272 [00:07<00:10, 17.37it/s, est. speed input: 1522.19 toks/s, output: 2604.55 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 97/272 [00:07<00:10, 17.10it/s, est. speed input: 1572.58 toks/s, output: 2803.79 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 113/272 [00:08<00:10, 15.59it/s, est. speed input: 1728.33 toks/s, output: 3116.94 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 121/272 [00:09<00:09, 16.08it/s, est. speed input: 1748.96 toks/s, output: 3336.53 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 137/272 [00:09<00:07, 18.20it/s, est. speed input: 1911.28 toks/s, output: 3825.65 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 144/272 [00:10<00:06, 20.45it/s, est. speed input: 1994.25 toks/s, output: 4094.70 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 152/272 [00:10<00:05, 21.19it/s, est. speed input: 2057.58 toks/s, output: 4328.69 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 160/272 [00:10<00:04, 23.83it/s, est. speed input: 2098.36 toks/s, output: 4538.25 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 168/272 [00:13<00:13,  7.74it/s, est. speed input: 1719.49 toks/s, output: 3891.85 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 176/272 [00:18<00:26,  3.60it/s, est. speed input: 1283.35 toks/s, output: 3114.37 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 183/272 [00:29<00:24,  3.60it/s, est. speed input: 1358.39 toks/s, output: 3479.67 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 184/272 [00:46<01:44,  1.19s/it, est. speed input: 554.01 toks/s, output: 1460.23 toks/s] [A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 192/272 [01:12<02:21,  1.76s/it, est. speed input: 369.89 toks/s, output: 1198.18 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [01:12<00:00,  3.77it/s, est. speed input: 629.78 toks/s, output: 4603.59 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/272 [00:00<?, ?it/s][A
Evaluate:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 241/272 [00:00<00:00, 488.16it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [00:01<00:00, 179.26it/s]
{'num_samples': 34, 'num_scores': 272, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 11.8, 'total_acc': 11.76470588235294, 'pass_at_k_percent': {'1': 11.8, '8': 11.8}, 'pass_at_k_valid_counts': {'1': 34, '8': 34}, 'type_acc': {'Differential Equations (18.03 Spring 2010)': 28.6, 'Ecology I (1.018J Fall 2009)': 20.0, 'Information and Entropy (6.050J Spring 2008)': 0.0, 'Introduction to Astronomy (8.282J Spring 2006)': 5.3}, 'type_pass_at_k_percent': {'Differential Equations (18.03 Spring 2010)': {'1': 28.6, '8': 28.6}, 'Ecology I (1.018J Fall 2009)': {'1': 20.0, '8': 20.0}, 'Information and Entropy (6.050J Spring 2008)': {'1': 0.0, '8': 0.0}, 'Introduction to Astronomy (8.282J Spring 2006)': {'1': 5.3, '8': 5.3}}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2/minerva_math/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 19:49:11] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2/minerva_math  acc=11.8 pass_at_k={'1': 11.8, '8': 11.8}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:13<02:27, 73.95s/ds][Info] Sharding enabled: Process 1/8 handling range [84:168]
==================================================
data: olympiadbench  ,remain samples: 84
{'idx': 84, 'id': 2006, 'subfield': 'Combinatorics', 'context': None, 'question': 'In each square of a garden shaped like a $2022 \\times 2022$ board, there is initially a tree of height 0 . A gardener and a lumberjack alternate turns playing the following game, with the gardener taking the first turn:\n\n- The gardener chooses a square in the garden. Each tree on that square and all the surrounding squares (of which there are at most eight) then becomes one unit taller.\n- The lumberjack then chooses four different squares on the board. Each tree of positive height on those squares then becomes one unit shorter.\n\nWe say that a tree is majestic if its height is at least $10^{6}$. Determine the largest number $K$ such that the gardener can ensure there are eventually $K$ majestic trees on the board, no matter how the lumberjack plays.', 'solution': ["We solve the problem for a general $3 N \\times 3 N$ board. First, we prove that the lumberjack has a strategy to ensure there are never more than $5 N^{2}$ majestic trees. Giving the squares of the board coordinates in the natural manner, colour each square where at least one of its coordinates are divisible by 3 , shown below for a $9 \\times 9$ board:\n\n<img_3271>\n\nThen, as each $3 \\times 3$ square on the board contains exactly 5 coloured squares, each move of the gardener will cause at most 4 trees on non-coloured squares to grow. The lumberjack may therefore cut those trees, ensuring no tree on a non-coloured square has positive height after his turn. Hence there cannot ever be more majestic trees than coloured squares, which is $5 N^{2}$.\n\nNext, we prove the gardener may ensure there are $5 N^{2}$ majestic trees. In fact, we prove this statement in a modified game which is more difficult for the gardener: on the lumberjack's turn in the modified game, he may decrement the height of all trees on the board except those the gardener did not just grow, in addition to four of the trees the gardener just grew. Clearly, a sequence of moves for the gardener which ensures that there are $K$ majestic trees in the modified game also ensures this in the original game.\n\n\n\nLet $M=\\left(\\begin{array}{l}9 \\\\ 5\\end{array}\\right)$; we say that a $m a p$ is one of the $M$ possible ways to mark 5 squares on a $3 \\times 3$ board. In the modified game, after the gardener chooses a $3 \\times 3$ subboard on the board, the lumberjack chooses a map in this subboard, and the total result of the two moves is that each tree marked on the map increases its height by 1, each tree in the subboard which is not in the map remains unchanged, and each tree outside the subboard decreases its height by 1 . Also note that if the gardener chooses a $3 \\times 3$ subboard $M l$ times, the lumberjack will have to choose some map at least $l$ times, so there will be at least 5 trees which each have height $\\geqslant l$.\n\nThe strategy for the gardener will be to divide the board into $N^{2}$ disjoint $3 \\times 3$ subboards, number them $0, \\ldots, N^{2}-1$ in some order. Then, for $b=N^{2}-1, \\ldots, 0$ in order, he plays $10^{6} M(M+1)^{b}$ times on subboard number $b$. Hence, on subboard number $b$, the moves on that subboard will first ensure 5 of its trees grows by at least $10^{6}(M+1)^{b}$, and then each move after that will decrease their heights by 1 . (As the trees on subboard $b$ had height 0 before the gardener started playing there, no move made on subboards $\\geqslant b$ decreased their heights.) As the gardener makes $10^{6} M(M+1)^{b-1}+\\ldots=10^{6}\\left((M+1)^{b}-1\\right)$ moves after he finishes playing on subboard $b$, this means that on subboard $b$, there will be 5 trees of height at least $10^{6}(M+1)^{b}-10^{6}\\left((M+1)^{b}-1\\right)=10^{6}$, hence each of the subboard has 5 majestic trees, which was what we wanted."], 'final_answer': ['2271380'], 'is_multiple_answer': False, 'unit': None, 'answer_type': 'Numerical', 'error': None}

  0%|          | 0/84 [00:00<?, ?it/s][A
 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 37/84 [00:00<00:00, 347.96it/s][A
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 77/84 [00:00<00:00, 375.53it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84/84 [00:00<00:00, 366.73it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/672 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/672 [00:00<05:42,  1.96it/s, est. speed input: 599.58 toks/s, output: 5.88 toks/s][A
Processed prompts:   3%|‚ñé         | 17/672 [00:04<03:04,  3.55it/s, est. speed input: 741.46 toks/s, output: 53.99 toks/s][A
Processed prompts:   4%|‚ñé         | 25/672 [00:07<03:28,  3.10it/s, est. speed input: 547.85 toks/s, output: 265.64 toks/s][A
Processed prompts:   5%|‚ñç         | 33/672 [00:08<02:32,  4.18it/s, est. speed input: 620.38 toks/s, output: 559.16 toks/s][A
Processed prompts:   6%|‚ñå         | 41/672 [00:09<02:00,  5.22it/s, est. speed input: 688.30 toks/s, output: 826.84 toks/s][A
Processed prompts:   7%|‚ñã         | 49/672 [00:09<01:30,  6.86it/s, est. speed input: 823.55 toks/s, output: 1117.61 toks/s][A
Processed prompts:   8%|‚ñä         | 57/672 [00:10<01:07,  9.09it/s, est. speed input: 900.79 toks/s, output: 1415.34 toks/s][A
Processed prompts:   9%|‚ñâ         | 61/672 [00:14<03:09,  3.23it/s, est. speed input: 667.35 toks/s, output: 1092.98 toks/s][A
Processed prompts:  10%|‚ñà         | 69/672 [00:15<02:15,  4.43it/s, est. speed input: 762.51 toks/s, output: 1369.00 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 77/672 [00:16<01:53,  5.25it/s, est. speed input: 819.09 toks/s, output: 1594.16 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 93/672 [00:17<01:14,  7.78it/s, est. speed input: 914.35 toks/s, output: 2106.43 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 100/672 [00:17<00:59,  9.60it/s, est. speed input: 982.15 toks/s, output: 2361.80 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 108/672 [00:17<00:46, 12.18it/s, est. speed input: 1026.65 toks/s, output: 2648.72 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 112/672 [00:18<00:51, 10.80it/s, est. speed input: 1032.44 toks/s, output: 2718.70 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 120/672 [00:18<00:46, 11.92it/s, est. speed input: 1064.70 toks/s, output: 2949.70 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 128/672 [00:19<00:38, 14.20it/s, est. speed input: 1088.97 toks/s, output: 3207.57 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 136/672 [00:19<00:41, 12.78it/s, est. speed input: 1128.45 toks/s, output: 3385.16 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 152/672 [00:21<00:42, 12.35it/s, est. speed input: 1192.00 toks/s, output: 3629.43 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 154/672 [00:22<00:54,  9.52it/s, est. speed input: 1155.75 toks/s, output: 3512.36 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 160/672 [00:22<00:51,  9.86it/s, est. speed input: 1145.84 toks/s, output: 3462.73 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 162/672 [00:24<01:29,  5.68it/s, est. speed input: 1076.28 toks/s, output: 3254.50 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 165/672 [00:24<01:18,  6.48it/s, est. speed input: 1077.43 toks/s, output: 3253.23 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 177/672 [00:24<00:41, 11.92it/s, est. speed input: 1099.80 toks/s, output: 3528.09 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 181/672 [00:24<00:37, 13.13it/s, est. speed input: 1101.82 toks/s, output: 3509.92 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 188/672 [00:24<00:28, 17.24it/s, est. speed input: 1117.17 toks/s, output: 3534.65 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñä       | 192/672 [00:25<00:27, 17.21it/s, est. speed input: 1119.52 toks/s, output: 3543.18 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 195/672 [00:27<01:15,  6.30it/s, est. speed input: 1056.04 toks/s, output: 3349.29 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 197/672 [00:28<02:00,  3.94it/s, est. speed input: 1004.58 toks/s, output: 3213.96 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 208/672 [00:28<00:59,  7.85it/s, est. speed input: 1037.81 toks/s, output: 3498.66 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 212/672 [00:29<00:53,  8.60it/s, est. speed input: 1045.08 toks/s, output: 3504.37 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 216/672 [00:29<00:46,  9.86it/s, est. speed input: 1068.06 toks/s, output: 3480.59 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 219/672 [00:30<01:08,  6.60it/s, est. speed input: 1058.04 toks/s, output: 3471.18 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 225/672 [00:31<01:06,  6.77it/s, est. speed input: 1075.75 toks/s, output: 3566.29 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 232/672 [00:31<00:44,  9.82it/s, est. speed input: 1096.00 toks/s, output: 3630.10 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 236/672 [00:31<00:37, 11.51it/s, est. speed input: 1112.63 toks/s, output: 3671.90 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 239/672 [00:32<00:53,  8.02it/s, est. speed input: 1095.88 toks/s, output: 3612.23 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 245/672 [00:34<01:37,  4.38it/s, est. speed input: 1039.52 toks/s, output: 3428.42 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 249/672 [00:35<01:38,  4.32it/s, est. speed input: 1025.12 toks/s, output: 3409.88 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 252/672 [00:36<01:38,  4.25it/s, est. speed input: 1018.76 toks/s, output: 3366.57 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 262/672 [00:36<00:53,  7.62it/s, est. speed input: 1045.68 toks/s, output: 3465.14 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 266/672 [00:37<00:52,  7.79it/s, est. speed input: 1051.13 toks/s, output: 3512.36 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 274/672 [00:38<00:59,  6.64it/s, est. speed input: 1032.87 toks/s, output: 3632.51 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 282/672 [00:39<00:43,  9.07it/s, est. speed input: 1040.51 toks/s, output: 3735.89 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 284/672 [00:39<00:46,  8.41it/s, est. speed input: 1037.23 toks/s, output: 3770.30 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 291/672 [00:40<00:42,  8.93it/s, est. speed input: 1042.33 toks/s, output: 3921.54 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 299/672 [00:42<01:03,  5.86it/s, est. speed input: 1016.71 toks/s, output: 3802.50 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 300/672 [00:42<01:05,  5.64it/s, est. speed input: 1015.50 toks/s, output: 3786.99 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 304/672 [00:42<00:53,  6.82it/s, est. speed input: 1027.37 toks/s, output: 3796.64 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 311/672 [00:43<00:41,  8.62it/s, est. speed input: 1026.61 toks/s, output: 3794.80 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 315/672 [00:43<00:42,  8.47it/s, est. speed input: 1022.44 toks/s, output: 3816.38 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 318/672 [00:44<00:44,  8.03it/s, est. speed input: 1018.59 toks/s, output: 3806.31 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 322/672 [00:44<00:34, 10.13it/s, est. speed input: 1025.66 toks/s, output: 3840.74 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 324/672 [00:44<00:38,  8.98it/s, est. speed input: 1023.39 toks/s, output: 3817.99 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 326/672 [00:44<00:34,  9.99it/s, est. speed input: 1024.01 toks/s, output: 3819.09 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 330/672 [00:45<00:32, 10.67it/s, est. speed input: 1022.57 toks/s, output: 3811.40 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 333/672 [00:45<00:31, 10.67it/s, est. speed input: 1021.07 toks/s, output: 3806.03 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 335/672 [00:45<00:30, 11.22it/s, est. speed input: 1025.31 toks/s, output: 3843.15 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 342/672 [00:45<00:20, 16.19it/s, est. speed input: 1045.78 toks/s, output: 3978.23 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 355/672 [00:46<00:15, 20.97it/s, est. speed input: 1076.60 toks/s, output: 4034.34 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 358/672 [00:46<00:18, 16.58it/s, est. speed input: 1073.99 toks/s, output: 4031.65 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 360/672 [00:48<00:44,  7.04it/s, est. speed input: 1047.01 toks/s, output: 3938.17 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 364/672 [00:49<01:08,  4.50it/s, est. speed input: 1016.76 toks/s, output: 3841.47 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 366/672 [00:50<01:07,  4.51it/s, est. speed input: 1014.99 toks/s, output: 3816.63 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 369/672 [00:50<00:58,  5.19it/s, est. speed input: 1024.91 toks/s, output: 3802.08 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 371/672 [00:50<00:55,  5.43it/s, est. speed input: 1025.99 toks/s, output: 3795.70 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 375/672 [00:51<00:37,  7.84it/s, est. speed input: 1031.02 toks/s, output: 3830.20 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 380/672 [00:52<00:52,  5.53it/s, est. speed input: 1016.27 toks/s, output: 3763.34 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 382/672 [00:52<00:50,  5.71it/s, est. speed input: 1013.35 toks/s, output: 3751.78 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 385/672 [00:52<00:43,  6.65it/s, est. speed input: 1012.59 toks/s, output: 3747.85 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 387/672 [00:53<00:49,  5.74it/s, est. speed input: 1005.69 toks/s, output: 3751.58 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 390/672 [00:54<01:02,  4.52it/s, est. speed input: 992.88 toks/s, output: 3733.41 toks/s] [A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 393/672 [00:56<01:54,  2.44it/s, est. speed input: 954.63 toks/s, output: 3594.75 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 394/672 [00:57<01:55,  2.41it/s, est. speed input: 948.39 toks/s, output: 3574.70 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 395/672 [00:58<02:30,  1.83it/s, est. speed input: 929.35 toks/s, output: 3506.61 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 401/672 [01:42<19:45,  4.37s/it, est. speed input: 538.11 toks/s, output: 2065.36 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 424/672 [01:42<04:25,  1.07s/it, est. speed input: 571.19 toks/s, output: 2748.35 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 484/672 [01:42<00:53,  3.53it/s, est. speed input: 656.67 toks/s, output: 4535.53 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 493/672 [01:45<00:50,  3.54it/s, est. speed input: 654.45 toks/s, output: 4577.75 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 500/672 [01:48<00:51,  3.32it/s, est. speed input: 648.38 toks/s, output: 4487.97 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 506/672 [01:48<00:43,  3.82it/s, est. speed input: 656.95 toks/s, output: 4512.97 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 511/672 [01:49<00:39,  4.05it/s, est. speed input: 658.77 toks/s, output: 4528.44 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 515/672 [01:49<00:34,  4.53it/s, est. speed input: 660.72 toks/s, output: 4538.74 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 521/672 [01:49<00:27,  5.54it/s, est. speed input: 663.73 toks/s, output: 4544.47 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 524/672 [01:50<00:31,  4.63it/s, est. speed input: 658.79 toks/s, output: 4509.56 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 526/672 [01:52<00:39,  3.68it/s, est. speed input: 651.86 toks/s, output: 4463.60 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 528/672 [01:52<00:38,  3.74it/s, est. speed input: 651.73 toks/s, output: 4499.08 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 530/672 [01:52<00:32,  4.31it/s, est. speed input: 652.92 toks/s, output: 4526.82 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 532/672 [01:53<00:28,  4.94it/s, est. speed input: 654.61 toks/s, output: 4574.55 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 535/672 [01:53<00:21,  6.49it/s, est. speed input: 657.88 toks/s, output: 4651.24 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 538/672 [01:53<00:16,  8.36it/s, est. speed input: 662.37 toks/s, output: 4666.25 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 540/672 [01:55<00:42,  3.13it/s, est. speed input: 651.71 toks/s, output: 4595.47 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 542/672 [01:56<00:55,  2.32it/s, est. speed input: 644.90 toks/s, output: 4544.89 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 544/672 [01:57<00:43,  2.92it/s, est. speed input: 647.03 toks/s, output: 4563.35 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 546/672 [01:57<00:33,  3.73it/s, est. speed input: 647.40 toks/s, output: 4570.50 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 549/672 [01:57<00:24,  5.09it/s, est. speed input: 649.94 toks/s, output: 4580.33 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 553/672 [01:57<00:15,  7.87it/s, est. speed input: 653.40 toks/s, output: 4605.91 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 557/672 [01:57<00:11,  9.84it/s, est. speed input: 656.49 toks/s, output: 4624.46 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 559/672 [01:57<00:11,  9.99it/s, est. speed input: 658.14 toks/s, output: 4627.84 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 562/672 [02:00<00:39,  2.81it/s, est. speed input: 647.45 toks/s, output: 4539.27 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 564/672 [02:00<00:32,  3.31it/s, est. speed input: 649.92 toks/s, output: 4544.79 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 566/672 [02:01<00:27,  3.92it/s, est. speed input: 650.26 toks/s, output: 4587.36 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 568/672 [02:01<00:21,  4.93it/s, est. speed input: 651.19 toks/s, output: 4633.97 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 570/672 [02:01<00:16,  6.16it/s, est. speed input: 652.12 toks/s, output: 4680.50 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 572/672 [02:01<00:13,  7.43it/s, est. speed input: 653.79 toks/s, output: 4709.97 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 574/672 [02:01<00:11,  8.32it/s, est. speed input: 655.01 toks/s, output: 4735.37 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 582/672 [02:03<00:18,  4.89it/s, est. speed input: 654.25 toks/s, output: 4709.82 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 590/672 [02:05<00:16,  4.84it/s, est. speed input: 651.37 toks/s, output: 4706.01 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 591/672 [02:05<00:16,  4.88it/s, est. speed input: 651.79 toks/s, output: 4723.44 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 595/672 [02:05<00:11,  6.59it/s, est. speed input: 656.70 toks/s, output: 4816.47 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 598/672 [02:05<00:09,  7.94it/s, est. speed input: 659.67 toks/s, output: 4868.65 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 600/672 [02:06<00:08,  8.04it/s, est. speed input: 660.74 toks/s, output: 4892.43 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 604/672 [02:12<00:42,  1.58it/s, est. speed input: 633.51 toks/s, output: 4698.38 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 606/672 [02:12<00:34,  1.90it/s, est. speed input: 633.83 toks/s, output: 4737.36 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 609/672 [02:15<00:39,  1.59it/s, est. speed input: 624.67 toks/s, output: 4703.84 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 611/672 [02:15<00:32,  1.85it/s, est. speed input: 625.05 toks/s, output: 4721.71 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 615/672 [02:17<00:28,  1.99it/s, est. speed input: 620.25 toks/s, output: 4750.08 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 619/672 [02:17<00:17,  3.00it/s, est. speed input: 622.26 toks/s, output: 4835.43 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 621/672 [02:17<00:14,  3.61it/s, est. speed input: 623.02 toks/s, output: 4876.22 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 623/672 [02:19<00:22,  2.20it/s, est. speed input: 614.67 toks/s, output: 4845.92 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 625/672 [02:22<00:34,  1.37it/s, est. speed input: 603.01 toks/s, output: 4781.09 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 628/672 [02:24<00:27,  1.57it/s, est. speed input: 600.16 toks/s, output: 4799.77 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 631/672 [02:27<00:30,  1.34it/s, est. speed input: 591.42 toks/s, output: 4766.74 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 633/672 [02:28<00:27,  1.41it/s, est. speed input: 589.25 toks/s, output: 4766.21 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 636/672 [02:28<00:17,  2.07it/s, est. speed input: 592.37 toks/s, output: 4824.25 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 638/672 [02:28<00:13,  2.50it/s, est. speed input: 593.67 toks/s, output: 4856.55 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 639/672 [02:29<00:15,  2.06it/s, est. speed input: 591.11 toks/s, output: 4845.62 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 643/672 [02:33<00:21,  1.35it/s, est. speed input: 579.51 toks/s, output: 4794.01 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 645/672 [02:34<00:16,  1.63it/s, est. speed input: 579.24 toks/s, output: 4820.25 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 650/672 [02:35<00:09,  2.21it/s, est. speed input: 577.44 toks/s, output: 4876.58 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 656/672 [02:36<00:04,  3.25it/s, est. speed input: 579.22 toks/s, output: 4970.81 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 657/672 [02:37<00:05,  2.77it/s, est. speed input: 577.29 toks/s, output: 4963.16 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 660/672 [02:38<00:03,  3.13it/s, est. speed input: 577.91 toks/s, output: 5000.89 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 662/672 [02:38<00:02,  3.70it/s, est. speed input: 578.30 toks/s, output: 5033.67 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 664/672 [02:38<00:01,  4.51it/s, est. speed input: 578.91 toks/s, output: 5068.39 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 667/672 [02:38<00:01,  4.86it/s, est. speed input: 578.63 toks/s, output: 5109.68 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 668/672 [02:39<00:01,  3.07it/s, est. speed input: 576.15 toks/s, output: 5094.47 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 672/672 [02:40<00:00,  4.20it/s, est. speed input: 580.25 toks/s, output: 5169.62 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/672 [00:00<?, ?it/s][A
Evaluate:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 623/672 [00:00<00:00, 6225.67it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 672/672 [00:00<00:00, 6177.37it/s]
{'num_samples': 84, 'num_scores': 672, 'timeout_samples': 0, 'empty_samples': 5, 'acc': 9.5, 'total_acc': 10.416666666666668, 'pass_at_k_percent': {'1': 10.4, '8': 13.1}, 'pass_at_k_valid_counts': {'1': 84, '8': 84}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2/olympiadbench/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 19:51:53] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2/olympiadbench  acc=9.5 pass_at_k={'1': 10.4, '8': 13.1}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [03:56<02:06, 126.07s/ds][Info] Sharding enabled: Process 1/8 handling range [62:124]
==================================================
data: math500  ,remain samples: 62
{'idx': 62, 'problem': 'Find the smallest positive real number $C$ for which\n\\[\\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\bold{v} \\right\\| \\le C \\|\\bold{v}\\|\\]for all two-dimensional vectors $\\bold{v}.$\n\nNote that for a two-dimensional vector $\\mathbf{a},$ $\\|\\mathbf{a}\\|$ is the magnitude of $\\mathbf{a}.$', 'solution': 'Let $\\bold{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.  Then\n\\[\\|\\bold{v}\\| = \\left\\| \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right\\| = \\sqrt{x^2 + y^2},\\]and\n\\begin{align*}\n\\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\bold{v} \\right\\| &= \\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right\\| \\\\\n&= \\left\\| \\begin{pmatrix} 2x + 3y \\\\ -2y \\end{pmatrix} \\right\\| \\\\\n&= \\sqrt{(2x + 3y)^2 + (-2y)^2} \\\\\n&= \\sqrt{4x^2 + 12xy + 13y^2},\n\\end{align*}so the given inequality becomes\n\\[\\sqrt{4x^2 + 12xy + 13y^2} \\le C \\sqrt{x^2 + y^2},\\]or\n\\[\\sqrt{\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2}} \\le C.\\]Thus, we can think of $C$ as the maximum value of the expression in the left-hand side.\n\nMaximizing the expression in the left-hand side is equivalent to maximizing its square, namely\n\\[\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2}.\\]Let $k$ be a possible value of this expression, which means the equation\n\\[\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2} = k\\]has a solution in $x$ and $y$.  We can re-write this equation as\n\\[(4 - k) x^2 + 12xy + (13 - k) y^2 = 0.\\]For this quadratic expression to have a solution in $x$ and $y$, its discriminant must be nonnegative.  In other words,\n\\[12^2 - 4 (4 - k)(13 - k) \\ge 0,\\]or $4k^2 - 68k + 64 \\le 0$.  This inequality factors as $4(k - 1)(k - 16) \\le 0$.  The largest value of $k$ that satisfies this inequality is 16, so the value of $C$ we seek is $\\sqrt{16} = \\boxed{4}$.  Note that equality occurs for\n\\[\\bold{v} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\\]', 'answer': '4', 'subject': 'Precalculus', 'level': 5, 'unique_id': 'test/precalculus/675.json'}

  0%|          | 0/62 [00:00<?, ?it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 23/62 [00:00<00:00, 223.05it/s][A
 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 46/62 [00:00<00:00, 217.27it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62/62 [00:00<00:00, 220.62it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/496 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   2%|‚ñè         | 9/496 [00:01<01:21,  5.96it/s, est. speed input: 462.18 toks/s, output: 67.54 toks/s][A
Processed prompts:   3%|‚ñé         | 17/496 [00:02<00:59,  8.07it/s, est. speed input: 571.23 toks/s, output: 342.92 toks/s][A
Processed prompts:   5%|‚ñå         | 25/496 [00:02<00:43, 10.90it/s, est. speed input: 657.33 toks/s, output: 645.57 toks/s][A
Processed prompts:   7%|‚ñã         | 33/496 [00:02<00:33, 13.63it/s, est. speed input: 778.05 toks/s, output: 933.12 toks/s][A
Processed prompts:   8%|‚ñä         | 41/496 [00:03<00:29, 15.19it/s, est. speed input: 814.43 toks/s, output: 1181.02 toks/s][A
Processed prompts:  10%|‚ñâ         | 49/496 [00:03<00:23, 19.29it/s, est. speed input: 894.85 toks/s, output: 1499.26 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 59/496 [00:04<00:23, 18.64it/s, est. speed input: 953.54 toks/s, output: 1733.77 toks/s][A
Processed prompts:  14%|‚ñà‚ñé        | 67/496 [00:04<00:28, 14.79it/s, est. speed input: 908.70 toks/s, output: 1795.47 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 81/496 [00:05<00:19, 21.45it/s, est. speed input: 1050.65 toks/s, output: 2365.72 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 89/496 [00:05<00:15, 25.98it/s, est. speed input: 1197.84 toks/s, output: 2692.14 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 97/496 [00:06<00:19, 20.00it/s, est. speed input: 1143.18 toks/s, output: 2752.68 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 105/496 [00:06<00:17, 22.25it/s, est. speed input: 1168.53 toks/s, output: 3010.46 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 121/496 [00:06<00:13, 26.81it/s, est. speed input: 1262.23 toks/s, output: 3532.75 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 129/496 [00:06<00:11, 31.78it/s, est. speed input: 1315.73 toks/s, output: 3852.87 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 137/496 [00:06<00:10, 35.51it/s, est. speed input: 1374.81 toks/s, output: 4142.12 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 145/496 [00:07<00:10, 33.56it/s, est. speed input: 1405.96 toks/s, output: 4350.81 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 153/496 [00:07<00:08, 38.65it/s, est. speed input: 1475.08 toks/s, output: 4648.37 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/496 [00:07<00:09, 35.44it/s, est. speed input: 1526.50 toks/s, output: 4845.36 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 169/496 [00:08<00:14, 23.27it/s, est. speed input: 1559.02 toks/s, output: 4821.18 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 177/496 [00:08<00:17, 18.70it/s, est. speed input: 1755.37 toks/s, output: 4792.47 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 185/496 [00:09<00:16, 18.55it/s, est. speed input: 1743.02 toks/s, output: 4690.72 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 201/496 [00:09<00:13, 21.71it/s, est. speed input: 1744.29 toks/s, output: 4978.58 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 209/496 [00:11<00:26, 10.78it/s, est. speed input: 1510.58 toks/s, output: 4293.09 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 219/496 [00:12<00:22, 12.48it/s, est. speed input: 1495.14 toks/s, output: 4302.58 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 233/496 [00:12<00:16, 16.00it/s, est. speed input: 1513.79 toks/s, output: 4544.17 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/496 [00:13<00:20, 12.70it/s, est. speed input: 1431.55 toks/s, output: 4284.75 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 249/496 [00:14<00:15, 15.53it/s, est. speed input: 1447.02 toks/s, output: 4347.98 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 257/496 [00:14<00:15, 15.09it/s, est. speed input: 1423.85 toks/s, output: 4310.94 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 265/496 [00:15<00:15, 14.97it/s, est. speed input: 1428.10 toks/s, output: 4303.99 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 268/496 [00:15<00:17, 13.07it/s, est. speed input: 1399.85 toks/s, output: 4209.84 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 273/496 [00:15<00:15, 13.99it/s, est. speed input: 1401.84 toks/s, output: 4183.28 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 281/496 [00:16<00:14, 15.16it/s, est. speed input: 1449.72 toks/s, output: 4193.85 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 289/496 [00:17<00:15, 13.38it/s, est. speed input: 1428.36 toks/s, output: 4212.82 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 297/496 [00:17<00:12, 15.75it/s, est. speed input: 1446.97 toks/s, output: 4468.68 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 305/496 [00:18<00:15, 12.54it/s, est. speed input: 1432.75 toks/s, output: 4510.27 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 320/496 [00:18<00:10, 16.34it/s, est. speed input: 1504.49 toks/s, output: 4944.46 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 328/496 [00:20<00:14, 11.69it/s, est. speed input: 1492.65 toks/s, output: 4882.52 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 336/496 [00:20<00:13, 11.81it/s, est. speed input: 1471.71 toks/s, output: 4913.56 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 352/496 [00:20<00:07, 19.16it/s, est. speed input: 1545.59 toks/s, output: 5375.31 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 360/496 [00:21<00:06, 22.50it/s, est. speed input: 1568.54 toks/s, output: 5474.41 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 368/496 [00:22<00:10, 12.77it/s, est. speed input: 1507.24 toks/s, output: 5269.48 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 382/496 [00:22<00:06, 16.85it/s, est. speed input: 1587.62 toks/s, output: 5630.50 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 398/496 [00:27<00:13,  7.12it/s, est. speed input: 1455.80 toks/s, output: 5078.82 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 405/496 [00:42<00:12,  7.12it/s, est. speed input: 1494.64 toks/s, output: 5310.35 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 406/496 [00:49<01:04,  1.41it/s, est. speed input: 826.44 toks/s, output: 2961.84 toks/s] [A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 414/496 [01:16<01:49,  1.33s/it, est. speed input: 553.59 toks/s, output: 2133.39 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 461/496 [01:16<00:15,  2.23it/s, est. speed input: 614.36 toks/s, output: 4025.52 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 479/496 [01:18<00:06,  2.74it/s, est. speed input: 615.89 toks/s, output: 4599.57 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 490/496 [01:19<00:01,  3.18it/s, est. speed input: 629.05 toks/s, output: 4962.63 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 496/496 [01:19<00:00,  6.22it/s, est. speed input: 650.78 toks/s, output: 5193.71 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/496 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 496/496 [00:00<00:00, 11794.85it/s]
{'num_samples': 62, 'num_scores': 496, 'timeout_samples': 0, 'empty_samples': 3, 'acc': 48.4, 'total_acc': 47.37903225806452, 'pass_at_k_percent': {'1': 47.4, '8': 50.0}, 'pass_at_k_valid_counts': {'1': 62, '8': 62}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2/math500/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 19:53:15] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2/math500  acc=48.4 pass_at_k={'1': 47.4, '8': 50.0}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:18<00:00, 105.75s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:18<00:00, 106.03s/ds]
[2025-12-03 19:53:15] ‚úÖ ÂÆåÊàêÔºöB_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_100Ôºàg1+g2 Áº∫Â§±Êï∞ÊçÆÈõÜÂ∑≤Ë°•ÂÖ®Ôºâ
[2025-12-03 19:53:16] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 19:53:16 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 19:53:16 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 19:53:16 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 19:53:23 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 19:53:29 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 19:53:30 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f87bf26ab90>
INFO 12-03 19:53:50 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 19:53:50 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 19:53:50 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 19:53:51 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_200...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.72s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.29s/it]

INFO 12-03 19:53:53 [loader.py:458] Loading weights took 2.67 seconds
INFO 12-03 19:53:54 [gpu_model_runner.py:1347] Model loading took 6.0160 GiB and 2.825252 seconds
INFO 12-03 19:53:54 [kv_cache_utils.py:634] GPU KV cache size: 258,240 tokens
INFO 12-03 19:53:54 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.97x
INFO 12-03 19:53:54 [core.py:159] init engine (profile, create kv cache, warmup model) took 0.72 seconds
INFO 12-03 19:53:54 [core_client.py:439] Core engine process 0 ready.
[2025-12-03 19:53:54] ‚ÑπÔ∏è  ÂΩìÂâçÂ∑•‰ΩúËäÇÁÇπÂàÜÁâá: 1/8
[2025-12-03 19:53:54] ‚úì Ê®°ÂûãÂ∞±Áª™ÔºåÂºÄÂßãËØÑÊµã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200ÔºàÂÖ±‰∫´Âêå‰∏Ä LLMÔºå‰ªÖË°•Áº∫Êï∞ÊçÆÈõÜÔºâ
[2025-12-03 19:53:54] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1  ÂæÖËØÑÊµã=['aime25x8', 'amc23x8', 'aime24x8']  T=0.6  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 1/8 handling range [30:60]
==================================================
data: aime25x8  ,remain samples: 30
{'idx': 30, 'problem': 'Find the sum of all integer bases $b>9$ for which $17_{b}$ is a divisor of $97_{b}$.', 'answer': '70'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 474.44it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:03<12:55,  3.24s/it, est. speed input: 48.09 toks/s, output: 48.09 toks/s][A
Processed prompts:   1%|          | 2/240 [00:04<08:01,  2.02s/it, est. speed input: 57.58 toks/s, output: 82.29 toks/s][A
Processed prompts:   1%|‚ñè         | 3/240 [00:05<06:33,  1.66s/it, est. speed input: 58.19 toks/s, output: 110.87 toks/s][A
Processed prompts:   2%|‚ñè         | 4/240 [00:06<04:47,  1.22s/it, est. speed input: 76.38 toks/s, output: 147.42 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:07<03:11,  1.22it/s, est. speed input: 110.25 toks/s, output: 214.45 toks/s][A
Processed prompts:   3%|‚ñé         | 7/240 [00:07<02:26,  1.59it/s, est. speed input: 123.75 toks/s, output: 256.36 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:07<02:14,  1.72it/s, est. speed input: 129.04 toks/s, output: 286.02 toks/s][A
Processed prompts:   4%|‚ñç         | 9/240 [00:07<01:43,  2.22it/s, est. speed input: 140.18 toks/s, output: 326.79 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:08<01:33,  2.47it/s, est. speed input: 169.34 toks/s, output: 359.75 toks/s][A
Processed prompts:   5%|‚ñç         | 11/240 [00:08<01:33,  2.44it/s, est. speed input: 175.47 toks/s, output: 386.54 toks/s][A
Processed prompts:   5%|‚ñå         | 12/240 [00:08<01:19,  2.87it/s, est. speed input: 182.54 toks/s, output: 422.31 toks/s][A
Processed prompts:   6%|‚ñå         | 14/240 [00:08<00:50,  4.47it/s, est. speed input: 201.70 toks/s, output: 503.64 toks/s][A
Processed prompts:   6%|‚ñã         | 15/240 [00:09<00:53,  4.21it/s, est. speed input: 223.49 toks/s, output: 532.59 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:09<01:04,  3.48it/s, est. speed input: 225.06 toks/s, output: 552.95 toks/s][A
Processed prompts:   7%|‚ñã         | 17/240 [00:09<00:52,  4.23it/s, est. speed input: 232.73 toks/s, output: 591.36 toks/s][A
Processed prompts:   8%|‚ñä         | 19/240 [00:10<00:45,  4.87it/s, est. speed input: 249.31 toks/s, output: 658.88 toks/s][A
Processed prompts:   9%|‚ñâ         | 21/240 [00:10<00:38,  5.72it/s, est. speed input: 273.53 toks/s, output: 729.94 toks/s][A
Processed prompts:  10%|‚ñà         | 24/240 [00:10<00:26,  8.15it/s, est. speed input: 306.86 toks/s, output: 847.33 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:10<00:28,  7.58it/s, est. speed input: 325.99 toks/s, output: 877.05 toks/s][A
Processed prompts:  11%|‚ñà         | 26/240 [00:10<00:32,  6.63it/s, est. speed input: 333.58 toks/s, output: 902.57 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:11<00:34,  6.15it/s, est. speed input: 337.68 toks/s, output: 929.49 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 28/240 [00:11<00:36,  5.81it/s, est. speed input: 341.47 toks/s, output: 956.31 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:11<00:22,  9.45it/s, est. speed input: 380.54 toks/s, output: 1075.08 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 33/240 [00:11<00:18, 11.41it/s, est. speed input: 423.79 toks/s, output: 1152.21 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 37/240 [00:11<00:14, 13.79it/s, est. speed input: 481.37 toks/s, output: 1301.55 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 42/240 [00:12<00:13, 14.49it/s, est. speed input: 509.26 toks/s, output: 1478.84 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 44/240 [00:12<00:15, 12.40it/s, est. speed input: 528.23 toks/s, output: 1534.96 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 46/240 [00:12<00:15, 12.61it/s, est. speed input: 540.13 toks/s, output: 1602.45 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 49/240 [00:12<00:18, 10.06it/s, est. speed input: 569.57 toks/s, output: 1676.83 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 51/240 [00:13<00:24,  7.73it/s, est. speed input: 566.96 toks/s, output: 1705.76 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 52/240 [00:13<00:24,  7.77it/s, est. speed input: 570.09 toks/s, output: 1732.75 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 53/240 [00:13<00:29,  6.32it/s, est. speed input: 566.82 toks/s, output: 1737.73 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 55/240 [00:14<00:28,  6.41it/s, est. speed input: 574.85 toks/s, output: 1785.31 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 56/240 [00:14<00:27,  6.66it/s, est. speed input: 587.83 toks/s, output: 1812.23 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 58/240 [00:14<00:20,  8.70it/s, est. speed input: 598.89 toks/s, output: 1884.88 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 61/240 [00:14<00:17, 10.25it/s, est. speed input: 614.35 toks/s, output: 1982.91 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 63/240 [00:14<00:19,  8.87it/s, est. speed input: 629.10 toks/s, output: 2027.79 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 65/240 [00:14<00:18,  9.49it/s, est. speed input: 649.40 toks/s, output: 2089.10 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 67/240 [00:15<00:19,  9.00it/s, est. speed input: 652.29 toks/s, output: 2139.86 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 69/240 [00:15<00:17,  9.61it/s, est. speed input: 656.85 toks/s, output: 2200.14 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 71/240 [00:15<00:25,  6.61it/s, est. speed input: 662.12 toks/s, output: 2211.64 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 73/240 [00:16<00:20,  8.00it/s, est. speed input: 673.16 toks/s, output: 2278.96 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà‚ñè      | 75/240 [00:16<00:28,  5.75it/s, est. speed input: 664.95 toks/s, output: 2283.90 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 76/240 [00:16<00:30,  5.45it/s, est. speed input: 661.73 toks/s, output: 2295.40 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:16<00:17,  9.34it/s, est. speed input: 682.98 toks/s, output: 2447.20 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 83/240 [00:17<00:15,  9.86it/s, est. speed input: 701.30 toks/s, output: 2534.42 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 85/240 [00:17<00:16,  9.36it/s, est. speed input: 715.02 toks/s, output: 2582.71 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 87/240 [00:17<00:16,  9.27it/s, est. speed input: 729.03 toks/s, output: 2634.73 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 89/240 [00:18<00:18,  7.96it/s, est. speed input: 725.30 toks/s, output: 2668.60 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 92/240 [00:18<00:14, 10.26it/s, est. speed input: 754.31 toks/s, output: 2773.26 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 94/240 [00:18<00:13, 10.59it/s, est. speed input: 764.47 toks/s, output: 2831.58 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 96/240 [00:18<00:13, 10.50it/s, est. speed input: 765.77 toks/s, output: 2885.96 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:18<00:13, 10.43it/s, est. speed input: 777.15 toks/s, output: 2940.09 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 100/240 [00:19<00:16,  8.54it/s, est. speed input: 784.32 toks/s, output: 2971.59 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/240 [00:19<00:14,  9.27it/s, est. speed input: 795.30 toks/s, output: 3029.38 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 104/240 [00:19<00:20,  6.80it/s, est. speed input: 794.78 toks/s, output: 3038.74 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/240 [00:19<00:19,  7.01it/s, est. speed input: 803.80 toks/s, output: 3062.28 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 106/240 [00:20<00:21,  6.18it/s, est. speed input: 801.05 toks/s, output: 3067.95 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 108/240 [00:20<00:17,  7.48it/s, est. speed input: 807.83 toks/s, output: 3126.62 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 110/240 [00:20<00:14,  8.88it/s, est. speed input: 822.66 toks/s, output: 3188.74 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 112/240 [00:20<00:18,  6.84it/s, est. speed input: 826.51 toks/s, output: 3206.57 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 113/240 [00:21<00:21,  5.84it/s, est. speed input: 821.26 toks/s, output: 3206.55 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 114/240 [00:21<00:22,  5.52it/s, est. speed input: 819.50 toks/s, output: 3216.10 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 118/240 [00:21<00:12,  9.49it/s, est. speed input: 845.77 toks/s, output: 3358.52 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/240 [00:21<00:17,  7.04it/s, est. speed input: 839.93 toks/s, output: 3370.38 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:22<00:14,  8.35it/s, est. speed input: 846.84 toks/s, output: 3435.31 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:22<00:24,  4.76it/s, est. speed input: 825.76 toks/s, output: 3389.82 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:23<00:27,  4.15it/s, est. speed input: 822.54 toks/s, output: 3375.73 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 127/240 [00:23<00:24,  4.64it/s, est. speed input: 826.44 toks/s, output: 3413.95 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:24<00:36,  3.05it/s, est. speed input: 804.46 toks/s, output: 3344.62 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:24<00:39,  2.81it/s, est. speed input: 799.04 toks/s, output: 3325.88 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 130/240 [00:25<00:35,  3.14it/s, est. speed input: 803.82 toks/s, output: 3342.55 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 131/240 [00:25<00:35,  3.06it/s, est. speed input: 797.23 toks/s, output: 3339.52 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 132/240 [00:26<01:07,  1.60it/s, est. speed input: 759.25 toks/s, output: 3202.11 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 133/240 [00:27<01:03,  1.69it/s, est. speed input: 749.86 toks/s, output: 3186.28 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:27<00:43,  2.42it/s, est. speed input: 750.35 toks/s, output: 3229.23 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 136/240 [00:28<00:37,  2.77it/s, est. speed input: 752.72 toks/s, output: 3249.57 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:28<00:28,  3.62it/s, est. speed input: 758.11 toks/s, output: 3300.30 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:29<00:40,  2.50it/s, est. speed input: 739.59 toks/s, output: 3249.85 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 142/240 [00:29<00:28,  3.43it/s, est. speed input: 740.57 toks/s, output: 3318.50 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:30<00:32,  2.94it/s, est. speed input: 731.49 toks/s, output: 3301.94 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:31<00:58,  1.64it/s, est. speed input: 696.96 toks/s, output: 3178.85 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:32<01:00,  1.56it/s, est. speed input: 684.92 toks/s, output: 3149.00 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 146/240 [00:34<01:36,  1.03s/it, est. speed input: 646.26 toks/s, output: 2993.63 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [00:36<01:42,  1.11s/it, est. speed input: 624.45 toks/s, output: 2927.56 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 148/240 [00:36<01:18,  1.17it/s, est. speed input: 625.06 toks/s, output: 2955.42 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:38<01:43,  1.14s/it, est. speed input: 598.53 toks/s, output: 2853.57 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [00:38<01:22,  1.10it/s, est. speed input: 596.59 toks/s, output: 2871.60 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [00:39<01:10,  1.26it/s, est. speed input: 591.68 toks/s, output: 2877.11 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:39<00:58,  1.51it/s, est. speed input: 589.86 toks/s, output: 2895.31 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 154/240 [00:40<00:53,  1.62it/s, est. speed input: 578.38 toks/s, output: 2899.64 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155/240 [00:40<00:47,  1.80it/s, est. speed input: 577.02 toks/s, output: 2916.43 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [00:41<00:40,  2.07it/s, est. speed input: 576.28 toks/s, output: 2940.14 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 157/240 [00:42<01:04,  1.28it/s, est. speed input: 560.35 toks/s, output: 2873.87 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 158/240 [00:46<02:03,  1.51s/it, est. speed input: 524.64 toks/s, output: 2704.75 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 159/240 [00:59<06:28,  4.80s/it, est. speed input: 411.64 toks/s, output: 2149.72 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 160/240 [01:17<11:40,  8.75s/it, est. speed input: 314.53 toks/s, output: 1677.75 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 209/240 [01:17<00:12,  2.41it/s, est. speed input: 396.30 toks/s, output: 3611.19 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:17<00:00,  3.09it/s, est. speed input: 465.72 toks/s, output: 4835.79 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 22633.68it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 0.0, 'total_acc': 0.0, 'pass_at_k_percent': {'1': 0.0, '8': 0.0}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime25x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 19:55:13] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime25x8  acc=0.0 pass_at_k={'1': 0.0, '8': 0.0}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:18<02:37, 78.64s/ds][Info] Sharding enabled: Process 1/8 handling range [40:80]
==================================================
data: amc23x8  ,remain samples: 40
{'idx': 40, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}

  0%|          | 0/40 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:00<00:00, 462.80it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/320 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 2/320 [00:02<05:52,  1.11s/it, est. speed input: 83.10 toks/s, output: 49.68 toks/s][A
Processed prompts:   1%|          | 3/320 [00:03<05:37,  1.07s/it, est. speed input: 93.70 toks/s, output: 81.91 toks/s][A
Processed prompts:   2%|‚ñè         | 5/320 [00:03<03:23,  1.55it/s, est. speed input: 110.82 toks/s, output: 157.82 toks/s][A
Processed prompts:   2%|‚ñè         | 6/320 [00:03<02:40,  1.96it/s, est. speed input: 149.80 toks/s, output: 198.38 toks/s][A
Processed prompts:   2%|‚ñè         | 7/320 [00:04<02:11,  2.39it/s, est. speed input: 171.40 toks/s, output: 236.31 toks/s][A
Processed prompts:   2%|‚ñé         | 8/320 [00:04<02:10,  2.39it/s, est. speed input: 170.24 toks/s, output: 261.37 toks/s][A
Processed prompts:   3%|‚ñé         | 9/320 [00:04<01:45,  2.96it/s, est. speed input: 189.99 toks/s, output: 299.71 toks/s][A
Processed prompts:   4%|‚ñç         | 12/320 [00:05<01:19,  3.87it/s, est. speed input: 204.38 toks/s, output: 396.12 toks/s][A
Processed prompts:   4%|‚ñç         | 13/320 [00:05<01:10,  4.34it/s, est. speed input: 220.08 toks/s, output: 432.98 toks/s][A
Processed prompts:   4%|‚ñç         | 14/320 [00:05<01:08,  4.50it/s, est. speed input: 232.15 toks/s, output: 463.77 toks/s][A
Processed prompts:   5%|‚ñç         | 15/320 [00:05<01:02,  4.89it/s, est. speed input: 245.33 toks/s, output: 497.40 toks/s][A
Processed prompts:   5%|‚ñå         | 16/320 [00:05<00:54,  5.60it/s, est. speed input: 259.83 toks/s, output: 534.10 toks/s][A
Processed prompts:   6%|‚ñå         | 18/320 [00:06<00:42,  7.15it/s, est. speed input: 271.42 toks/s, output: 607.89 toks/s][A
Processed prompts:   6%|‚ñã         | 20/320 [00:06<00:35,  8.48it/s, est. speed input: 301.07 toks/s, output: 681.38 toks/s][A
Processed prompts:   7%|‚ñã         | 22/320 [00:06<00:28, 10.28it/s, est. speed input: 313.76 toks/s, output: 758.12 toks/s][A
Processed prompts:   8%|‚ñä         | 24/320 [00:06<00:35,  8.30it/s, est. speed input: 333.45 toks/s, output: 808.53 toks/s][A
Processed prompts:   8%|‚ñä         | 26/320 [00:06<00:34,  8.45it/s, est. speed input: 340.99 toks/s, output: 870.52 toks/s][A
Processed prompts:   8%|‚ñä         | 27/320 [00:07<00:38,  7.66it/s, est. speed input: 342.18 toks/s, output: 892.00 toks/s][A
Processed prompts:   9%|‚ñâ         | 28/320 [00:07<00:47,  6.16it/s, est. speed input: 345.13 toks/s, output: 901.78 toks/s][A
Processed prompts:   9%|‚ñâ         | 30/320 [00:07<00:34,  8.32it/s, est. speed input: 363.67 toks/s, output: 977.42 toks/s][A
Processed prompts:  10%|‚ñà         | 32/320 [00:07<00:38,  7.43it/s, est. speed input: 371.32 toks/s, output: 1023.20 toks/s][A
Processed prompts:  11%|‚ñà         | 34/320 [00:07<00:31,  9.01it/s, est. speed input: 382.89 toks/s, output: 1093.20 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 36/320 [00:08<00:35,  8.09it/s, est. speed input: 391.89 toks/s, output: 1138.55 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 38/320 [00:08<00:28,  9.92it/s, est. speed input: 414.35 toks/s, output: 1210.12 toks/s][A
Processed prompts:  12%|‚ñà‚ñé        | 40/320 [00:08<00:43,  6.43it/s, est. speed input: 413.87 toks/s, output: 1218.16 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 42/320 [00:09<00:36,  7.61it/s, est. speed input: 434.23 toks/s, output: 1239.67 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 44/320 [00:09<00:30,  8.98it/s, est. speed input: 456.72 toks/s, output: 1264.14 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 46/320 [00:09<00:26, 10.26it/s, est. speed input: 469.67 toks/s, output: 1247.00 toks/s][A
Processed prompts:  15%|‚ñà‚ñå        | 49/320 [00:09<00:21, 12.53it/s, est. speed input: 496.11 toks/s, output: 1309.75 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 54/320 [00:09<00:14, 17.99it/s, est. speed input: 544.69 toks/s, output: 1414.60 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 57/320 [00:09<00:13, 20.10it/s, est. speed input: 580.20 toks/s, output: 1523.15 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 60/320 [00:10<00:18, 14.36it/s, est. speed input: 590.26 toks/s, output: 1592.84 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 62/320 [00:10<00:18, 13.94it/s, est. speed input: 605.68 toks/s, output: 1650.20 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 64/320 [00:10<00:17, 14.81it/s, est. speed input: 621.24 toks/s, output: 1714.60 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 66/320 [00:10<00:20, 12.26it/s, est. speed input: 625.14 toks/s, output: 1729.60 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 70/320 [00:10<00:14, 17.22it/s, est. speed input: 669.74 toks/s, output: 1875.76 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 73/320 [00:10<00:16, 15.04it/s, est. speed input: 688.49 toks/s, output: 1952.89 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 75/320 [00:11<00:27,  8.85it/s, est. speed input: 671.60 toks/s, output: 1941.62 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 77/320 [00:11<00:25,  9.60it/s, est. speed input: 687.01 toks/s, output: 1996.56 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 81/320 [00:12<00:24,  9.64it/s, est. speed input: 703.93 toks/s, output: 2086.95 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 85/320 [00:12<00:20, 11.53it/s, est. speed input: 720.73 toks/s, output: 2183.96 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 87/320 [00:12<00:19, 12.18it/s, est. speed input: 732.10 toks/s, output: 2218.33 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 89/320 [00:12<00:18, 12.81it/s, est. speed input: 740.69 toks/s, output: 2276.29 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 91/320 [00:12<00:17, 12.86it/s, est. speed input: 751.19 toks/s, output: 2308.33 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 93/320 [00:12<00:20, 10.96it/s, est. speed input: 757.74 toks/s, output: 2342.86 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 95/320 [00:13<00:22, 10.21it/s, est. speed input: 765.96 toks/s, output: 2382.11 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 97/320 [00:13<00:19, 11.28it/s, est. speed input: 774.66 toks/s, output: 2439.59 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 99/320 [00:13<00:18, 12.24it/s, est. speed input: 784.17 toks/s, output: 2496.95 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 102/320 [00:13<00:13, 15.73it/s, est. speed input: 793.86 toks/s, output: 2578.22 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 105/320 [00:13<00:13, 15.42it/s, est. speed input: 798.87 toks/s, output: 2660.75 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 108/320 [00:13<00:12, 17.39it/s, est. speed input: 819.68 toks/s, output: 2756.71 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 114/320 [00:14<00:09, 21.92it/s, est. speed input: 855.96 toks/s, output: 2921.11 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 117/320 [00:14<00:10, 19.66it/s, est. speed input: 871.10 toks/s, output: 2979.52 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 120/320 [00:14<00:13, 14.66it/s, est. speed input: 865.63 toks/s, output: 2972.68 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 124/320 [00:14<00:11, 16.75it/s, est. speed input: 885.23 toks/s, output: 3098.49 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 126/320 [00:15<00:18, 10.56it/s, est. speed input: 870.48 toks/s, output: 3081.82 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 128/320 [00:15<00:17, 11.21it/s, est. speed input: 873.61 toks/s, output: 3133.69 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 130/320 [00:15<00:20,  9.17it/s, est. speed input: 871.24 toks/s, output: 3122.98 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 134/320 [00:15<00:16, 11.14it/s, est. speed input: 900.97 toks/s, output: 3215.87 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 136/320 [00:16<00:19,  9.25it/s, est. speed input: 905.03 toks/s, output: 3218.64 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 138/320 [00:16<00:23,  7.71it/s, est. speed input: 892.58 toks/s, output: 3205.97 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 140/320 [00:16<00:20,  8.66it/s, est. speed input: 900.01 toks/s, output: 3238.01 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 144/320 [00:17<00:17,  9.98it/s, est. speed input: 915.76 toks/s, output: 3323.98 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 146/320 [00:17<00:15, 10.96it/s, est. speed input: 925.62 toks/s, output: 3380.57 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 148/320 [00:17<00:16, 10.43it/s, est. speed input: 924.66 toks/s, output: 3394.76 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 150/320 [00:17<00:14, 11.54it/s, est. speed input: 926.39 toks/s, output: 3427.70 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 152/320 [00:17<00:14, 11.58it/s, est. speed input: 932.11 toks/s, output: 3450.17 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 154/320 [00:17<00:13, 12.12it/s, est. speed input: 943.21 toks/s, output: 3502.53 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 157/320 [00:18<00:10, 15.01it/s, est. speed input: 959.76 toks/s, output: 3562.60 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 159/320 [00:18<00:14, 11.07it/s, est. speed input: 956.51 toks/s, output: 3581.72 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 161/320 [00:18<00:15, 10.49it/s, est. speed input: 964.22 toks/s, output: 3605.83 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 163/320 [00:19<00:19,  7.92it/s, est. speed input: 953.46 toks/s, output: 3608.16 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 166/320 [00:19<00:14, 10.65it/s, est. speed input: 972.71 toks/s, output: 3688.86 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 169/320 [00:19<00:11, 12.84it/s, est. speed input: 982.97 toks/s, output: 3763.23 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 171/320 [00:19<00:16,  8.93it/s, est. speed input: 981.95 toks/s, output: 3745.48 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 173/320 [00:19<00:15,  9.56it/s, est. speed input: 988.90 toks/s, output: 3781.16 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 175/320 [00:20<00:13, 10.46it/s, est. speed input: 996.72 toks/s, output: 3834.54 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 177/320 [00:20<00:16,  8.93it/s, est. speed input: 996.26 toks/s, output: 3856.84 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 180/320 [00:20<00:11, 11.80it/s, est. speed input: 1007.27 toks/s, output: 3939.53 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 183/320 [00:20<00:09, 13.96it/s, est. speed input: 1021.32 toks/s, output: 4025.67 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 185/320 [00:20<00:13,  9.98it/s, est. speed input: 1011.11 toks/s, output: 4034.48 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 188/320 [00:21<00:13,  9.97it/s, est. speed input: 1012.96 toks/s, output: 4083.28 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 190/320 [00:21<00:18,  7.02it/s, est. speed input: 1000.28 toks/s, output: 4063.29 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 191/320 [00:21<00:17,  7.22it/s, est. speed input: 1000.36 toks/s, output: 4082.24 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 193/320 [00:22<00:14,  8.87it/s, est. speed input: 1005.33 toks/s, output: 4125.20 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 196/320 [00:22<00:12, 10.16it/s, est. speed input: 1010.00 toks/s, output: 4170.18 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 198/320 [00:22<00:20,  6.05it/s, est. speed input: 989.53 toks/s, output: 4110.70 toks/s] [A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 199/320 [00:23<00:19,  6.30it/s, est. speed input: 995.01 toks/s, output: 4121.05 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 201/320 [00:23<00:14,  8.00it/s, est. speed input: 1000.39 toks/s, output: 4184.10 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 203/320 [00:23<00:13,  8.45it/s, est. speed input: 999.34 toks/s, output: 4211.61 toks/s] [A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 205/320 [00:23<00:14,  7.90it/s, est. speed input: 997.69 toks/s, output: 4241.73 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 208/320 [00:23<00:12,  8.74it/s, est. speed input: 1002.42 toks/s, output: 4296.66 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 210/320 [00:24<00:12,  8.79it/s, est. speed input: 1002.05 toks/s, output: 4322.75 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 211/320 [00:24<00:12,  8.95it/s, est. speed input: 1004.50 toks/s, output: 4345.98 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 213/320 [00:24<00:10, 10.50it/s, est. speed input: 1007.75 toks/s, output: 4407.06 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 215/320 [00:25<00:19,  5.42it/s, est. speed input: 991.98 toks/s, output: 4348.91 toks/s] [A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 217/320 [00:25<00:15,  6.84it/s, est. speed input: 1002.35 toks/s, output: 4411.31 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 220/320 [00:25<00:13,  7.68it/s, est. speed input: 1002.62 toks/s, output: 4464.26 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 222/320 [00:26<00:14,  6.60it/s, est. speed input: 993.77 toks/s, output: 4461.06 toks/s] [A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 223/320 [00:26<00:20,  4.66it/s, est. speed input: 976.96 toks/s, output: 4413.15 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 224/320 [00:27<00:33,  2.90it/s, est. speed input: 950.44 toks/s, output: 4310.81 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 225/320 [00:27<00:32,  2.96it/s, est. speed input: 945.75 toks/s, output: 4305.12 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 226/320 [00:27<00:27,  3.37it/s, est. speed input: 943.26 toks/s, output: 4322.21 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 227/320 [00:28<00:25,  3.72it/s, est. speed input: 945.90 toks/s, output: 4336.23 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 228/320 [00:28<00:28,  3.21it/s, est. speed input: 935.41 toks/s, output: 4299.19 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 229/320 [00:28<00:28,  3.15it/s, est. speed input: 930.28 toks/s, output: 4292.56 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 231/320 [00:29<00:28,  3.10it/s, est. speed input: 920.83 toks/s, output: 4280.89 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 232/320 [00:30<00:34,  2.57it/s, est. speed input: 905.56 toks/s, output: 4238.43 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 235/320 [00:31<00:39,  2.18it/s, est. speed input: 870.01 toks/s, output: 4151.63 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 236/320 [00:32<00:37,  2.23it/s, est. speed input: 863.15 toks/s, output: 4143.06 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 238/320 [00:32<00:26,  3.04it/s, est. speed input: 867.12 toks/s, output: 4199.74 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 239/320 [00:32<00:25,  3.20it/s, est. speed input: 864.04 toks/s, output: 4199.81 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 240/320 [00:34<01:01,  1.30it/s, est. speed input: 809.15 toks/s, output: 3960.55 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 242/320 [00:35<00:49,  1.58it/s, est. speed input: 795.34 toks/s, output: 3943.25 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 243/320 [00:37<01:13,  1.05it/s, est. speed input: 754.66 toks/s, output: 3770.64 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 244/320 [00:38<01:07,  1.12it/s, est. speed input: 742.66 toks/s, output: 3745.43 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 245/320 [00:38<00:56,  1.34it/s, est. speed input: 738.42 toks/s, output: 3758.39 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 248/320 [00:40<00:47,  1.53it/s, est. speed input: 718.84 toks/s, output: 3729.14 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 249/320 [00:44<01:27,  1.23s/it, est. speed input: 665.66 toks/s, output: 3471.50 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 250/320 [00:44<01:12,  1.03s/it, est. speed input: 663.37 toks/s, output: 3491.44 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 251/320 [00:48<01:56,  1.68s/it, est. speed input: 613.43 toks/s, output: 3264.74 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 252/320 [01:02<05:33,  4.90s/it, est. speed input: 475.06 toks/s, output: 2562.11 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 253/320 [01:12<06:53,  6.17s/it, est. speed input: 412.63 toks/s, output: 2262.42 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 310/320 [01:14<00:03,  3.14it/s, est. speed input: 492.25 toks/s, output: 4511.99 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 311/320 [01:15<00:02,  3.11it/s, est. speed input: 491.01 toks/s, output: 4525.57 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 312/320 [01:15<00:02,  3.18it/s, est. speed input: 492.06 toks/s, output: 4560.04 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 313/320 [01:15<00:02,  3.22it/s, est. speed input: 491.74 toks/s, output: 4587.44 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 314/320 [01:16<00:02,  2.90it/s, est. speed input: 487.47 toks/s, output: 4573.65 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 315/320 [01:16<00:01,  3.01it/s, est. speed input: 487.81 toks/s, output: 4602.75 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 316/320 [01:17<00:01,  3.06it/s, est. speed input: 487.57 toks/s, output: 4626.26 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 317/320 [01:17<00:00,  3.24it/s, est. speed input: 487.59 toks/s, output: 4655.14 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 319/320 [01:17<00:00,  3.85it/s, est. speed input: 488.39 toks/s, output: 4719.55 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:17<00:00,  4.10it/s, est. speed input: 488.34 toks/s, output: 4749.34 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [01:17<00:00,  4.12it/s, est. speed input: 488.34 toks/s, output: 4749.34 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/320 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [00:00<00:00, 18570.94it/s]
{'num_samples': 40, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 22.5, 'total_acc': 21.875, 'pass_at_k_percent': {'1': 21.9, '8': 60.0}, 'pass_at_k_valid_counts': {'1': 40, '8': 40}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/amc23x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 19:56:32] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/amc23x8  acc=22.5 pass_at_k={'1': 21.9, '8': 60.0}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [02:37<01:18, 78.74s/ds][Info] Sharding enabled: Process 1/8 handling range [30:60]
==================================================
data: aime24x8  ,remain samples: 30
{'idx': 30, 'id': 60, 'problem': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.', 'solution': '$\\frac{9}{s} + t = 4$ in hours and $\\frac{9}{s+2} + t = 2.4$ in hours.\nSubtracting the second equation from the first, we get, \n$\\frac{9}{s} - \\frac{9}{s+2} = 1.6$\nMultiplying by $(s)(s+2)$, we get \n$9s+18-9s=18=1.6s^{2} + 3.2s$\nMultiplying by 5/2 on both sides, we get\n$0 = 4s^{2} + 8s - 45$\nFactoring gives us \n$(2s-5)(2s+9) = 0$, of which the solution we want is $s=2.5$.\nSubstituting this back to the first equation, we can find that $t = 0.4$ hours.\nLastly, $s + \\frac{1}{2} = 3$ kilometers per hour, so\n$\\frac{9}{3} + 0.4 = 3.4$ hours, or $\\framebox{204}$ minutes\n-Failure.net\nThe amount of hours spent while walking on the first travel is $\\frac{240-t}{6}$. Thus, we have the equation $(240-t)(s) = 540$, and by the same logic, the second equation yields $(144-t)(s+2) = 540$. We have $240s-st = 540$, and $288+144s-2t-st = 540$. We subtract the two equations to get $96s+2t-288 = 0$, so we have $48s+t = 144$, so $t = 144-48s$, and now we have $(96+48s)(s) = 540$. The numerator of $s$ must evenly divide 540, however, $s$ must be less than 3. We can guess that $s = 2.5$. Now, $2.5+0.5 = 3$. Taking $\\frac{9}{3} = 3$, we find that it will take three hours for the 9 kilometers to be traveled. The t minutes spent at the coffeeshop can be written as $144-48(2.5)$, so t = 24. $180 + 24 = 204$. -sepehr2010', 'answer': '204', 'url': 'https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems/Problem_1', 'question': 'Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.'}

  0%|          | 0/30 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 471.29it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/240 [00:00<00:46,  5.17it/s, est. speed input: 460.62 toks/s, output: 31.05 toks/s][A
Processed prompts:   1%|          | 2/240 [00:03<09:03,  2.29s/it, est. speed input: 72.27 toks/s, output: 47.42 toks/s] [A
Processed prompts:   1%|‚ñè         | 3/240 [00:05<07:41,  1.95s/it, est. speed input: 87.62 toks/s, output: 79.60 toks/s][A
Processed prompts:   2%|‚ñè         | 4/240 [00:07<07:03,  1.80s/it, est. speed input: 84.08 toks/s, output: 107.05 toks/s][A
Processed prompts:   2%|‚ñè         | 5/240 [00:07<04:40,  1.19s/it, est. speed input: 98.28 toks/s, output: 150.28 toks/s][A
Processed prompts:   2%|‚ñé         | 6/240 [00:07<03:44,  1.04it/s, est. speed input: 103.33 toks/s, output: 184.92 toks/s][A
Processed prompts:   3%|‚ñé         | 8/240 [00:08<02:26,  1.58it/s, est. speed input: 122.39 toks/s, output: 257.48 toks/s][A
Processed prompts:   4%|‚ñç         | 10/240 [00:08<01:31,  2.52it/s, est. speed input: 161.06 toks/s, output: 341.99 toks/s][A
Processed prompts:   5%|‚ñå         | 12/240 [00:08<01:05,  3.49it/s, est. speed input: 178.02 toks/s, output: 421.85 toks/s][A
Processed prompts:   5%|‚ñå         | 13/240 [00:08<01:10,  3.22it/s, est. speed input: 182.02 toks/s, output: 447.17 toks/s][A
Processed prompts:   7%|‚ñã         | 16/240 [00:09<00:48,  4.57it/s, est. speed input: 210.61 toks/s, output: 559.31 toks/s][A
Processed prompts:   8%|‚ñä         | 18/240 [00:09<00:41,  5.39it/s, est. speed input: 237.03 toks/s, output: 632.95 toks/s][A
Processed prompts:   8%|‚ñä         | 20/240 [00:09<00:34,  6.29it/s, est. speed input: 257.79 toks/s, output: 706.85 toks/s][A
Processed prompts:   9%|‚ñâ         | 21/240 [00:10<00:42,  5.17it/s, est. speed input: 259.96 toks/s, output: 726.13 toks/s][A
Processed prompts:  10%|‚ñà         | 24/240 [00:10<00:30,  7.05it/s, est. speed input: 283.21 toks/s, output: 839.38 toks/s][A
Processed prompts:  10%|‚ñà         | 25/240 [00:10<00:30,  6.99it/s, est. speed input: 297.87 toks/s, output: 870.83 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 27/240 [00:10<00:30,  6.89it/s, est. speed input: 336.41 toks/s, output: 932.41 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 28/240 [00:11<00:34,  6.06it/s, est. speed input: 338.49 toks/s, output: 954.63 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 29/240 [00:11<00:35,  5.99it/s, est. speed input: 343.75 toks/s, output: 983.01 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 31/240 [00:11<00:37,  5.56it/s, est. speed input: 355.95 toks/s, output: 1033.94 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 33/240 [00:11<00:29,  6.98it/s, est. speed input: 373.73 toks/s, output: 1106.40 toks/s][A
Processed prompts:  14%|‚ñà‚ñç        | 34/240 [00:11<00:31,  6.46it/s, est. speed input: 382.91 toks/s, output: 1130.85 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 38/240 [00:12<00:18, 10.84it/s, est. speed input: 409.33 toks/s, output: 1287.80 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 40/240 [00:12<00:17, 11.41it/s, est. speed input: 436.80 toks/s, output: 1357.50 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 42/240 [00:12<00:22,  8.90it/s, est. speed input: 442.05 toks/s, output: 1404.74 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 44/240 [00:12<00:19,  9.84it/s, est. speed input: 458.76 toks/s, output: 1473.43 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 46/240 [00:12<00:20,  9.53it/s, est. speed input: 471.17 toks/s, output: 1532.68 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 48/240 [00:13<00:18, 10.39it/s, est. speed input: 492.46 toks/s, output: 1600.27 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 50/240 [00:13<00:22,  8.36it/s, est. speed input: 501.44 toks/s, output: 1643.64 toks/s][A
Processed prompts:  21%|‚ñà‚ñà‚ñè       | 51/240 [00:13<00:22,  8.29it/s, est. speed input: 506.66 toks/s, output: 1671.08 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 53/240 [00:13<00:18,  9.87it/s, est. speed input: 520.31 toks/s, output: 1740.85 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 55/240 [00:13<00:17, 10.77it/s, est. speed input: 531.82 toks/s, output: 1806.80 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 57/240 [00:14<00:18, 10.11it/s, est. speed input: 544.55 toks/s, output: 1862.82 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñç       | 59/240 [00:14<00:15, 11.41it/s, est. speed input: 558.99 toks/s, output: 1931.54 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 61/240 [00:14<00:25,  6.91it/s, est. speed input: 551.66 toks/s, output: 1944.08 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 63/240 [00:14<00:23,  7.63it/s, est. speed input: 560.25 toks/s, output: 2002.47 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 66/240 [00:15<00:18,  9.20it/s, est. speed input: 589.02 toks/s, output: 2098.98 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 69/240 [00:15<00:14, 11.85it/s, est. speed input: 607.52 toks/s, output: 2208.55 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 71/240 [00:15<00:14, 11.77it/s, est. speed input: 620.19 toks/s, output: 2268.34 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 74/240 [00:15<00:14, 11.51it/s, est. speed input: 630.49 toks/s, output: 2355.41 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 76/240 [00:15<00:14, 11.52it/s, est. speed input: 637.81 toks/s, output: 2414.23 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñé      | 78/240 [00:16<00:21,  7.37it/s, est. speed input: 631.02 toks/s, output: 2419.07 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 80/240 [00:16<00:19,  8.19it/s, est. speed input: 635.46 toks/s, output: 2478.03 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñç      | 82/240 [00:16<00:20,  7.53it/s, est. speed input: 640.84 toks/s, output: 2515.25 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 85/240 [00:17<00:17,  9.05it/s, est. speed input: 654.64 toks/s, output: 2608.70 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 88/240 [00:17<00:13, 11.23it/s, est. speed input: 666.72 toks/s, output: 2712.72 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 91/240 [00:17<00:11, 13.23it/s, est. speed input: 702.74 toks/s, output: 2815.86 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 93/240 [00:17<00:14, 10.49it/s, est. speed input: 708.61 toks/s, output: 2850.20 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñâ      | 95/240 [00:18<00:14, 10.13it/s, est. speed input: 712.70 toks/s, output: 2900.03 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 97/240 [00:18<00:19,  7.31it/s, est. speed input: 712.94 toks/s, output: 2908.21 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 98/240 [00:18<00:26,  5.41it/s, est. speed input: 701.19 toks/s, output: 2883.46 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 100/240 [00:19<00:26,  5.22it/s, est. speed input: 699.70 toks/s, output: 2905.34 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 101/240 [00:19<00:28,  4.86it/s, est. speed input: 694.62 toks/s, output: 2907.67 toks/s][A
Processed prompts:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 103/240 [00:19<00:25,  5.27it/s, est. speed input: 693.29 toks/s, output: 2944.87 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/240 [00:20<00:20,  6.61it/s, est. speed input: 714.16 toks/s, output: 3007.22 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 106/240 [00:20<00:28,  4.67it/s, est. speed input: 703.95 toks/s, output: 2978.84 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 107/240 [00:21<00:38,  3.48it/s, est. speed input: 690.94 toks/s, output: 2942.36 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/240 [00:21<00:26,  4.96it/s, est. speed input: 696.87 toks/s, output: 3009.34 toks/s][A
Processed prompts:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 111/240 [00:21<00:25,  5.03it/s, est. speed input: 697.59 toks/s, output: 3038.84 toks/s][A
Processed prompts:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 114/240 [00:21<00:17,  7.00it/s, est. speed input: 712.74 toks/s, output: 3135.48 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/240 [00:22<00:14,  8.45it/s, est. speed input: 727.13 toks/s, output: 3227.23 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 119/240 [00:22<00:15,  7.86it/s, est. speed input: 726.38 toks/s, output: 3267.46 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 121/240 [00:22<00:14,  8.40it/s, est. speed input: 728.66 toks/s, output: 3323.18 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 122/240 [00:22<00:16,  7.04it/s, est. speed input: 730.60 toks/s, output: 3327.66 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 123/240 [00:23<00:26,  4.42it/s, est. speed input: 716.28 toks/s, output: 3286.82 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 124/240 [00:23<00:25,  4.55it/s, est. speed input: 717.90 toks/s, output: 3301.94 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 125/240 [00:23<00:24,  4.67it/s, est. speed input: 721.78 toks/s, output: 3317.12 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 126/240 [00:23<00:22,  5.04it/s, est. speed input: 720.75 toks/s, output: 3338.44 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 128/240 [00:24<00:25,  4.46it/s, est. speed input: 714.49 toks/s, output: 3351.43 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/240 [00:24<00:23,  4.81it/s, est. speed input: 715.22 toks/s, output: 3373.19 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 130/240 [00:24<00:20,  5.30it/s, est. speed input: 714.88 toks/s, output: 3397.91 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 131/240 [00:25<00:24,  4.46it/s, est. speed input: 710.97 toks/s, output: 3396.47 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 132/240 [00:25<00:26,  4.07it/s, est. speed input: 707.15 toks/s, output: 3398.59 toks/s][A
Processed prompts:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 133/240 [00:25<00:35,  3.02it/s, est. speed input: 697.26 toks/s, output: 3368.93 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/240 [00:26<00:36,  2.90it/s, est. speed input: 685.03 toks/s, output: 3361.89 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 136/240 [00:27<00:39,  2.63it/s, est. speed input: 677.82 toks/s, output: 3343.74 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/240 [00:27<00:36,  2.80it/s, est. speed input: 670.14 toks/s, output: 3351.09 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 139/240 [00:28<00:35,  2.86it/s, est. speed input: 667.47 toks/s, output: 3355.54 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/240 [00:28<00:25,  3.93it/s, est. speed input: 668.65 toks/s, output: 3415.84 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 143/240 [00:28<00:21,  4.53it/s, est. speed input: 681.68 toks/s, output: 3463.28 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/240 [00:29<00:31,  3.08it/s, est. speed input: 667.80 toks/s, output: 3420.72 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 145/240 [00:29<00:30,  3.07it/s, est. speed input: 666.06 toks/s, output: 3426.08 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 146/240 [00:30<00:43,  2.14it/s, est. speed input: 650.21 toks/s, output: 3368.29 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/240 [00:30<00:36,  2.55it/s, est. speed input: 651.93 toks/s, output: 3392.22 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 148/240 [00:32<01:06,  1.38it/s, est. speed input: 622.60 toks/s, output: 3265.11 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 149/240 [00:33<01:23,  1.08it/s, est. speed input: 599.73 toks/s, output: 3170.46 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/240 [00:34<01:12,  1.24it/s, est. speed input: 597.70 toks/s, output: 3167.21 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 151/240 [00:34<00:59,  1.49it/s, est. speed input: 594.20 toks/s, output: 3179.06 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 152/240 [00:44<04:52,  3.33s/it, est. speed input: 466.06 toks/s, output: 2524.04 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/240 [00:47<04:34,  3.15s/it, est. speed input: 447.81 toks/s, output: 2420.66 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 154/240 [00:50<04:35,  3.20s/it, est. speed input: 421.70 toks/s, output: 2303.83 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155/240 [00:59<07:01,  4.96s/it, est. speed input: 360.00 toks/s, output: 1992.41 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/240 [01:01<05:30,  3.93s/it, est. speed input: 353.54 toks/s, output: 1984.68 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 157/240 [01:19<11:24,  8.24s/it, est. speed input: 273.98 toks/s, output: 1564.80 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 166/240 [01:19<02:07,  1.73s/it, est. speed input: 288.70 toks/s, output: 1906.17 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 239/240 [01:19<00:00,  5.85it/s, est. speed input: 413.32 toks/s, output: 4707.71 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [01:20<00:00,  2.98it/s, est. speed input: 415.96 toks/s, output: 4717.89 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/240 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 19876.65it/s]
{'num_samples': 30, 'num_scores': 240, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 3.3, 'total_acc': 7.5, 'pass_at_k_percent': {'1': 7.5, '8': 23.3}, 'pass_at_k_valid_counts': {'1': 30, '8': 30}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime24x8/test_think-boxed_-1_seed0_t0.6_s0_e-1_part1.jsonl
[2025-12-03 19:57:53] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1/aime24x8  acc=3.3 pass_at_k={'1': 7.5, '8': 23.3}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:59<00:00, 80.07s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:59<00:00, 79.70s/ds]
[2025-12-03 19:57:53] ‚ñ∂ B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2  ÂæÖËØÑÊµã=['minerva_math', 'olympiadbench', 'math500']  T=0.0  n=8
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:   0%|          | 0/3 [00:00<?, ?ds/s][Info] Sharding enabled: Process 1/8 handling range [34:68]
==================================================
data: minerva_math  ,remain samples: 34
{'problem': 'The Hubble Space telescope has an effective diameter of $2.5 \\mathrm{~m}$, and a typical wavelength used for observation by the Hubble might be $0.6 \\mu \\mathrm{m}$, or 600 nanometers (typical optical wavelength). Based on this information, compute an estimate for the angular resolution of the Hubble Space telescope in arcseconds.', 'solution': 'Using the formula for angular resolution $\\theta$ in terms of the effective size $d$ and the wavelength $\\lambda$, namely $\\theta = \\lambda/d$, gives \\boxed{0.05} arcseconds.', 'type': 'Introduction to Astronomy (8.282J Spring 2006)', 'idx': 34}

  0%|          | 0/34 [00:00<?, ?it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:00<00:00, 13590.62it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/272 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/272 [00:02<12:52,  2.85s/it, est. speed input: 85.26 toks/s, output: 48.07 toks/s][A
Processed prompts:   3%|‚ñé         | 9/272 [00:03<01:23,  3.14it/s, est. speed input: 554.92 toks/s, output: 350.62 toks/s][A
Processed prompts:   6%|‚ñã         | 17/272 [00:04<00:52,  4.85it/s, est. speed input: 566.79 toks/s, output: 589.00 toks/s][A
Processed prompts:   9%|‚ñâ         | 25/272 [00:06<00:56,  4.33it/s, est. speed input: 544.99 toks/s, output: 678.99 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 33/272 [00:08<00:50,  4.76it/s, est. speed input: 561.51 toks/s, output: 857.52 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 49/272 [00:08<00:23,  9.55it/s, est. speed input: 766.71 toks/s, output: 1440.71 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 57/272 [00:08<00:17, 12.03it/s, est. speed input: 966.75 toks/s, output: 1762.03 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 65/272 [00:10<00:24,  8.47it/s, est. speed input: 903.30 toks/s, output: 1785.08 toks/s][A
Processed prompts:  27%|‚ñà‚ñà‚ñã       | 73/272 [00:10<00:19, 10.15it/s, est. speed input: 954.52 toks/s, output: 2067.39 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñâ       | 81/272 [00:10<00:15, 12.51it/s, est. speed input: 1059.39 toks/s, output: 2351.20 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 89/272 [00:11<00:14, 12.77it/s, est. speed input: 1079.45 toks/s, output: 2465.59 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 97/272 [00:12<00:15, 11.47it/s, est. speed input: 1072.84 toks/s, output: 2629.64 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñã      | 99/272 [00:12<00:14, 11.55it/s, est. speed input: 1078.27 toks/s, output: 2685.18 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 106/272 [00:12<00:12, 13.64it/s, est. speed input: 1122.02 toks/s, output: 2926.11 toks/s][A
Processed prompts:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 113/272 [00:13<00:10, 14.83it/s, est. speed input: 1178.08 toks/s, output: 3147.30 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 121/272 [00:13<00:07, 20.29it/s, est. speed input: 1241.46 toks/s, output: 3477.87 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 129/272 [00:13<00:07, 19.64it/s, est. speed input: 1293.89 toks/s, output: 3716.91 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 153/272 [00:14<00:04, 24.88it/s, est. speed input: 1451.53 toks/s, output: 4542.19 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 160/272 [00:16<00:08, 12.74it/s, est. speed input: 1386.95 toks/s, output: 4367.82 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 168/272 [00:16<00:06, 15.29it/s, est. speed input: 1421.70 toks/s, output: 4682.96 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 176/272 [00:17<00:08, 10.73it/s, est. speed input: 1378.13 toks/s, output: 4663.84 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 184/272 [00:18<00:07, 11.45it/s, est. speed input: 1465.33 toks/s, output: 4886.18 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 192/272 [00:19<00:09,  8.66it/s, est. speed input: 1398.83 toks/s, output: 4869.60 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 200/272 [00:20<00:07,  9.08it/s, est. speed input: 1421.96 toks/s, output: 5059.19 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 208/272 [00:22<00:10,  6.37it/s, est. speed input: 1332.62 toks/s, output: 4930.18 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 216/272 [00:25<00:11,  4.86it/s, est. speed input: 1227.00 toks/s, output: 4787.33 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 224/272 [00:58<01:04,  1.35s/it, est. speed input: 553.34 toks/s, output: 2291.21 toks/s] [AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [00:58<00:00,  4.67it/s, est. speed input: 780.32 toks/s, output: 4821.52 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/272 [00:00<?, ?it/s][A
Evaluate:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 225/272 [00:00<00:00, 668.01it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [00:00<00:00, 300.47it/s]
{'num_samples': 34, 'num_scores': 272, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 5.9, 'total_acc': 5.88235294117647, 'pass_at_k_percent': {'1': 5.9, '8': 5.9}, 'pass_at_k_valid_counts': {'1': 34, '8': 34}, 'type_acc': {'Differential Equations (18.03 Spring 2010)': 14.3, 'Ecology I (1.018J Fall 2009)': 20.0, 'Information and Entropy (6.050J Spring 2008)': 0.0, 'Introduction to Astronomy (8.282J Spring 2006)': 0.0}, 'type_pass_at_k_percent': {'Differential Equations (18.03 Spring 2010)': {'1': 14.3, '8': 14.3}, 'Ecology I (1.018J Fall 2009)': {'1': 20.0, '8': 20.0}, 'Information and Entropy (6.050J Spring 2008)': {'1': 0.0, '8': 0.0}, 'Introduction to Astronomy (8.282J Spring 2006)': {'1': 0.0, '8': 0.0}}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/minerva_math/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 19:58:53] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/minerva_math  acc=5.9 pass_at_k={'1': 5.9, '8': 5.9}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:59<01:58, 59.42s/ds][Info] Sharding enabled: Process 1/8 handling range [84:168]
==================================================
data: olympiadbench  ,remain samples: 84
{'idx': 84, 'id': 2006, 'subfield': 'Combinatorics', 'context': None, 'question': 'In each square of a garden shaped like a $2022 \\times 2022$ board, there is initially a tree of height 0 . A gardener and a lumberjack alternate turns playing the following game, with the gardener taking the first turn:\n\n- The gardener chooses a square in the garden. Each tree on that square and all the surrounding squares (of which there are at most eight) then becomes one unit taller.\n- The lumberjack then chooses four different squares on the board. Each tree of positive height on those squares then becomes one unit shorter.\n\nWe say that a tree is majestic if its height is at least $10^{6}$. Determine the largest number $K$ such that the gardener can ensure there are eventually $K$ majestic trees on the board, no matter how the lumberjack plays.', 'solution': ["We solve the problem for a general $3 N \\times 3 N$ board. First, we prove that the lumberjack has a strategy to ensure there are never more than $5 N^{2}$ majestic trees. Giving the squares of the board coordinates in the natural manner, colour each square where at least one of its coordinates are divisible by 3 , shown below for a $9 \\times 9$ board:\n\n<img_3271>\n\nThen, as each $3 \\times 3$ square on the board contains exactly 5 coloured squares, each move of the gardener will cause at most 4 trees on non-coloured squares to grow. The lumberjack may therefore cut those trees, ensuring no tree on a non-coloured square has positive height after his turn. Hence there cannot ever be more majestic trees than coloured squares, which is $5 N^{2}$.\n\nNext, we prove the gardener may ensure there are $5 N^{2}$ majestic trees. In fact, we prove this statement in a modified game which is more difficult for the gardener: on the lumberjack's turn in the modified game, he may decrement the height of all trees on the board except those the gardener did not just grow, in addition to four of the trees the gardener just grew. Clearly, a sequence of moves for the gardener which ensures that there are $K$ majestic trees in the modified game also ensures this in the original game.\n\n\n\nLet $M=\\left(\\begin{array}{l}9 \\\\ 5\\end{array}\\right)$; we say that a $m a p$ is one of the $M$ possible ways to mark 5 squares on a $3 \\times 3$ board. In the modified game, after the gardener chooses a $3 \\times 3$ subboard on the board, the lumberjack chooses a map in this subboard, and the total result of the two moves is that each tree marked on the map increases its height by 1, each tree in the subboard which is not in the map remains unchanged, and each tree outside the subboard decreases its height by 1 . Also note that if the gardener chooses a $3 \\times 3$ subboard $M l$ times, the lumberjack will have to choose some map at least $l$ times, so there will be at least 5 trees which each have height $\\geqslant l$.\n\nThe strategy for the gardener will be to divide the board into $N^{2}$ disjoint $3 \\times 3$ subboards, number them $0, \\ldots, N^{2}-1$ in some order. Then, for $b=N^{2}-1, \\ldots, 0$ in order, he plays $10^{6} M(M+1)^{b}$ times on subboard number $b$. Hence, on subboard number $b$, the moves on that subboard will first ensure 5 of its trees grows by at least $10^{6}(M+1)^{b}$, and then each move after that will decrease their heights by 1 . (As the trees on subboard $b$ had height 0 before the gardener started playing there, no move made on subboards $\\geqslant b$ decreased their heights.) As the gardener makes $10^{6} M(M+1)^{b-1}+\\ldots=10^{6}\\left((M+1)^{b}-1\\right)$ moves after he finishes playing on subboard $b$, this means that on subboard $b$, there will be 5 trees of height at least $10^{6}(M+1)^{b}-10^{6}\\left((M+1)^{b}-1\\right)=10^{6}$, hence each of the subboard has 5 majestic trees, which was what we wanted."], 'final_answer': ['2271380'], 'is_multiple_answer': False, 'unit': None, 'answer_type': 'Numerical', 'error': None}

  0%|          | 0/84 [00:00<?, ?it/s][A
 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 37/84 [00:00<00:00, 354.38it/s][A
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 78/84 [00:00<00:00, 379.83it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84/84 [00:00<00:00, 373.64it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/672 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/672 [00:00<02:44,  4.08it/s, est. speed input: 1247.93 toks/s, output: 12.23 toks/s][A
Processed prompts:   3%|‚ñé         | 17/672 [00:07<04:58,  2.19it/s, est. speed input: 482.54 toks/s, output: 50.81 toks/s][A
Processed prompts:   4%|‚ñé         | 25/672 [00:09<03:53,  2.77it/s, est. speed input: 555.19 toks/s, output: 338.76 toks/s][A
Processed prompts:   5%|‚ñç         | 32/672 [00:10<02:50,  3.76it/s, est. speed input: 617.43 toks/s, output: 609.55 toks/s][A
Processed prompts:   6%|‚ñå         | 40/672 [00:10<01:58,  5.33it/s, est. speed input: 716.41 toks/s, output: 924.29 toks/s][A
Processed prompts:   6%|‚ñã         | 43/672 [00:11<02:32,  4.12it/s, est. speed input: 666.89 toks/s, output: 917.98 toks/s][A
Processed prompts:   7%|‚ñã         | 48/672 [00:12<02:10,  4.77it/s, est. speed input: 711.24 toks/s, output: 1076.75 toks/s][A
Processed prompts:   8%|‚ñä         | 56/672 [00:13<01:56,  5.29it/s, est. speed input: 778.99 toks/s, output: 1287.79 toks/s][A
Processed prompts:  10%|‚ñâ         | 64/672 [00:14<01:20,  7.54it/s, est. speed input: 888.50 toks/s, output: 1591.90 toks/s][A
Processed prompts:  10%|‚ñâ         | 66/672 [00:14<01:14,  8.09it/s, est. speed input: 897.58 toks/s, output: 1662.45 toks/s][A
Processed prompts:  11%|‚ñà         | 73/672 [00:14<00:52, 11.44it/s, est. speed input: 943.76 toks/s, output: 1928.77 toks/s][A
Processed prompts:  12%|‚ñà‚ñè        | 81/672 [00:17<01:48,  5.44it/s, est. speed input: 858.59 toks/s, output: 1896.16 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 89/672 [00:17<01:12,  7.99it/s, est. speed input: 933.37 toks/s, output: 2203.26 toks/s][A
Processed prompts:  16%|‚ñà‚ñå        | 105/672 [00:19<01:11,  7.93it/s, est. speed input: 1035.36 toks/s, output: 2548.94 toks/s][A
Processed prompts:  17%|‚ñà‚ñã        | 113/672 [00:20<01:12,  7.75it/s, est. speed input: 1033.87 toks/s, output: 2691.89 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 120/672 [00:20<00:59,  9.32it/s, est. speed input: 1093.28 toks/s, output: 2784.50 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 123/672 [00:21<01:05,  8.42it/s, est. speed input: 1083.90 toks/s, output: 2824.87 toks/s][A
Processed prompts:  19%|‚ñà‚ñâ        | 129/672 [00:25<02:35,  3.49it/s, est. speed input: 933.43 toks/s, output: 2522.38 toks/s] [A
Processed prompts:  19%|‚ñà‚ñâ        | 131/672 [00:25<02:20,  3.84it/s, est. speed input: 938.44 toks/s, output: 2530.42 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 133/672 [00:25<02:06,  4.26it/s, est. speed input: 938.27 toks/s, output: 2515.21 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 135/672 [00:26<01:52,  4.76it/s, est. speed input: 937.52 toks/s, output: 2498.64 toks/s][A
Processed prompts:  20%|‚ñà‚ñà        | 137/672 [00:26<01:40,  5.33it/s, est. speed input: 936.30 toks/s, output: 2481.02 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 139/672 [00:27<02:44,  3.25it/s, est. speed input: 895.02 toks/s, output: 2366.99 toks/s][A
Processed prompts:  22%|‚ñà‚ñà‚ñè       | 151/672 [00:28<01:08,  7.62it/s, est. speed input: 928.98 toks/s, output: 2480.50 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 153/672 [00:29<01:43,  5.04it/s, est. speed input: 897.83 toks/s, output: 2447.03 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 161/672 [00:31<01:41,  5.01it/s, est. speed input: 881.72 toks/s, output: 2554.15 toks/s][A
Processed prompts:  25%|‚ñà‚ñà‚ñå       | 169/672 [00:31<01:06,  7.52it/s, est. speed input: 901.34 toks/s, output: 2670.65 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñã       | 177/672 [00:31<00:52,  9.47it/s, est. speed input: 913.10 toks/s, output: 2765.33 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 185/672 [00:33<01:03,  7.66it/s, est. speed input: 923.07 toks/s, output: 2920.99 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñä       | 193/672 [00:34<01:11,  6.69it/s, est. speed input: 915.98 toks/s, output: 3039.22 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 195/672 [00:35<01:26,  5.50it/s, est. speed input: 898.14 toks/s, output: 3002.32 toks/s][A
Processed prompts:  29%|‚ñà‚ñà‚ñâ       | 197/672 [00:35<01:21,  5.83it/s, est. speed input: 898.83 toks/s, output: 3026.35 toks/s][A
Processed prompts:  30%|‚ñà‚ñà‚ñà       | 202/672 [00:35<00:58,  8.04it/s, est. speed input: 906.96 toks/s, output: 3049.04 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 205/672 [00:37<01:34,  4.96it/s, est. speed input: 878.84 toks/s, output: 2999.64 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà‚ñè      | 210/672 [00:38<01:32,  5.01it/s, est. speed input: 867.71 toks/s, output: 3030.10 toks/s][A
Processed prompts:  32%|‚ñà‚ñà‚ñà‚ñè      | 218/672 [00:39<01:26,  5.27it/s, est. speed input: 852.27 toks/s, output: 3068.31 toks/s][A
Processed prompts:  34%|‚ñà‚ñà‚ñà‚ñé      | 226/672 [00:43<02:14,  3.32it/s, est. speed input: 799.74 toks/s, output: 3027.68 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñç      | 232/672 [00:43<01:37,  4.49it/s, est. speed input: 809.81 toks/s, output: 3073.38 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 240/672 [00:44<01:06,  6.53it/s, est. speed input: 817.03 toks/s, output: 3124.27 toks/s][A
Processed prompts:  36%|‚ñà‚ñà‚ñà‚ñå      | 243/672 [00:46<02:02,  3.49it/s, est. speed input: 771.63 toks/s, output: 2976.67 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 250/672 [00:48<01:56,  3.62it/s, est. speed input: 757.16 toks/s, output: 2997.60 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 258/672 [00:52<02:28,  2.78it/s, est. speed input: 723.09 toks/s, output: 2851.11 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñä      | 259/672 [00:53<02:37,  2.62it/s, est. speed input: 715.71 toks/s, output: 2824.53 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñä      | 260/672 [00:54<02:48,  2.44it/s, est. speed input: 708.27 toks/s, output: 2797.62 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 262/672 [01:42<32:36,  4.77s/it, est. speed input: 380.05 toks/s, output: 1524.65 toks/s][A
Processed prompts:  40%|‚ñà‚ñà‚ñà‚ñà      | 269/672 [01:42<16:45,  2.50s/it, est. speed input: 393.52 toks/s, output: 1730.02 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 318/672 [01:42<02:33,  2.31it/s, est. speed input: 455.32 toks/s, output: 3191.69 toks/s][A
Processed prompts:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 345/672 [01:42<01:27,  3.75it/s, est. speed input: 482.16 toks/s, output: 3989.52 toks/s][A
Processed prompts:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 353/672 [01:43<01:14,  4.26it/s, est. speed input: 490.04 toks/s, output: 4029.53 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 360/672 [01:43<01:03,  4.93it/s, est. speed input: 497.59 toks/s, output: 4082.72 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 366/672 [01:47<01:27,  3.50it/s, est. speed input: 482.82 toks/s, output: 4003.75 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 373/672 [01:48<01:10,  4.27it/s, est. speed input: 487.32 toks/s, output: 4008.09 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 377/672 [01:48<01:00,  4.86it/s, est. speed input: 494.97 toks/s, output: 4019.99 toks/s][A
Processed prompts:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 381/672 [01:48<00:53,  5.46it/s, est. speed input: 497.84 toks/s, output: 4022.08 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 388/672 [01:48<00:39,  7.26it/s, est. speed input: 501.84 toks/s, output: 4052.46 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 391/672 [01:48<00:34,  8.06it/s, est. speed input: 503.80 toks/s, output: 4131.59 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 394/672 [01:49<00:32,  8.67it/s, est. speed input: 505.39 toks/s, output: 4207.41 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 397/672 [01:49<00:29,  9.29it/s, est. speed input: 506.43 toks/s, output: 4230.34 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 403/672 [01:50<00:44,  6.08it/s, est. speed input: 502.95 toks/s, output: 4182.47 toks/s][A
Processed prompts:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 405/672 [01:52<01:04,  4.12it/s, est. speed input: 499.90 toks/s, output: 4139.28 toks/s][A
Processed prompts:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 407/672 [01:52<00:56,  4.66it/s, est. speed input: 502.25 toks/s, output: 4141.01 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 414/672 [01:52<00:36,  7.03it/s, est. speed input: 508.19 toks/s, output: 4147.38 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 416/672 [01:53<00:36,  6.93it/s, est. speed input: 508.27 toks/s, output: 4143.43 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 419/672 [01:53<00:29,  8.57it/s, est. speed input: 511.58 toks/s, output: 4152.68 toks/s][A
Processed prompts:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 421/672 [01:53<00:35,  7.05it/s, est. speed input: 510.84 toks/s, output: 4142.51 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 430/672 [01:53<00:17, 13.62it/s, est. speed input: 515.90 toks/s, output: 4168.24 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 433/672 [01:54<00:17, 13.89it/s, est. speed input: 519.57 toks/s, output: 4175.98 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 436/672 [01:55<00:33,  7.09it/s, est. speed input: 519.13 toks/s, output: 4179.38 toks/s][A
Processed prompts:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 439/672 [01:55<00:27,  8.48it/s, est. speed input: 523.57 toks/s, output: 4189.35 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 448/672 [01:55<00:17, 13.03it/s, est. speed input: 533.42 toks/s, output: 4212.36 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 451/672 [01:56<00:16, 13.06it/s, est. speed input: 535.50 toks/s, output: 4276.24 toks/s][A
Processed prompts:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 453/672 [01:56<00:16, 13.68it/s, est. speed input: 536.84 toks/s, output: 4325.14 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 455/672 [01:56<00:16, 12.94it/s, est. speed input: 537.61 toks/s, output: 4348.20 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 457/672 [01:56<00:17, 12.35it/s, est. speed input: 538.40 toks/s, output: 4379.33 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 460/672 [01:56<00:14, 14.33it/s, est. speed input: 540.02 toks/s, output: 4409.78 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 462/672 [01:56<00:14, 14.48it/s, est. speed input: 540.89 toks/s, output: 4428.38 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 465/672 [01:57<00:24,  8.42it/s, est. speed input: 539.87 toks/s, output: 4430.69 toks/s][A
Processed prompts:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 467/672 [01:57<00:21,  9.41it/s, est. speed input: 540.67 toks/s, output: 4435.40 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 469/672 [01:57<00:20, 10.05it/s, est. speed input: 542.60 toks/s, output: 4440.92 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 478/672 [01:57<00:09, 20.53it/s, est. speed input: 549.61 toks/s, output: 4473.40 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 481/672 [01:58<00:14, 13.61it/s, est. speed input: 549.37 toks/s, output: 4468.60 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 486/672 [01:58<00:14, 12.40it/s, est. speed input: 550.36 toks/s, output: 4464.47 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 488/672 [01:59<00:22,  8.03it/s, est. speed input: 548.99 toks/s, output: 4446.66 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 496/672 [02:01<00:31,  5.58it/s, est. speed input: 549.54 toks/s, output: 4435.61 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 499/672 [02:01<00:25,  6.69it/s, est. speed input: 552.32 toks/s, output: 4507.57 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 501/672 [02:01<00:23,  7.26it/s, est. speed input: 553.77 toks/s, output: 4552.20 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 503/672 [02:01<00:23,  7.08it/s, est. speed input: 553.40 toks/s, output: 4548.69 toks/s][A
Processed prompts:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 506/672 [02:02<00:30,  5.37it/s, est. speed input: 550.96 toks/s, output: 4529.00 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 508/672 [02:03<00:35,  4.67it/s, est. speed input: 550.07 toks/s, output: 4518.37 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 509/672 [02:04<00:45,  3.60it/s, est. speed input: 549.26 toks/s, output: 4499.26 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 517/672 [02:04<00:25,  6.03it/s, est. speed input: 562.59 toks/s, output: 4539.22 toks/s][A
Processed prompts:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 520/672 [02:05<00:20,  7.38it/s, est. speed input: 564.88 toks/s, output: 4593.66 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 524/672 [02:05<00:14,  9.92it/s, est. speed input: 567.70 toks/s, output: 4647.98 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 526/672 [02:05<00:13, 10.63it/s, est. speed input: 569.00 toks/s, output: 4692.25 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 528/672 [02:06<00:22,  6.50it/s, est. speed input: 567.88 toks/s, output: 4696.70 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 530/672 [02:07<00:34,  4.13it/s, est. speed input: 565.29 toks/s, output: 4668.45 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 533/672 [02:07<00:26,  5.18it/s, est. speed input: 567.75 toks/s, output: 4676.46 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 535/672 [02:07<00:25,  5.45it/s, est. speed input: 567.76 toks/s, output: 4674.08 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 536/672 [02:07<00:23,  5.70it/s, est. speed input: 567.88 toks/s, output: 4673.85 toks/s][A
Processed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 538/672 [02:09<00:43,  3.08it/s, est. speed input: 563.84 toks/s, output: 4633.62 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 543/672 [02:09<00:24,  5.19it/s, est. speed input: 568.79 toks/s, output: 4645.59 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 544/672 [02:09<00:25,  4.96it/s, est. speed input: 568.93 toks/s, output: 4641.05 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 546/672 [02:17<02:24,  1.15s/it, est. speed input: 540.28 toks/s, output: 4415.56 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 547/672 [02:17<02:03,  1.01it/s, est. speed input: 540.74 toks/s, output: 4432.84 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 549/672 [02:17<01:26,  1.42it/s, est. speed input: 542.29 toks/s, output: 4472.40 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 550/672 [02:17<01:15,  1.61it/s, est. speed input: 542.34 toks/s, output: 4486.23 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 551/672 [02:23<03:14,  1.61s/it, est. speed input: 522.59 toks/s, output: 4339.01 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 552/672 [02:24<03:03,  1.53s/it, est. speed input: 519.07 toks/s, output: 4315.78 toks/s][A
Processed prompts:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 553/672 [02:25<02:57,  1.50s/it, est. speed input: 515.25 toks/s, output: 4295.39 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 556/672 [02:25<01:26,  1.34it/s, est. speed input: 518.35 toks/s, output: 4355.21 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 558/672 [02:26<00:59,  1.92it/s, est. speed input: 520.28 toks/s, output: 4393.94 toks/s][A
Processed prompts:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 560/672 [02:29<01:37,  1.14it/s, est. speed input: 511.59 toks/s, output: 4328.89 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 562/672 [02:29<01:10,  1.55it/s, est. speed input: 513.46 toks/s, output: 4351.55 toks/s][A
Processed prompts:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 566/672 [02:29<00:37,  2.81it/s, est. speed input: 518.22 toks/s, output: 4429.57 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 569/672 [02:29<00:25,  4.02it/s, est. speed input: 521.77 toks/s, output: 4487.84 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 571/672 [02:34<01:15,  1.33it/s, est. speed input: 507.69 toks/s, output: 4383.49 toks/s][A
Processed prompts:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 574/672 [02:34<00:49,  1.98it/s, est. speed input: 508.91 toks/s, output: 4439.97 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 577/672 [02:34<00:33,  2.84it/s, est. speed input: 510.14 toks/s, output: 4496.39 toks/s][A
Processed prompts:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 580/672 [02:38<00:56,  1.62it/s, est. speed input: 500.71 toks/s, output: 4453.39 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 583/672 [02:38<00:38,  2.29it/s, est. speed input: 502.65 toks/s, output: 4508.47 toks/s][A
Processed prompts:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 585/672 [02:38<00:30,  2.87it/s, est. speed input: 503.83 toks/s, output: 4544.17 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 591/672 [02:39<00:23,  3.50it/s, est. speed input: 507.49 toks/s, output: 4584.29 toks/s][A
Processed prompts:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 593/672 [02:45<01:01,  1.29it/s, est. speed input: 492.42 toks/s, output: 4454.83 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 600/672 [02:47<00:40,  1.78it/s, est. speed input: 493.51 toks/s, output: 4488.73 toks/s][A
Processed prompts:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 608/672 [02:51<00:31,  2.02it/s, est. speed input: 492.27 toks/s, output: 4518.39 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 612/672 [02:51<00:23,  2.61it/s, est. speed input: 495.29 toks/s, output: 4586.96 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 614/672 [02:51<00:19,  2.96it/s, est. speed input: 496.56 toks/s, output: 4619.04 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 616/672 [02:52<00:22,  2.50it/s, est. speed input: 493.98 toks/s, output: 4617.44 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 620/672 [02:52<00:14,  3.62it/s, est. speed input: 496.49 toks/s, output: 4685.54 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 622/672 [02:54<00:16,  2.98it/s, est. speed input: 494.32 toks/s, output: 4689.36 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 624/672 [02:54<00:16,  2.87it/s, est. speed input: 493.00 toks/s, output: 4703.14 toks/s][A
Processed prompts:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 627/672 [02:55<00:11,  3.79it/s, est. speed input: 494.53 toks/s, output: 4749.36 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 629/672 [02:56<00:14,  3.05it/s, est. speed input: 493.68 toks/s, output: 4752.36 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 630/672 [02:57<00:16,  2.48it/s, est. speed input: 492.58 toks/s, output: 4746.55 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 631/672 [02:58<00:20,  1.96it/s, est. speed input: 490.72 toks/s, output: 4736.22 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 632/672 [03:00<00:32,  1.23it/s, est. speed input: 486.07 toks/s, output: 4698.78 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 638/672 [03:00<00:10,  3.13it/s, est. speed input: 491.67 toks/s, output: 4797.52 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 640/672 [03:01<00:11,  2.75it/s, est. speed input: 491.01 toks/s, output: 4804.36 toks/s][A
Processed prompts:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 643/672 [03:01<00:08,  3.45it/s, est. speed input: 492.46 toks/s, output: 4843.89 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 653/672 [03:01<00:02,  8.34it/s, est. speed input: 496.92 toks/s, output: 5009.73 toks/s][A
Processed prompts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 658/672 [03:04<00:03,  4.59it/s, est. speed input: 494.05 toks/s, output: 5032.96 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 662/672 [03:04<00:01,  5.87it/s, est. speed input: 496.21 toks/s, output: 5095.99 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 671/672 [03:04<00:00, 10.03it/s, est. speed input: 502.69 toks/s, output: 5242.36 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 672/672 [03:04<00:00,  3.64it/s, est. speed input: 503.52 toks/s, output: 5258.65 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/672 [00:00<?, ?it/s][A
Evaluate:  18%|‚ñà‚ñä        | 121/672 [00:00<00:01, 322.82it/s][A
Evaluate:  30%|‚ñà‚ñà‚ñâ       | 201/672 [00:00<00:01, 271.98it/s][A
Evaluate:  34%|‚ñà‚ñà‚ñà‚ñç      | 229/672 [00:01<00:04, 88.62it/s] [AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 672/672 [00:01<00:00, 341.43it/s]
{'num_samples': 84, 'num_scores': 672, 'timeout_samples': 0, 'empty_samples': 9, 'acc': 13.1, 'total_acc': 14.880952380952381, 'pass_at_k_percent': {'1': 14.9, '8': 15.5}, 'pass_at_k_valid_counts': {'1': 84, '8': 84}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/olympiadbench/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 20:02:02] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/olympiadbench  acc=13.1 pass_at_k={'1': 14.9, '8': 15.5}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [04:08<02:15, 135.47s/ds][Info] Sharding enabled: Process 1/8 handling range [62:124]
==================================================
data: math500  ,remain samples: 62
{'idx': 62, 'problem': 'Find the smallest positive real number $C$ for which\n\\[\\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\bold{v} \\right\\| \\le C \\|\\bold{v}\\|\\]for all two-dimensional vectors $\\bold{v}.$\n\nNote that for a two-dimensional vector $\\mathbf{a},$ $\\|\\mathbf{a}\\|$ is the magnitude of $\\mathbf{a}.$', 'solution': 'Let $\\bold{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.  Then\n\\[\\|\\bold{v}\\| = \\left\\| \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right\\| = \\sqrt{x^2 + y^2},\\]and\n\\begin{align*}\n\\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\bold{v} \\right\\| &= \\left\\| \\begin{pmatrix} 2 & 3 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right\\| \\\\\n&= \\left\\| \\begin{pmatrix} 2x + 3y \\\\ -2y \\end{pmatrix} \\right\\| \\\\\n&= \\sqrt{(2x + 3y)^2 + (-2y)^2} \\\\\n&= \\sqrt{4x^2 + 12xy + 13y^2},\n\\end{align*}so the given inequality becomes\n\\[\\sqrt{4x^2 + 12xy + 13y^2} \\le C \\sqrt{x^2 + y^2},\\]or\n\\[\\sqrt{\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2}} \\le C.\\]Thus, we can think of $C$ as the maximum value of the expression in the left-hand side.\n\nMaximizing the expression in the left-hand side is equivalent to maximizing its square, namely\n\\[\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2}.\\]Let $k$ be a possible value of this expression, which means the equation\n\\[\\frac{4x^2 + 12xy + 13y^2}{x^2 + y^2} = k\\]has a solution in $x$ and $y$.  We can re-write this equation as\n\\[(4 - k) x^2 + 12xy + (13 - k) y^2 = 0.\\]For this quadratic expression to have a solution in $x$ and $y$, its discriminant must be nonnegative.  In other words,\n\\[12^2 - 4 (4 - k)(13 - k) \\ge 0,\\]or $4k^2 - 68k + 64 \\le 0$.  This inequality factors as $4(k - 1)(k - 16) \\le 0$.  The largest value of $k$ that satisfies this inequality is 16, so the value of $C$ we seek is $\\sqrt{16} = \\boxed{4}$.  Note that equality occurs for\n\\[\\bold{v} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\\]', 'answer': '4', 'subject': 'Precalculus', 'level': 5, 'unique_id': 'test/precalculus/675.json'}

  0%|          | 0/62 [00:00<?, ?it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 23/62 [00:00<00:00, 222.00it/s][A
 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 46/62 [00:00<00:00, 211.92it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62/62 [00:00<00:00, 218.78it/s]
-------------------- Epoch 0

Processed prompts:   0%|          | 0/496 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   2%|‚ñè         | 9/496 [00:02<02:01,  4.02it/s, est. speed input: 303.97 toks/s, output: 59.37 toks/s][A
Processed prompts:   5%|‚ñå         | 25/496 [00:02<00:47,  9.94it/s, est. speed input: 599.07 toks/s, output: 660.48 toks/s][A
Processed prompts:   7%|‚ñã         | 33/496 [00:03<00:34, 13.47it/s, est. speed input: 746.94 toks/s, output: 991.29 toks/s][A
Processed prompts:   8%|‚ñä         | 41/496 [00:03<00:24, 18.34it/s, est. speed input: 847.60 toks/s, output: 1332.66 toks/s][A
Processed prompts:  11%|‚ñà‚ñè        | 57/496 [00:03<00:17, 24.85it/s, est. speed input: 1050.84 toks/s, output: 1880.14 toks/s][A
Processed prompts:  13%|‚ñà‚ñé        | 65/496 [00:03<00:17, 24.28it/s, est. speed input: 1102.59 toks/s, output: 2064.96 toks/s][A
Processed prompts:  15%|‚ñà‚ñç        | 73/496 [00:04<00:17, 24.39it/s, est. speed input: 1169.54 toks/s, output: 2263.41 toks/s][A
Processed prompts:  16%|‚ñà‚ñã        | 81/496 [00:04<00:16, 25.69it/s, est. speed input: 1210.36 toks/s, output: 2489.73 toks/s][A
Processed prompts:  18%|‚ñà‚ñä        | 89/496 [00:05<00:19, 20.46it/s, est. speed input: 1231.29 toks/s, output: 2542.49 toks/s][A
Processed prompts:  20%|‚ñà‚ñâ        | 97/496 [00:05<00:16, 23.60it/s, est. speed input: 1296.11 toks/s, output: 2805.93 toks/s][A
Processed prompts:  21%|‚ñà‚ñà        | 105/496 [00:06<00:21, 18.44it/s, est. speed input: 1248.43 toks/s, output: 2834.89 toks/s][A
Processed prompts:  23%|‚ñà‚ñà‚ñé       | 113/496 [00:06<00:19, 19.66it/s, est. speed input: 1251.38 toks/s, output: 3014.76 toks/s][A
Processed prompts:  24%|‚ñà‚ñà‚ñç       | 121/496 [00:06<00:17, 21.26it/s, est. speed input: 1263.84 toks/s, output: 3078.16 toks/s][A
Processed prompts:  26%|‚ñà‚ñà‚ñå       | 129/496 [00:06<00:14, 24.76it/s, est. speed input: 1323.03 toks/s, output: 3351.16 toks/s][A
Processed prompts:  28%|‚ñà‚ñà‚ñä       | 137/496 [00:07<00:19, 18.47it/s, est. speed input: 1267.84 toks/s, output: 3382.70 toks/s][A
Processed prompts:  31%|‚ñà‚ñà‚ñà       | 153/496 [00:08<00:14, 23.60it/s, est. speed input: 1335.84 toks/s, output: 3884.40 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 162/496 [00:08<00:12, 26.91it/s, est. speed input: 1410.06 toks/s, output: 4103.48 toks/s][A
Processed prompts:  33%|‚ñà‚ñà‚ñà‚ñé      | 166/496 [00:08<00:13, 24.22it/s, est. speed input: 1421.44 toks/s, output: 4154.25 toks/s][A
Processed prompts:  35%|‚ñà‚ñà‚ñà‚ñå      | 174/496 [00:09<00:19, 16.42it/s, est. speed input: 1367.56 toks/s, output: 4083.42 toks/s][A
Processed prompts:  37%|‚ñà‚ñà‚ñà‚ñã      | 182/496 [00:10<00:23, 13.51it/s, est. speed input: 1296.24 toks/s, output: 3924.27 toks/s][A
Processed prompts:  38%|‚ñà‚ñà‚ñà‚ñä      | 190/496 [00:10<00:19, 16.05it/s, est. speed input: 1314.22 toks/s, output: 3968.95 toks/s][A
Processed prompts:  39%|‚ñà‚ñà‚ñà‚ñâ      | 193/496 [00:10<00:22, 13.45it/s, est. speed input: 1286.04 toks/s, output: 3909.72 toks/s][A
Processed prompts:  41%|‚ñà‚ñà‚ñà‚ñà      | 201/496 [00:11<00:15, 18.67it/s, est. speed input: 1378.69 toks/s, output: 4202.12 toks/s][A
Processed prompts:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 217/496 [00:11<00:08, 32.27it/s, est. speed input: 1625.55 toks/s, output: 4707.73 toks/s][A
Processed prompts:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 225/496 [00:11<00:13, 20.85it/s, est. speed input: 1578.37 toks/s, output: 4738.14 toks/s][A
Processed prompts:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 233/496 [00:12<00:18, 14.00it/s, est. speed input: 1486.26 toks/s, output: 4673.57 toks/s][A
Processed prompts:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/496 [00:13<00:15, 16.43it/s, est. speed input: 1491.37 toks/s, output: 4895.19 toks/s][A
Processed prompts:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 249/496 [00:14<00:19, 12.48it/s, est. speed input: 1419.91 toks/s, output: 4691.79 toks/s][A
Processed prompts:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 257/496 [00:14<00:17, 13.73it/s, est. speed input: 1419.53 toks/s, output: 4873.46 toks/s][A
Processed prompts:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 270/496 [00:14<00:10, 21.15it/s, est. speed input: 1477.32 toks/s, output: 5124.60 toks/s][A
Processed prompts:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 278/496 [00:15<00:11, 18.71it/s, est. speed input: 1489.12 toks/s, output: 5180.07 toks/s][A
Processed prompts:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 286/496 [00:15<00:09, 22.48it/s, est. speed input: 1516.76 toks/s, output: 5284.26 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 291/496 [00:16<00:11, 17.62it/s, est. speed input: 1484.00 toks/s, output: 5180.52 toks/s][A
Processed prompts:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 295/496 [00:16<00:11, 18.19it/s, est. speed input: 1480.36 toks/s, output: 5189.89 toks/s][A
Processed prompts:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 310/496 [00:16<00:06, 28.97it/s, est. speed input: 1670.02 toks/s, output: 5612.65 toks/s][A
Processed prompts:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 318/496 [00:17<00:07, 22.80it/s, est. speed input: 1654.80 toks/s, output: 5748.76 toks/s][A
Processed prompts:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 329/496 [00:17<00:07, 21.19it/s, est. speed input: 1656.84 toks/s, output: 5784.04 toks/s][A
Processed prompts:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 337/496 [00:18<00:10, 14.68it/s, est. speed input: 1596.20 toks/s, output: 5593.30 toks/s][A
Processed prompts:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 345/496 [00:18<00:08, 18.29it/s, est. speed input: 1627.25 toks/s, output: 5692.48 toks/s][A
Processed prompts:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 353/496 [00:19<00:09, 15.73it/s, est. speed input: 1616.99 toks/s, output: 5771.71 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 356/496 [00:19<00:08, 16.42it/s, est. speed input: 1623.08 toks/s, output: 5807.57 toks/s][A
Processed prompts:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 359/496 [00:19<00:08, 15.68it/s, est. speed input: 1620.30 toks/s, output: 5811.42 toks/s][A
Processed prompts:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 362/496 [00:20<00:13, 10.02it/s, est. speed input: 1573.19 toks/s, output: 5656.02 toks/s][A
Processed prompts:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 369/496 [00:22<00:20,  6.30it/s, est. speed input: 1484.47 toks/s, output: 5336.52 toks/s][A
Processed prompts:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 377/496 [00:28<00:48,  2.47it/s, est. speed input: 1226.60 toks/s, output: 4293.50 toks/s][A
Processed prompts:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 385/496 [00:30<00:37,  2.97it/s, est. speed input: 1198.01 toks/s, output: 4367.88 toks/s][A
Processed prompts:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 393/496 [00:37<00:53,  1.94it/s, est. speed input: 1021.26 toks/s, output: 3822.25 toks/s][A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/496 [01:17<03:06,  1.96s/it, est. speed input: 507.84 toks/s, output: 2017.91 toks/s] [A
Processed prompts:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 402/496 [01:17<02:55,  1.86s/it, est. speed input: 508.73 toks/s, output: 2053.81 toks/s][A
Processed prompts:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 441/496 [01:20<00:29,  1.89it/s, est. speed input: 540.88 toks/s, output: 3442.95 toks/s][A
Processed prompts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 449/496 [01:21<00:21,  2.21it/s, est. speed input: 556.00 toks/s, output: 3706.30 toks/s][A
Processed prompts:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 457/496 [01:22<00:14,  2.74it/s, est. speed input: 568.08 toks/s, output: 3995.85 toks/s][A
Processed prompts:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 465/496 [01:23<00:10,  3.05it/s, est. speed input: 568.65 toks/s, output: 4213.77 toks/s][A
Processed prompts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 473/496 [01:23<00:05,  3.99it/s, est. speed input: 583.49 toks/s, output: 4501.62 toks/s][A
Processed prompts:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 481/496 [01:26<00:03,  3.86it/s, est. speed input: 582.67 toks/s, output: 4669.28 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 489/496 [01:26<00:01,  4.80it/s, est. speed input: 594.62 toks/s, output: 4921.72 toks/s][A
Processed prompts:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 493/496 [01:26<00:00,  5.58it/s, est. speed input: 596.52 toks/s, output: 5057.42 toks/s][A
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 496/496 [01:26<00:00,  6.20it/s, est. speed input: 597.27 toks/s, output: 5153.08 toks/s][AProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 496/496 [01:26<00:00,  5.71it/s, est. speed input: 597.27 toks/s, output: 5153.08 toks/s]
-------------------- Epoch 1
Unsolved samples: 0

Evaluate:   0%|          | 0/496 [00:00<?, ?it/s][AEvaluate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 496/496 [00:00<00:00, 13357.31it/s]
{'num_samples': 62, 'num_scores': 496, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 45.2, 'total_acc': 45.16129032258064, 'pass_at_k_percent': {'1': 45.2, '8': 45.2}, 'pass_at_k_valid_counts': {'1': 62, '8': 62}}
Saved to /uge_mnt/home/caixq/project/noisy-RLVR/eval_results/noise_rlvr_1_5b_128batchsize_deepscaler_v2_think-boxed/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/math500/test_think-boxed_-1_seed0_t0.0_s0_e-1_part1.jsonl
[2025-12-03 20:03:30] ‚úì B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2/math500  acc=45.2 pass_at_k={'1': 45.2, '8': 45.2}
B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:36<00:00, 114.13s/ds]B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200/g2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:36<00:00, 112.29s/ds]
[2025-12-03 20:03:30] ‚úÖ ÂÆåÊàêÔºöB_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_200Ôºàg1+g2 Áº∫Â§±Êï∞ÊçÆÈõÜÂ∑≤Ë°•ÂÖ®Ôºâ
[2025-12-03 20:03:30] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 20:03:30 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:03:30 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:03:30 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:03:37 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:03:44 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:03:44 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb7d2a798d0>
INFO 12-03 20:03:55 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:03:55 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:03:55 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:03:55 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:03:56 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:03:56 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:03:56 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:03:56 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:03:56 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:03:56 [core.py:396]     self._init_executor()
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:03:56 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:03:56 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:03:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:03:56 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:03:56 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:03:56 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:03:56 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:03:56 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:03:56 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:03:56 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:03:56 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:03:56 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:03:56 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:03:56 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:03:56 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:03:56 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:03:56 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:03:56 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:03:56 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:03:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:03:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:03:56 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3288135 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3288135 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fba8676c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fba86715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fba3728e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fba86b37b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fba86b3820e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fba86b4eb0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fba86b3a329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fba7e8864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fba7dfa5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fba7dfa64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5607eb172c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5607eb0ff2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5607eb0ffbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5607eb0ffc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5607eb172b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5607eb1dec3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5607eb1e1f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5607eb100c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5607eb23fc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5607eb265407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5607eb265634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5607eb265718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5607eb26575b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5607eb265972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5607eb26bf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5607eb26c1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5607eb26c469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fba87633d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fba87633e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5607eb1d72d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:03:58] ‚ö† Ê®°Âûã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:03:58] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 20:03:58 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:03:58 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:03:58 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:04:05 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:04:12 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:04:12 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0961ce5ba0>
INFO 12-03 20:04:38 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:04:38 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:04:38 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:04:38 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:04:38 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:04:38 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:04:38 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:04:38 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:04:38 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:04:38 [core.py:396]     self._init_executor()
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:04:38 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:04:38 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:04:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:04:38 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:04:38 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:04:38 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:04:38 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:04:38 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:04:38 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:04:38 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:04:38 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:04:38 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:04:38 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:04:38 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:04:38 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:04:38 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:04:38 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:04:38 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:04:38 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:04:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:04:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:04:38 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3288828 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3288828 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f0c1576c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f0c15715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f0bc628e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f0c15ba2b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f0c15ba320e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f0c15bb9b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f0c15ba5329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f0c0d8864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f0c0cfa5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f0c0cfa64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5612bda9dc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5612bda2a2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5612bda2abbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5612bda2ac83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5612bda9db85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5612bdb09c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5612bdb0cf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5612bda2bc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5612bdb6ac30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5612bdb90407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5612bdb90634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5612bdb90718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5612bdb9075b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5612bdb90972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5612bdb96f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5612bdb971ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5612bdb97469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f0c1669ed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f0c1669ee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5612bdb022d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:04:40] ‚ö† Ê®°Âûã B_rb_manual_algo2_est_r00.10_r10.20_est0.10_0.20_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:04:40] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 20:04:40 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:04:40 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:04:40 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:04:46 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:04:53 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:04:53 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f06b6ce6c80>
INFO 12-03 20:05:18 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:05:18 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:05:18 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:05:19 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:05:19 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:05:19 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:05:19 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:05:19 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:05:19 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:05:19 [core.py:396]     self._init_executor()
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:05:19 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:05:19 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:05:19 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:05:19 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:05:19 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:05:19 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:05:19 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:05:19 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:05:19 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:05:19 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:05:19 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:05:19 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:05:19 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:05:19 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:05:19 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:05:19 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:05:19 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:05:19 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:05:19 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:05:19 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:05:19 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:05:19 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3289822 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3289822 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f096a76c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f096a715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f091b28e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f096aba0b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f096aba120e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f096abb7b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f096aba3329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f09628864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f0961fa5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f0961fa64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5645f01e6c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5645f01732a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5645f0173bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5645f0173c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5645f01e6b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5645f0252c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5645f0255f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5645f0174c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5645f02b3c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5645f02d9407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5645f02d9634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5645f02d9718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5645f02d975b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5645f02d9972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5645f02dff60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5645f02e01ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5645f02e0469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f096b69cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f096b69ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5645f024b2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:05:20] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:05:20] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 20:05:20 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:05:20 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:05:20 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:05:27 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:05:34 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:05:35 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f11762424a0>
INFO 12-03 20:05:55 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:05:55 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:05:55 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:05:55 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:05:56 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:05:56 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:05:56 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:05:56 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:05:56 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:05:56 [core.py:396]     self._init_executor()
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:05:56 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:05:56 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:05:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:05:56 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:05:56 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:05:56 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:05:56 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:05:56 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:05:56 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:05:56 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:05:56 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:05:56 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:05:56 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:05:56 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:05:56 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:05:56 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:05:56 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:05:56 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:05:56 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:05:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:05:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:05:56 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3291234 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3291234 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f1429b6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f1429b15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f13daa8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f1429f6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f1429f6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f1429f82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f1429f6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f14220864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f14217a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f14217a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5586ca481c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5586ca40e2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5586ca40ebbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5586ca40ec83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5586ca481b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5586ca4edc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5586ca4f0f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5586ca40fc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5586ca54ec30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5586ca574407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5586ca574634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5586ca574718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5586ca57475b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5586ca574972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5586ca57af60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5586ca57b1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5586ca57b469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f142adc5d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f142adc5e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5586ca4e62d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:05:57] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:05:57] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 20:05:57 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:05:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:05:57 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:06:04 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:06:10 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:06:10 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f29959f24d0>
INFO 12-03 20:06:21 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:06:21 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:06:21 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:06:21 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:06:21 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:06:21 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:06:21 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:06:21 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:06:21 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:06:21 [core.py:396]     self._init_executor()
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:06:21 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:06:21 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:06:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:06:21 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:06:21 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:06:21 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:06:21 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:06:21 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:06:21 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:06:21 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:06:21 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:06:21 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:06:21 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:06:21 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:06:21 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:06:21 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:06:21 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:06:21 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:06:21 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:06:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:06:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:06:21 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3292440 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3292440 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2c4936c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f2c49315b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f2bfa28e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f2c4976bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f2c4976c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f2c49782b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f2c4976e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f2c418864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f2c40fa5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f2c40fa64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x560045312c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x56004529f2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x56004529fbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x56004529fc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x560045312b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x56004537ec3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x560045381f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5600452a0c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5600453dfc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x560045405407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x560045405634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x560045405718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56004540575b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x560045405972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x56004540bf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x56004540c1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x56004540c469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f2c4a569d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f2c4a569e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5600453772d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:06:23] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:06:23] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 20:06:23 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:06:23 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:06:23 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:06:29 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:06:36 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:06:36 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f38ee5be4a0>
INFO 12-03 20:07:02 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:07:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:07:02 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:07:02 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:07:02 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:07:02 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:07:02 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:07:02 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:07:02 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:07:02 [core.py:396]     self._init_executor()
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:07:02 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:07:02 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:07:02 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:07:02 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:07:02 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:07:02 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:07:02 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:07:02 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:07:02 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:07:02 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:07:02 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:07:02 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:07:02 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:07:02 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:07:02 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:07:02 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:07:02 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:07:02 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:07:02 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:07:02 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:07:02 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:07:02 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3293018 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3293018 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3ba1d6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f3ba1d15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f3b52c8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f3ba216bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f3ba216c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f3ba2182b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f3ba216e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f3b9a2864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f3b999a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f3b999a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5638b0f2bc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5638b0eb82a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5638b0eb8bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5638b0eb8c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5638b0f2bb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5638b0f97c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5638b0f9af1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5638b0eb9c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5638b0ff8c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5638b101e407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5638b101e634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5638b101e718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5638b101e75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5638b101e972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5638b1024f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5638b10251ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5638b1025469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f3ba2f81d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f3ba2f81e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5638b0f902d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:07:03] ‚ö† Ê®°Âûã B_rb_manual_grpo_r00.10_r10.20_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:07:03] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 20:07:04 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:07:04 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:07:04 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:07:10 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:07:17 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:07:18 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f63810286a0>
INFO 12-03 20:07:28 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:07:28 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:07:28 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:07:28 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:07:28 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:07:28 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:07:28 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:07:28 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:07:28 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:07:28 [core.py:396]     self._init_executor()
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:07:28 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:07:28 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:07:28 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:07:28 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:07:28 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:07:28 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:07:28 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:07:28 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:07:28 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:07:28 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:07:28 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:07:28 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:07:28 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:07:28 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:07:28 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:07:28 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:07:28 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:07:28 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:07:28 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:07:28 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:07:28 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:07:28 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3294239 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3294239 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f663496c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f6634915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f65e588e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f6634d6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f6634d6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f6634d82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f6634d6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f662ce864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f662c5a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f662c5a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x560504179c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5605041062a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x560504106bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x560504106c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x560504179b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5605041e5c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5605041e8f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x560504107c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x560504246c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x56050426c407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x56050426c634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x56050426c718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x56050426c75b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x56050426c972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x560504272f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5605042731ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x560504273469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f6635bc6d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f6635bc6e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5605041de2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:07:30] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:07:30] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 20:07:30 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:07:30 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:07:30 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:07:36 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:07:43 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:07:44 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f7ed825c6a0>
INFO 12-03 20:07:54 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:07:54 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:07:54 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:07:54 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:07:55 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:07:55 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:07:55 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:07:55 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:07:55 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:07:55 [core.py:396]     self._init_executor()
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:07:55 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:07:55 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:07:55 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:07:55 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:07:55 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:07:55 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:07:55 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:07:55 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:07:55 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:07:55 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:07:55 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:07:55 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:07:55 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:07:55 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:07:55 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:07:55 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:07:55 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:07:55 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:07:55 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:07:55 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:07:55 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:07:55 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3294996 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3294996 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f818bd6c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f818bd15b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f813c88e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f818c144b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f818c14520e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f818c15bb0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f818c147329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f8183e864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f81835a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f81835a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x562a0fa7ec04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x562a0fa0b2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x562a0fa0bbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x562a0fa0bc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x562a0fa7eb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x562a0faeac3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x562a0faedf1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x562a0fa0cc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x562a0fb4bc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x562a0fb71407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x562a0fb71634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x562a0fb71718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x562a0fb7175b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x562a0fb71972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x562a0fb77f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x562a0fb781ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x562a0fb78469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f818cc40d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f818cc40e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x562a0fae32d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:07:56] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:07:56] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 20:07:56 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:07:56 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:07:56 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:08:03 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:08:10 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:08:10 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd3be951ea0>
INFO 12-03 20:08:20 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:08:20 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:08:20 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:08:20 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:08:21 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:08:21 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:08:21 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:08:21 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:08:21 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:08:21 [core.py:396]     self._init_executor()
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:08:21 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:08:21 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:08:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:08:21 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:08:21 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:08:21 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:08:21 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:08:21 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:08:21 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:08:21 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:08:21 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:08:21 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:08:21 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:08:21 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:08:21 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:08:21 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:08:21 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:08:21 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:08:21 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:08:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:08:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:08:21 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3295570 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3295570 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd67236c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fd672315b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fd62328e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fd672739b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fd67273a20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fd672750b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fd67273c329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fd66a8864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fd669fa5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fd669fa64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55b6daf12c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55b6dae9f2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55b6dae9fbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55b6dae9fc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55b6daf12b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55b6daf7ec3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55b6daf81f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55b6daea0c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55b6dafdfc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55b6db005407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55b6db005634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55b6db005718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55b6db00575b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55b6db005972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55b6db00bf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55b6db00c1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55b6db00c469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fd6734e7d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fd6734e7e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55b6daf772d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:08:22] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:08:22] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 20:08:22 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:08:22 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:08:22 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:08:29 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:08:35 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:08:35 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8cf5ea24a0>
INFO 12-03 20:08:46 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:08:46 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:08:46 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:08:46 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/C_llm_verifier_grpo_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:08:46 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:08:46 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:08:46 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:08:46 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:08:46 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:08:46 [core.py:396]     self._init_executor()
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:08:46 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:08:46 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:08:46 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:08:46 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:08:46 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:08:46 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:08:46 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:08:46 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:08:46 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:08:46 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:08:46 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:08:46 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:08:46 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:08:46 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:08:46 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:08:46 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:08:46 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:08:46 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:08:46 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:08:46 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:08:46 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:08:46 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3296189 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3296189 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f8fa996c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f8fa9915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f8f5a48e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f8fa9d70b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f8fa9d7120e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f8fa9d87b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f8fa9d73329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f8fa1a864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f8fa11a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f8fa11a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55fc88b33c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55fc88ac02a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55fc88ac0bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55fc88ac0c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55fc88b33b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55fc88b9fc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55fc88ba2f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55fc88ac1c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55fc88c00c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55fc88c26407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55fc88c26634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55fc88c26718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55fc88c2675b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55fc88c26972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55fc88c2cf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55fc88c2d1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55fc88c2d469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f8faa86cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f8faa86ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55fc88b982d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:08:48] ‚ö† Ê®°Âûã C_llm_verifier_grpo_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:08:48] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 20:08:48 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:08:48 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:08:48 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:08:54 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:09:01 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:09:01 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4826de9900>
INFO 12-03 20:09:11 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:09:11 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:09:11 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:09:11 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:09:12 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:09:12 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:09:12 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:09:12 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:09:12 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:09:12 [core.py:396]     self._init_executor()
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:09:12 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:09:12 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:09:12 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:09:12 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:09:12 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:09:12 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:09:12 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:09:12 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:09:12 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:09:12 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:09:12 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:09:12 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:09:12 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:09:12 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:09:12 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:09:12 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:09:12 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:09:12 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:09:12 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:09:12 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:09:12 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:09:12 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3296883 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3296883 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f4ada96c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f4ada915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f4a8b48e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f4adacdbb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f4adacdc20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f4adacf2b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f4adacde329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f4ad2a864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f4ad21a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f4ad21a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55c0b095fc04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55c0b08ec2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55c0b08ecbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55c0b08ecc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55c0b095fb85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55c0b09cbc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55c0b09cef1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55c0b08edc98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55c0b0a2cc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55c0b0a52407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55c0b0a52634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55c0b0a52718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55c0b0a5275b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55c0b0a52972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55c0b0a58f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55c0b0a591ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55c0b0a59469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f4adb7d7d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f4adb7d7e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55c0b09c42d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:09:13] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:09:13] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 20:09:13 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:09:13 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:09:13 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:09:20 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:09:27 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:09:27 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd700e064d0>
INFO 12-03 20:09:38 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:09:38 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:09:38 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:09:38 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:09:38 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:09:38 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:09:38 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:09:38 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:09:38 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:09:38 [core.py:396]     self._init_executor()
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:09:38 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:09:38 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:09:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:09:38 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:09:38 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:09:38 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:09:38 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:09:38 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:09:38 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:09:38 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:09:38 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:09:38 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:09:38 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:09:38 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:09:38 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:09:38 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:09:38 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:09:38 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:09:38 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:09:38 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:09:38 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:09:38 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3297346 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3297346 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd9b496c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fd9b4915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fd96548e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fd9b4cdcb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fd9b4cdd20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fd9b4cf3b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fd9b4cdf329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fd9aca864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fd9ac1a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fd9ac1a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5585df450c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5585df3dd2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5585df3ddbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5585df3ddc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5585df450b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5585df4bcc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5585df4bff1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5585df3dec98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5585df51dc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5585df543407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5585df543634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5585df543718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5585df54375b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5585df543972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5585df549f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5585df54a1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5585df54a469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fd9b57d8d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fd9b57d8e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5585df4b52d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:09:40] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:09:40] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 20:09:40 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:09:40 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:09:40 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:09:47 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:09:53 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:09:54 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f7fdeaf6b30>
INFO 12-03 20:10:04 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:10:04 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:10:04 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:10:04 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:10:05 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:10:05 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:10:05 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:10:05 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:10:05 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:10:05 [core.py:396]     self._init_executor()
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:10:05 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:10:05 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:10:05 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:10:05 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:10:05 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:10:05 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:10:05 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:10:05 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:10:05 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:10:05 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:10:05 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:10:05 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:10:05 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:10:05 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:10:05 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:10:05 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:10:05 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:10:05 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:10:05 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:10:05 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:10:05 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:10:05 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3298100 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3298100 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f829256c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f8292515b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f824308e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f82929b2b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f82929b320e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f82929c9b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f82929b5329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f828a6864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f8289da5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f8289da64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x560cd0d20c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x560cd0cad2a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x560cd0cadbbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x560cd0cadc83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x560cd0d20b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x560cd0d8cc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x560cd0d8ff1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x560cd0caec98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x560cd0dedc30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x560cd0e13407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x560cd0e13634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x560cd0e13718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x560cd0e1375b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x560cd0e13972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x560cd0e19f60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x560cd0e1a1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x560cd0e1a469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f82934aed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f82934aee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x560cd0d852d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:10:06] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:10:06] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 20:10:06 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:10:06 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:10:06 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:10:13 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:10:19 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:10:19 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0d1dca86a0>
INFO 12-03 20:10:30 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:10:30 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:10:30 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:10:30 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:10:30 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:10:30 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:10:30 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:10:30 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:10:30 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:10:30 [core.py:396]     self._init_executor()
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:10:30 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:10:30 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:10:30 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:10:30 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:10:30 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:10:30 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:10:30 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:10:30 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:10:30 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:10:30 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:10:30 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:10:30 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:10:30 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:10:30 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:10:30 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:10:30 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:10:30 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:10:30 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:10:30 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:10:30 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:10:30 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:10:30 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3298706 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3298706 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f0fd196c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f0fd1915b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7f0f8248e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7f0fd1d32b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7f0fd1d3320e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7f0fd1d49b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7f0fd1d35329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7f0fc9a864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7f0fc91a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7f0fc91a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55bc06343c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55bc062d02a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55bc062d0bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55bc062d0c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55bc06343b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55bc063afc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55bc063b2f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55bc062d1c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55bc06410c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55bc06436407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55bc06436634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55bc06436718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55bc0643675b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55bc06436972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55bc0643cf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55bc0643d1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55bc0643d469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7f0fd282ed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7f0fd282ee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55bc063a82d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:10:31] ‚ö† Ê®°Âûã D_rb_online_rho1_algo2_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:10:31] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100
INFO 12-03 20:10:32 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:10:32 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:10:32 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:10:38 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:10:44 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:10:45 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc6bc51a500>
INFO 12-03 20:10:55 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:10:55 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:10:55 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:10:55 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_100...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:10:56 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:10:56 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:10:56 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:10:56 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:10:56 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:10:56 [core.py:396]     self._init_executor()
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:10:56 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:10:56 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:10:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:10:56 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:10:56 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:10:56 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:10:56 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:10:56 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:10:56 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:10:56 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:10:56 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:10:56 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:10:56 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:10:56 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:10:56 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:10:56 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:10:56 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:10:56 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:10:56 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:10:56 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:10:56 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:10:56 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3299449 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3299449 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc97016c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fc970115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fc920c8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fc9705b1b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fc9705b220e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fc9705c8b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fc9705b4329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fc9682864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fc9679a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fc9679a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x55a5141a4c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x55a5141312a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x55a514131bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x55a514131c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x55a5141a4b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x55a514210c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x55a514213f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x55a514132c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x55a514271c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x55a514297407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x55a514297634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x55a514297718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x55a51429775b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x55a514297972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x55a51429df60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x55a51429e1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x55a51429e469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fc9710add90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fc9710ade40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x55a5142092d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:10:57] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_100 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:10:57] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200
INFO 12-03 20:10:57 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:10:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:10:57 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:11:04 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:11:10 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:11:10 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7feb7a55e770>
INFO 12-03 20:11:21 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:11:21 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:11:21 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:11:21 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_200...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:11:21 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:11:21 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:11:21 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:11:21 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:11:21 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:11:21 [core.py:396]     self._init_executor()
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:11:21 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:11:21 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:11:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:11:21 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:11:21 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:11:21 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:11:21 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:11:21 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:11:21 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:11:21 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:11:21 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:11:21 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:11:21 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:11:21 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:11:21 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:11:21 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:11:21 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:11:21 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:11:21 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:11:21 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:11:21 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:11:21 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3299798 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3299798 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fee2e16c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fee2e115b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7feddec8e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fee2e5b1b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fee2e5b220e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fee2e5c8b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fee2e5b4329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fee262864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fee259a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fee259a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x561e27356c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x561e272e32a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x561e272e3bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x561e272e3c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x561e27356b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x561e273c2c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x561e273c5f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x561e272e4c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x561e27423c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x561e27449407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x561e27449634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x561e27449718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x561e2744975b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x561e27449972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x561e2744ff60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x561e274501ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x561e27450469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fee2f0add90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fee2f0ade40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x561e273bb2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:11:23] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_200 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:11:23] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300
INFO 12-03 20:11:23 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:11:23 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:11:23 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:11:29 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:11:36 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:11:36 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc261862bc0>
INFO 12-03 20:11:47 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:11:47 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:11:47 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:11:47 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_300...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:11:47 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:11:47 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:11:47 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:11:47 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:11:47 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:11:47 [core.py:396]     self._init_executor()
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:11:47 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:11:47 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:11:47 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:11:47 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:11:47 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:11:47 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:11:47 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:11:47 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:11:47 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:11:47 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:11:47 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:11:47 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:11:47 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:11:47 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:11:47 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:11:47 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:11:47 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:11:47 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:11:47 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:11:47 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:11:47 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:11:47 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3300556 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3300556 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc51556c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fc515515b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fc4c608e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fc5158e0b78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fc5158e120e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fc5158f7b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fc5158e3329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fc50d6864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fc50cda5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fc50cda64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x563462ac6c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x563462a532a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x563462a53bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x563462a53c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x563462ac6b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x563462b32c3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x563462b35f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x563462a54c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x563462b93c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x563462bb9407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x563462bb9634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x563462bb9718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x563462bb975b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x563462bb9972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x563462bbff60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x563462bc01ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x563462bc0469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fc5163dcd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fc5163dce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x563462b2b2d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:11:49] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_300 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
[2025-12-03 20:11:49] ‚ñ∂ Âä†ËΩΩÊ®°ÂûãÔºà‰∏ÄÊ¨°ÔºâÔºö/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313
INFO 12-03 20:11:49 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-03 20:11:49 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-03 20:11:49 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-03 20:11:55 [__init__.py:239] Automatically detected platform cuda.
INFO 12-03 20:12:02 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313', speculative_config=None, tokenizer='/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 12-03 20:12:02 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fca1df2d8d0>
INFO 12-03 20:12:12 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-03 20:12:12 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 12-03 20:12:12 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 12-03 20:12:12 [gpu_model_runner.py:1329] Starting to load model /data/giil/caixq/export/E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct/global_step_313...
CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62
ERROR 12-03 20:12:13 [core.py:396] EngineCore failed to start.
ERROR 12-03 20:12:13 [core.py:396] Traceback (most recent call last):
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 12-03 20:12:13 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 12-03 20:12:13 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 12-03 20:12:13 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 12-03 20:12:13 [core.py:396]     self._init_executor()
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 12-03 20:12:13 [core.py:396]     self.collective_rpc("load_model")
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 12-03 20:12:13 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
ERROR 12-03 20:12:13 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 12-03 20:12:13 [core.py:396]     self.model_runner.load_model()
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
ERROR 12-03 20:12:13 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 12-03 20:12:13 [core.py:396]     return loader.load_model(vllm_config=vllm_config)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
ERROR 12-03 20:12:13 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
ERROR 12-03 20:12:13 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
ERROR 12-03 20:12:13 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
ERROR 12-03 20:12:13 [core.py:396]     return LlamaModel(vllm_config=vllm_config,
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 12-03 20:12:13 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
ERROR 12-03 20:12:13 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
ERROR 12-03 20:12:13 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
ERROR 12-03 20:12:13 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
ERROR 12-03 20:12:13 [core.py:396]     lambda prefix: layer_type(config=config,
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
ERROR 12-03 20:12:13 [core.py:396]     self.mlp = LlamaMLP(
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
ERROR 12-03 20:12:13 [core.py:396]     self.down_proj = RowParallelLinear(
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
ERROR 12-03 20:12:13 [core.py:396]     self.quant_method.create_weights(
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 12-03 20:12:13 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 12-03 20:12:13 [core.py:396]   File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 12-03 20:12:13 [core.py:396]     return func(*args, **kwargs)
ERROR 12-03 20:12:13 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3301098 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 329, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 64, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
    self.model_runner.load_model()
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1332, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 496, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 542, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 321, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 609, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 610, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 323, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 254, in __init__
    self.mlp = LlamaMLP(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 77, in __init__
    self.down_proj = RowParallelLinear(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 1194, in __init__
    self.quant_method.create_weights(
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 3278228 has 35.91 GiB memory in use. Process 3301098 has 3.46 GiB memory in use. Of the allocated memory 3.06 GiB is allocated by PyTorch, with 33.83 MiB allocated in private pools (e.g., CUDA Graphs), and 13.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Trying to free a pointer not allocated here
Exception raised from raw_delete at /pytorch/torch/csrc/cuda/CUDAPluggableAllocator.cpp:151 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fccd176c1b6 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fccd1715b3f in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::cuda::CUDAPluggableAllocator::CUDAPluggableAllocator::raw_delete(void*) + 0x1a7 (0x7fcc8268e667 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x22b78 (0x7fccd1b6bb78 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x2320e (0x7fccd1b6c20e in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x39b0d (0x7fccd1b82b0d in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #6: c10::cuda::MemPool::~MemPool() + 0x1b9 (0x7fccd1b6e329 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0xdf74f0 (0x7fccc9c864f0 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x516907 (0x7fccc93a5907 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x5174d1 (0x7fccc93a64d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x1acc04 (0x5630e7eb3c04 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #11: <unknown function> + 0x1392a2 (0x5630e7e402a2 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #12: <unknown function> + 0x139bbb (0x5630e7e40bbb in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #13: <unknown function> + 0x139c83 (0x5630e7e40c83 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #14: <unknown function> + 0x1acb85 (0x5630e7eb3b85 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #15: <unknown function> + 0x218c3c (0x5630e7f1fc3c in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #16: <unknown function> + 0x21bf1b (0x5630e7f22f1b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #17: <unknown function> + 0x13ac98 (0x5630e7e41c98 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #18: <unknown function> + 0x279c30 (0x5630e7f80c30 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #19: <unknown function> + 0x29f407 (0x5630e7fa6407 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #20: Py_FinalizeEx + 0x134 (0x5630e7fa6634 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #21: Py_Exit + 0x8 (0x5630e7fa6718 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #22: <unknown function> + 0x29f75b (0x5630e7fa675b in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #23: PyErr_PrintEx + 0x12 (0x5630e7fa6972 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #24: PyRun_SimpleStringFlags + 0x50 (0x5630e7facf60 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #25: Py_RunMain + 0x26c (0x5630e7fad1ec in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #26: Py_BytesMain + 0x39 (0x5630e7fad469 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)
frame #27: <unknown function> + 0x29d90 (0x7fccd2930d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #28: __libc_start_main + 0x80 (0x7fccd2930e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #29: <unknown function> + 0x2112d1 (0x5630e7f182d1 in /uge_mnt/home/caixq/miniconda3/envs/eval/bin/python3)

[2025-12-03 20:12:14] ‚ö† Ê®°Âûã E_rb_rule_addon_tinyv_Llama-3.2-3B-Instruct__global_step_313 ËØÑÊµãÂ§±Ë¥•ÔºöRuntimeError('Engine core initialization failed. See root cause above.')
